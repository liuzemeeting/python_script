"""Bridges between the `asyncio` module and Tornado IOLoop.



.. versionadded:: 3.2



This module integrates Tornado with the ``asyncio`` module introduced

in Python 3.4. This makes it possible to combine the two libraries on

the same event loop.



.. deprecated:: 5.0



   While the code in this module is still used, it is now enabled

   automatically when `asyncio` is available, so applications should

   no longer need to refer to this module directly.



.. note::



   Tornado requires the `~asyncio.AbstractEventLoop.add_reader` family of

   methods, so it is not compatible with the `~asyncio.ProactorEventLoop` on

   Windows. Use the `~asyncio.SelectorEventLoop` instead.

"""



import concurrent.futures

import functools



from threading import get_ident

from tornado.gen import convert_yielded

from tornado.ioloop import IOLoop, _Selectable



import asyncio



import typing

from typing import Any, TypeVar, Awaitable, Callable, Union, Optional



if typing.TYPE_CHECKING:

    from typing import Set, Dict, Tuple  # noqa: F401



_T = TypeVar("_T")





class BaseAsyncIOLoop(IOLoop):

    def initialize(  # type: ignore

        self, asyncio_loop: asyncio.AbstractEventLoop, **kwargs: Any

    ) -> None:

        self.asyncio_loop = asyncio_loop

        # Maps fd to (fileobj, handler function) pair (as in IOLoop.add_handler)

        self.handlers = {}  # type: Dict[int, Tuple[Union[int, _Selectable], Callable]]

        # Set of fds listening for reads/writes

        self.readers = set()  # type: Set[int]

        self.writers = set()  # type: Set[int]

        self.closing = False

        # If an asyncio loop was closed through an asyncio interface

        # instead of IOLoop.close(), we'd never hear about it and may

        # have left a dangling reference in our map. In case an

        # application (or, more likely, a test suite) creates and

        # destroys a lot of event loops in this way, check here to

        # ensure that we don't have a lot of dead loops building up in

        # the map.

        #

        # TODO(bdarnell): consider making self.asyncio_loop a weakref

        # for AsyncIOMainLoop and make _ioloop_for_asyncio a

        # WeakKeyDictionary.

        for loop in list(IOLoop._ioloop_for_asyncio):

            if loop.is_closed():

                del IOLoop._ioloop_for_asyncio[loop]

        IOLoop._ioloop_for_asyncio[asyncio_loop] = self



        self._thread_identity = 0



        super(BaseAsyncIOLoop, self).initialize(**kwargs)



        def assign_thread_identity() -> None:

            self._thread_identity = get_ident()



        self.add_callback(assign_thread_identity)



    def close(self, all_fds: bool = False) -> None:

        self.closing = True

        for fd in list(self.handlers):

            fileobj, handler_func = self.handlers[fd]

            self.remove_handler(fd)

            if all_fds:

                self.close_fd(fileobj)

        # Remove the mapping before closing the asyncio loop. If this

        # happened in the other order, we could race against another

        # initialize() call which would see the closed asyncio loop,

        # assume it was closed from the asyncio side, and do this

        # cleanup for us, leading to a KeyError.

        del IOLoop._ioloop_for_asyncio[self.asyncio_loop]

        self.asyncio_loop.close()



    def add_handler(

        self, fd: Union[int, _Selectable], handler: Callable[..., None], events: int

    ) -> None:

        fd, fileobj = self.split_fd(fd)

        if fd in self.handlers:

            raise ValueError("fd %s added twice" % fd)

        self.handlers[fd] = (fileobj, handler)

        if events & IOLoop.READ:

            self.asyncio_loop.add_reader(fd, self._handle_events, fd, IOLoop.READ)

            self.readers.add(fd)

        if events & IOLoop.WRITE:

            self.asyncio_loop.add_writer(fd, self._handle_events, fd, IOLoop.WRITE)

            self.writers.add(fd)



    def update_handler(self, fd: Union[int, _Selectable], events: int) -> None:

        fd, fileobj = self.split_fd(fd)

        if events & IOLoop.READ:

            if fd not in self.readers:

                self.asyncio_loop.add_reader(fd, self._handle_events, fd, IOLoop.READ)

                self.readers.add(fd)

        else:

            if fd in self.readers:

                self.asyncio_loop.remove_reader(fd)

                self.readers.remove(fd)

        if events & IOLoop.WRITE:

            if fd not in self.writers:

                self.asyncio_loop.add_writer(fd, self._handle_events, fd, IOLoop.WRITE)

                self.writers.add(fd)

        else:

            if fd in self.writers:

                self.asyncio_loop.remove_writer(fd)

                self.writers.remove(fd)



    def remove_handler(self, fd: Union[int, _Selectable]) -> None:

        fd, fileobj = self.split_fd(fd)

        if fd not in self.handlers:

            return

        if fd in self.readers:

            self.asyncio_loop.remove_reader(fd)

            self.readers.remove(fd)

        if fd in self.writers:

            self.asyncio_loop.remove_writer(fd)

            self.writers.remove(fd)

        del self.handlers[fd]



    def _handle_events(self, fd: int, events: int) -> None:

        fileobj, handler_func = self.handlers[fd]

        handler_func(fileobj, events)



    def start(self) -> None:

        try:

            old_loop = asyncio.get_event_loop()

        except (RuntimeError, AssertionError):

            old_loop = None  # type: ignore

        try:

            self._setup_logging()

            asyncio.set_event_loop(self.asyncio_loop)

            self.asyncio_loop.run_forever()

        finally:

            asyncio.set_event_loop(old_loop)



    def stop(self) -> None:

        self.asyncio_loop.stop()



    def call_at(

        self, when: float, callback: Callable[..., None], *args: Any, **kwargs: Any

    ) -> object:

        # asyncio.call_at supports *args but not **kwargs, so bind them here.

        # We do not synchronize self.time and asyncio_loop.time, so

        # convert from absolute to relative.

        return self.asyncio_loop.call_later(

            max(0, when - self.time()),

            self._run_callback,

            functools.partial(callback, *args, **kwargs),

        )



    def remove_timeout(self, timeout: object) -> None:

        timeout.cancel()  # type: ignore



    def add_callback(self, callback: Callable, *args: Any, **kwargs: Any) -> None:

        if get_ident() == self._thread_identity:

            call_soon = self.asyncio_loop.call_soon

        else:

            call_soon = self.asyncio_loop.call_soon_threadsafe

        try:

            call_soon(self._run_callback, functools.partial(callback, *args, **kwargs))

        except RuntimeError:

            # "Event loop is closed". Swallow the exception for

            # consistency with PollIOLoop (and logical consistency

            # with the fact that we can't guarantee that an

            # add_callback that completes without error will

            # eventually execute).

            pass



    def add_callback_from_signal(

        self, callback: Callable, *args: Any, **kwargs: Any

    ) -> None:

        try:

            self.asyncio_loop.call_soon_threadsafe(

                self._run_callback, functools.partial(callback, *args, **kwargs)

            )

        except RuntimeError:

            pass



    def run_in_executor(

        self,

        executor: Optional[concurrent.futures.Executor],

        func: Callable[..., _T],

        *args: Any

    ) -> Awaitable[_T]:

        return self.asyncio_loop.run_in_executor(executor, func, *args)



    def set_default_executor(self, executor: concurrent.futures.Executor) -> None:

        return self.asyncio_loop.set_default_executor(executor)





class AsyncIOMainLoop(BaseAsyncIOLoop):

    """``AsyncIOMainLoop`` creates an `.IOLoop` that corresponds to the

    current ``asyncio`` event loop (i.e. the one returned by

    ``asyncio.get_event_loop()``).



    .. deprecated:: 5.0



       Now used automatically when appropriate; it is no longer necessary

       to refer to this class directly.



    .. versionchanged:: 5.0



       Closing an `AsyncIOMainLoop` now closes the underlying asyncio loop.

    """



    def initialize(self, **kwargs: Any) -> None:  # type: ignore

        super(AsyncIOMainLoop, self).initialize(asyncio.get_event_loop(), **kwargs)



    def make_current(self) -> None:

        # AsyncIOMainLoop already refers to the current asyncio loop so

        # nothing to do here.

        pass





class AsyncIOLoop(BaseAsyncIOLoop):

    """``AsyncIOLoop`` is an `.IOLoop` that runs on an ``asyncio`` event loop.

    This class follows the usual Tornado semantics for creating new

    ``IOLoops``; these loops are not necessarily related to the

    ``asyncio`` default event loop.



    Each ``AsyncIOLoop`` creates a new ``asyncio.EventLoop``; this object

    can be accessed with the ``asyncio_loop`` attribute.



    .. versionchanged:: 5.0



       When an ``AsyncIOLoop`` becomes the current `.IOLoop`, it also sets

       the current `asyncio` event loop.



    .. deprecated:: 5.0



       Now used automatically when appropriate; it is no longer necessary

       to refer to this class directly.

    """



    def initialize(self, **kwargs: Any) -> None:  # type: ignore

        self.is_current = False

        loop = asyncio.new_event_loop()

        try:

            super(AsyncIOLoop, self).initialize(loop, **kwargs)

        except Exception:

            # If initialize() does not succeed (taking ownership of the loop),

            # we have to close it.

            loop.close()

            raise



    def close(self, all_fds: bool = False) -> None:

        if self.is_current:

            self.clear_current()

        super(AsyncIOLoop, self).close(all_fds=all_fds)



    def make_current(self) -> None:

        if not self.is_current:

            try:

                self.old_asyncio = asyncio.get_event_loop()

            except (RuntimeError, AssertionError):

                self.old_asyncio = None  # type: ignore

            self.is_current = True

        asyncio.set_event_loop(self.asyncio_loop)



    def _clear_current_hook(self) -> None:

        if self.is_current:

            asyncio.set_event_loop(self.old_asyncio)

            self.is_current = False





def to_tornado_future(asyncio_future: asyncio.Future) -> asyncio.Future:

    """Convert an `asyncio.Future` to a `tornado.concurrent.Future`.



    .. versionadded:: 4.1



    .. deprecated:: 5.0

       Tornado ``Futures`` have been merged with `asyncio.Future`,

       so this method is now a no-op.

    """

    return asyncio_future





def to_asyncio_future(tornado_future: asyncio.Future) -> asyncio.Future:

    """Convert a Tornado yieldable object to an `asyncio.Future`.



    .. versionadded:: 4.1



    .. versionchanged:: 4.3

       Now accepts any yieldable object, not just

       `tornado.concurrent.Future`.



    .. deprecated:: 5.0

       Tornado ``Futures`` have been merged with `asyncio.Future`,

       so this method is now equivalent to `tornado.gen.convert_yielded`.

    """

    return convert_yielded(tornado_future)





class AnyThreadEventLoopPolicy(asyncio.DefaultEventLoopPolicy):  # type: ignore

    """Event loop policy that allows loop creation on any thread.



    The default `asyncio` event loop policy only automatically creates

    event loops in the main threads. Other threads must create event

    loops explicitly or `asyncio.get_event_loop` (and therefore

    `.IOLoop.current`) will fail. Installing this policy allows event

    loops to be created automatically on any thread, matching the

    behavior of Tornado versions prior to 5.0 (or 5.0 on Python 2).



    Usage::



        asyncio.set_event_loop_policy(AnyThreadEventLoopPolicy())



    .. versionadded:: 5.0



    """



    def get_event_loop(self) -> asyncio.AbstractEventLoop:

        try:

            return super().get_event_loop()

        except (RuntimeError, AssertionError):

            # This was an AssertionError in python 3.4.2 (which ships with debian jessie)

            # and changed to a RuntimeError in 3.4.3.

            # "There is no current event loop in thread %r"

            loop = self.new_event_loop()

            self.set_event_loop(loop)

            return loop

#

# Copyright 2011 Facebook

#

# Licensed under the Apache License, Version 2.0 (the "License"); you may

# not use this file except in compliance with the License. You may obtain

# a copy of the License at

#

#     http://www.apache.org/licenses/LICENSE-2.0

#

# Unless required by applicable law or agreed to in writing, software

# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT

# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the

# License for the specific language governing permissions and limitations

# under the License.



"""Implementation of platform-specific functionality.



For each function or class described in `tornado.platform.interface`,

the appropriate platform-specific implementation exists in this module.

Most code that needs access to this functionality should do e.g.::



    from tornado.platform.auto import set_close_exec

"""



import os



if os.name == "nt":

    from tornado.platform.windows import set_close_exec

else:

    from tornado.platform.posix import set_close_exec



__all__ = ["set_close_exec"]

import pycares  # type: ignore

import socket



from tornado.concurrent import Future

from tornado import gen

from tornado.ioloop import IOLoop

from tornado.netutil import Resolver, is_valid_ip



import typing



if typing.TYPE_CHECKING:

    from typing import Generator, Any, List, Tuple, Dict  # noqa: F401





class CaresResolver(Resolver):

    """Name resolver based on the c-ares library.



    This is a non-blocking and non-threaded resolver.  It may not produce

    the same results as the system resolver, but can be used for non-blocking

    resolution when threads cannot be used.



    c-ares fails to resolve some names when ``family`` is ``AF_UNSPEC``,

    so it is only recommended for use in ``AF_INET`` (i.e. IPv4).  This is

    the default for ``tornado.simple_httpclient``, but other libraries

    may default to ``AF_UNSPEC``.



    .. versionchanged:: 5.0

       The ``io_loop`` argument (deprecated since version 4.1) has been removed.

    """



    def initialize(self) -> None:

        self.io_loop = IOLoop.current()

        self.channel = pycares.Channel(sock_state_cb=self._sock_state_cb)

        self.fds = {}  # type: Dict[int, int]



    def _sock_state_cb(self, fd: int, readable: bool, writable: bool) -> None:

        state = (IOLoop.READ if readable else 0) | (IOLoop.WRITE if writable else 0)

        if not state:

            self.io_loop.remove_handler(fd)

            del self.fds[fd]

        elif fd in self.fds:

            self.io_loop.update_handler(fd, state)

            self.fds[fd] = state

        else:

            self.io_loop.add_handler(fd, self._handle_events, state)

            self.fds[fd] = state



    def _handle_events(self, fd: int, events: int) -> None:

        read_fd = pycares.ARES_SOCKET_BAD

        write_fd = pycares.ARES_SOCKET_BAD

        if events & IOLoop.READ:

            read_fd = fd

        if events & IOLoop.WRITE:

            write_fd = fd

        self.channel.process_fd(read_fd, write_fd)



    @gen.coroutine

    def resolve(

        self, host: str, port: int, family: int = 0

    ) -> "Generator[Any, Any, List[Tuple[int, Any]]]":

        if is_valid_ip(host):

            addresses = [host]

        else:

            # gethostbyname doesn't take callback as a kwarg

            fut = Future()  # type: Future[Tuple[Any, Any]]

            self.channel.gethostbyname(

                host, family, lambda result, error: fut.set_result((result, error))

            )

            result, error = yield fut

            if error:

                raise IOError(

                    "C-Ares returned error %s: %s while resolving %s"

                    % (error, pycares.errno.strerror(error), host)

                )

            addresses = result.addresses

        addrinfo = []

        for address in addresses:

            if "." in address:

                address_family = socket.AF_INET

            elif ":" in address:

                address_family = socket.AF_INET6

            else:

                address_family = socket.AF_UNSPEC

            if family != socket.AF_UNSPEC and family != address_family:

                raise IOError(

                    "Requested socket family %d but got %d" % (family, address_family)

                )

            addrinfo.append((typing.cast(int, address_family), (address, port)))

        return addrinfo

#

# Copyright 2011 Facebook

#

# Licensed under the Apache License, Version 2.0 (the "License"); you may

# not use this file except in compliance with the License. You may obtain

# a copy of the License at

#

#     http://www.apache.org/licenses/LICENSE-2.0

#

# Unless required by applicable law or agreed to in writing, software

# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT

# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the

# License for the specific language governing permissions and limitations

# under the License.



"""Interfaces for platform-specific functionality.



This module exists primarily for documentation purposes and as base classes

for other tornado.platform modules.  Most code should import the appropriate

implementation from `tornado.platform.auto`.

"""





def set_close_exec(fd: int) -> None:

    """Sets the close-on-exec bit (``FD_CLOEXEC``)for a file descriptor."""

    raise NotImplementedError()

#

# Copyright 2011 Facebook

#

# Licensed under the Apache License, Version 2.0 (the "License"); you may

# not use this file except in compliance with the License. You may obtain

# a copy of the License at

#

#     http://www.apache.org/licenses/LICENSE-2.0

#

# Unless required by applicable law or agreed to in writing, software

# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT

# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the

# License for the specific language governing permissions and limitations

# under the License.



"""Posix implementations of platform-specific functionality."""



import fcntl

import os





def set_close_exec(fd: int) -> None:

    flags = fcntl.fcntl(fd, fcntl.F_GETFD)

    fcntl.fcntl(fd, fcntl.F_SETFD, flags | fcntl.FD_CLOEXEC)





def _set_nonblocking(fd: int) -> None:

    flags = fcntl.fcntl(fd, fcntl.F_GETFL)

    fcntl.fcntl(fd, fcntl.F_SETFL, flags | os.O_NONBLOCK)

# Author: Ovidiu Predescu

# Date: July 2011

#

# Licensed under the Apache License, Version 2.0 (the "License"); you may

# not use this file except in compliance with the License. You may obtain

# a copy of the License at

#

#     http://www.apache.org/licenses/LICENSE-2.0

#

# Unless required by applicable law or agreed to in writing, software

# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT

# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the

# License for the specific language governing permissions and limitations

# under the License.

"""Bridges between the Twisted reactor and Tornado IOLoop.



This module lets you run applications and libraries written for

Twisted in a Tornado application.  It can be used in two modes,

depending on which library's underlying event loop you want to use.



This module has been tested with Twisted versions 11.0.0 and newer.

"""



import socket

import sys



import twisted.internet.abstract  # type: ignore

import twisted.internet.asyncioreactor  # type: ignore

from twisted.internet.defer import Deferred  # type: ignore

from twisted.python import failure  # type: ignore

import twisted.names.cache  # type: ignore

import twisted.names.client  # type: ignore

import twisted.names.hosts  # type: ignore

import twisted.names.resolve  # type: ignore





from tornado.concurrent import Future, future_set_exc_info

from tornado.escape import utf8

from tornado import gen

from tornado.netutil import Resolver



import typing



if typing.TYPE_CHECKING:

    from typing import Generator, Any, List, Tuple  # noqa: F401





class TwistedResolver(Resolver):

    """Twisted-based asynchronous resolver.



    This is a non-blocking and non-threaded resolver.  It is

    recommended only when threads cannot be used, since it has

    limitations compared to the standard ``getaddrinfo``-based

    `~tornado.netutil.Resolver` and

    `~tornado.netutil.DefaultExecutorResolver`.  Specifically, it returns at

    most one result, and arguments other than ``host`` and ``family``

    are ignored.  It may fail to resolve when ``family`` is not

    ``socket.AF_UNSPEC``.



    Requires Twisted 12.1 or newer.



    .. versionchanged:: 5.0

       The ``io_loop`` argument (deprecated since version 4.1) has been removed.

    """



    def initialize(self) -> None:

        # partial copy of twisted.names.client.createResolver, which doesn't

        # allow for a reactor to be passed in.

        self.reactor = twisted.internet.asyncioreactor.AsyncioSelectorReactor()



        host_resolver = twisted.names.hosts.Resolver("/etc/hosts")

        cache_resolver = twisted.names.cache.CacheResolver(reactor=self.reactor)

        real_resolver = twisted.names.client.Resolver(

            "/etc/resolv.conf", reactor=self.reactor

        )

        self.resolver = twisted.names.resolve.ResolverChain(

            [host_resolver, cache_resolver, real_resolver]

        )



    @gen.coroutine

    def resolve(

        self, host: str, port: int, family: int = 0

    ) -> "Generator[Any, Any, List[Tuple[int, Any]]]":

        # getHostByName doesn't accept IP addresses, so if the input

        # looks like an IP address just return it immediately.

        if twisted.internet.abstract.isIPAddress(host):

            resolved = host

            resolved_family = socket.AF_INET

        elif twisted.internet.abstract.isIPv6Address(host):

            resolved = host

            resolved_family = socket.AF_INET6

        else:

            deferred = self.resolver.getHostByName(utf8(host))

            fut = Future()  # type: Future[Any]

            deferred.addBoth(fut.set_result)

            resolved = yield fut

            if isinstance(resolved, failure.Failure):

                try:

                    resolved.raiseException()

                except twisted.names.error.DomainError as e:

                    raise IOError(e)

            elif twisted.internet.abstract.isIPAddress(resolved):

                resolved_family = socket.AF_INET

            elif twisted.internet.abstract.isIPv6Address(resolved):

                resolved_family = socket.AF_INET6

            else:

                resolved_family = socket.AF_UNSPEC

        if family != socket.AF_UNSPEC and family != resolved_family:

            raise Exception(

                "Requested socket family %d but got %d" % (family, resolved_family)

            )

        result = [(typing.cast(int, resolved_family), (resolved, port))]

        return result





if hasattr(gen.convert_yielded, "register"):



    @gen.convert_yielded.register(Deferred)  # type: ignore

    def _(d: Deferred) -> Future:

        f = Future()  # type: Future[Any]



        def errback(failure: failure.Failure) -> None:

            try:

                failure.raiseException()

                # Should never happen, but just in case

                raise Exception("errback called without error")

            except:

                future_set_exc_info(f, sys.exc_info())



        d.addCallbacks(f.set_result, errback)

        return f

# NOTE: win32 support is currently experimental, and not recommended

# for production use.



import ctypes

import ctypes.wintypes



# See: http://msdn.microsoft.com/en-us/library/ms724935(VS.85).aspx

SetHandleInformation = ctypes.windll.kernel32.SetHandleInformation  # type: ignore

SetHandleInformation.argtypes = (

    ctypes.wintypes.HANDLE,

    ctypes.wintypes.DWORD,

    ctypes.wintypes.DWORD,

)

SetHandleInformation.restype = ctypes.wintypes.BOOL



HANDLE_FLAG_INHERIT = 0x00000001





def set_close_exec(fd: int) -> None:

    success = SetHandleInformation(fd, HANDLE_FLAG_INHERIT, 0)

    if not success:

        raise ctypes.WinError()  # type: ignore

"school","école"

# SOME DESCRIPTIVE TITLE.

# Copyright (C) YEAR THE PACKAGE'S COPYRIGHT HOLDER

# This file is distributed under the same license as the PACKAGE package.

# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.

#

#, fuzzy

msgid ""

msgstr ""

"Project-Id-Version: PACKAGE VERSION\n"

"Report-Msgid-Bugs-To: \n"

"POT-Creation-Date: 2015-01-27 11:05+0300\n"

"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"

"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"

"Language-Team: LANGUAGE <LL@li.org>\n"

"Language: \n"

"MIME-Version: 1.0\n"

"Content-Type: text/plain; charset=utf-8\n"

"Content-Transfer-Encoding: 8bit\n"

"Plural-Forms: nplurals=2; plural=(n > 1);\n"



#: extract_me.py:11

msgid "school"

msgstr "école"



#: extract_me.py:12

msgctxt "law"

msgid "right"

msgstr "le droit"



#: extract_me.py:13

msgctxt "good"

msgid "right"

msgstr "le bien"



#: extract_me.py:14

msgctxt "organization"

msgid "club"

msgid_plural "clubs"

msgstr[0] "le club"

msgstr[1] "les clubs"



#: extract_me.py:15

msgctxt "stick"

msgid "club"

msgid_plural "clubs"

msgstr[0] "le bâton"

msgstr[1] "les bâtons"

this is the index

User-agent: *

Disallow: /

<?xml version="1.0"?>

<data>

    <country name="Liechtenstein">

        <rank>1</rank>

        <year>2008</year>

        <gdppc>141100</gdppc>

        <neighbor name="Austria" direction="E"/>

        <neighbor name="Switzerland" direction="W"/>

    </country>

    <country name="Singapore">

        <rank>4</rank>

        <year>2011</year>

        <gdppc>59900</gdppc>

        <neighbor name="Malaysia" direction="N"/>

    </country>

    <country name="Panama">

        <rank>68</rank>

        <year>2011</year>

        <gdppc>13600</gdppc>

        <neighbor name="Costa Rica" direction="W"/>

        <neighbor name="Colombia" direction="E"/>

    </country>

</data>

Héllo

# Licensed under the Apache License, Version 2.0 (the "License"); you may

# not use this file except in compliance with the License. You may obtain

# a copy of the License at

#

#     http://www.apache.org/licenses/LICENSE-2.0

#

# Unless required by applicable law or agreed to in writing, software

# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT

# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the

# License for the specific language governing permissions and limitations

# under the License.



import asyncio

import unittest



from concurrent.futures import ThreadPoolExecutor

from tornado import gen

from tornado.ioloop import IOLoop

from tornado.platform.asyncio import (

    AsyncIOLoop,

    to_asyncio_future,

    AnyThreadEventLoopPolicy,

)

from tornado.testing import AsyncTestCase, gen_test





class AsyncIOLoopTest(AsyncTestCase):

    def get_new_ioloop(self):

        io_loop = AsyncIOLoop()

        return io_loop



    def test_asyncio_callback(self):

        # Basic test that the asyncio loop is set up correctly.

        asyncio.get_event_loop().call_soon(self.stop)

        self.wait()



    @gen_test

    def test_asyncio_future(self):

        # Test that we can yield an asyncio future from a tornado coroutine.

        # Without 'yield from', we must wrap coroutines in ensure_future,

        # which was introduced during Python 3.4, deprecating the prior "async".

        if hasattr(asyncio, "ensure_future"):

            ensure_future = asyncio.ensure_future

        else:

            # async is a reserved word in Python 3.7

            ensure_future = getattr(asyncio, "async")



        x = yield ensure_future(

            asyncio.get_event_loop().run_in_executor(None, lambda: 42)

        )

        self.assertEqual(x, 42)



    @gen_test

    def test_asyncio_yield_from(self):

        @gen.coroutine

        def f():

            event_loop = asyncio.get_event_loop()

            x = yield from event_loop.run_in_executor(None, lambda: 42)

            return x



        result = yield f()

        self.assertEqual(result, 42)



    def test_asyncio_adapter(self):

        # This test demonstrates that when using the asyncio coroutine

        # runner (i.e. run_until_complete), the to_asyncio_future

        # adapter is needed. No adapter is needed in the other direction,

        # as demonstrated by other tests in the package.

        @gen.coroutine

        def tornado_coroutine():

            yield gen.moment

            raise gen.Return(42)



        async def native_coroutine_without_adapter():

            return await tornado_coroutine()



        async def native_coroutine_with_adapter():

            return await to_asyncio_future(tornado_coroutine())



        # Use the adapter, but two degrees from the tornado coroutine.

        async def native_coroutine_with_adapter2():

            return await to_asyncio_future(native_coroutine_without_adapter())



        # Tornado supports native coroutines both with and without adapters

        self.assertEqual(self.io_loop.run_sync(native_coroutine_without_adapter), 42)

        self.assertEqual(self.io_loop.run_sync(native_coroutine_with_adapter), 42)

        self.assertEqual(self.io_loop.run_sync(native_coroutine_with_adapter2), 42)



        # Asyncio only supports coroutines that yield asyncio-compatible

        # Futures (which our Future is since 5.0).

        self.assertEqual(

            asyncio.get_event_loop().run_until_complete(

                native_coroutine_without_adapter()

            ),

            42,

        )

        self.assertEqual(

            asyncio.get_event_loop().run_until_complete(

                native_coroutine_with_adapter()

            ),

            42,

        )

        self.assertEqual(

            asyncio.get_event_loop().run_until_complete(

                native_coroutine_with_adapter2()

            ),

            42,

        )





class LeakTest(unittest.TestCase):

    def setUp(self):

        # Trigger a cleanup of the mapping so we start with a clean slate.

        AsyncIOLoop().close()

        # If we don't clean up after ourselves other tests may fail on

        # py34.

        self.orig_policy = asyncio.get_event_loop_policy()

        asyncio.set_event_loop_policy(asyncio.DefaultEventLoopPolicy())



    def tearDown(self):

        asyncio.get_event_loop().close()

        asyncio.set_event_loop_policy(self.orig_policy)



    def test_ioloop_close_leak(self):

        orig_count = len(IOLoop._ioloop_for_asyncio)

        for i in range(10):

            # Create and close an AsyncIOLoop using Tornado interfaces.

            loop = AsyncIOLoop()

            loop.close()

        new_count = len(IOLoop._ioloop_for_asyncio) - orig_count

        self.assertEqual(new_count, 0)



    def test_asyncio_close_leak(self):

        orig_count = len(IOLoop._ioloop_for_asyncio)

        for i in range(10):

            # Create and close an AsyncIOMainLoop using asyncio interfaces.

            loop = asyncio.new_event_loop()

            loop.call_soon(IOLoop.current)

            loop.call_soon(loop.stop)

            loop.run_forever()

            loop.close()

        new_count = len(IOLoop._ioloop_for_asyncio) - orig_count

        # Because the cleanup is run on new loop creation, we have one

        # dangling entry in the map (but only one).

        self.assertEqual(new_count, 1)





class AnyThreadEventLoopPolicyTest(unittest.TestCase):

    def setUp(self):

        self.orig_policy = asyncio.get_event_loop_policy()

        self.executor = ThreadPoolExecutor(1)



    def tearDown(self):

        asyncio.set_event_loop_policy(self.orig_policy)

        self.executor.shutdown()



    def get_event_loop_on_thread(self):

        def get_and_close_event_loop():

            """Get the event loop. Close it if one is returned.



            Returns the (closed) event loop. This is a silly thing

            to do and leaves the thread in a broken state, but it's

            enough for this test. Closing the loop avoids resource

            leak warnings.

            """

            loop = asyncio.get_event_loop()

            loop.close()

            return loop



        future = self.executor.submit(get_and_close_event_loop)

        return future.result()



    def run_policy_test(self, accessor, expected_type):

        # With the default policy, non-main threads don't get an event

        # loop.

        self.assertRaises(

            (RuntimeError, AssertionError), self.executor.submit(accessor).result

        )

        # Set the policy and we can get a loop.

        asyncio.set_event_loop_policy(AnyThreadEventLoopPolicy())

        self.assertIsInstance(self.executor.submit(accessor).result(), expected_type)

        # Clean up to silence leak warnings. Always use asyncio since

        # IOLoop doesn't (currently) close the underlying loop.

        self.executor.submit(lambda: asyncio.get_event_loop().close()).result()



    def test_asyncio_accessor(self):

        self.run_policy_test(asyncio.get_event_loop, asyncio.AbstractEventLoop)



    def test_tornado_accessor(self):

        self.run_policy_test(IOLoop.current, IOLoop)

# These tests do not currently do much to verify the correct implementation

# of the openid/oauth protocols, they just exercise the major code paths

# and ensure that it doesn't blow up (e.g. with unicode/bytes issues in

# python 3)



import unittest



from tornado.auth import (

    OpenIdMixin,

    OAuthMixin,

    OAuth2Mixin,

    GoogleOAuth2Mixin,

    FacebookGraphMixin,

    TwitterMixin,

)

from tornado.escape import json_decode

from tornado import gen

from tornado.httpclient import HTTPClientError

from tornado.httputil import url_concat

from tornado.log import app_log

from tornado.testing import AsyncHTTPTestCase, ExpectLog

from tornado.web import RequestHandler, Application, HTTPError



try:

    from unittest import mock

except ImportError:

    mock = None  # type: ignore





class OpenIdClientLoginHandler(RequestHandler, OpenIdMixin):

    def initialize(self, test):

        self._OPENID_ENDPOINT = test.get_url("/openid/server/authenticate")



    @gen.coroutine

    def get(self):

        if self.get_argument("openid.mode", None):

            user = yield self.get_authenticated_user(

                http_client=self.settings["http_client"]

            )

            if user is None:

                raise Exception("user is None")

            self.finish(user)

            return

        res = self.authenticate_redirect()

        assert res is None





class OpenIdServerAuthenticateHandler(RequestHandler):

    def post(self):

        if self.get_argument("openid.mode") != "check_authentication":

            raise Exception("incorrect openid.mode %r")

        self.write("is_valid:true")





class OAuth1ClientLoginHandler(RequestHandler, OAuthMixin):

    def initialize(self, test, version):

        self._OAUTH_VERSION = version

        self._OAUTH_REQUEST_TOKEN_URL = test.get_url("/oauth1/server/request_token")

        self._OAUTH_AUTHORIZE_URL = test.get_url("/oauth1/server/authorize")

        self._OAUTH_ACCESS_TOKEN_URL = test.get_url("/oauth1/server/access_token")



    def _oauth_consumer_token(self):

        return dict(key="asdf", secret="qwer")



    @gen.coroutine

    def get(self):

        if self.get_argument("oauth_token", None):

            user = yield self.get_authenticated_user(

                http_client=self.settings["http_client"]

            )

            if user is None:

                raise Exception("user is None")

            self.finish(user)

            return

        yield self.authorize_redirect(http_client=self.settings["http_client"])



    @gen.coroutine

    def _oauth_get_user_future(self, access_token):

        if self.get_argument("fail_in_get_user", None):

            raise Exception("failing in get_user")

        if access_token != dict(key="uiop", secret="5678"):

            raise Exception("incorrect access token %r" % access_token)

        return dict(email="foo@example.com")





class OAuth1ClientLoginCoroutineHandler(OAuth1ClientLoginHandler):

    """Replaces OAuth1ClientLoginCoroutineHandler's get() with a coroutine."""



    @gen.coroutine

    def get(self):

        if self.get_argument("oauth_token", None):

            # Ensure that any exceptions are set on the returned Future,

            # not simply thrown into the surrounding StackContext.

            try:

                yield self.get_authenticated_user()

            except Exception as e:

                self.set_status(503)

                self.write("got exception: %s" % e)

        else:

            yield self.authorize_redirect()





class OAuth1ClientRequestParametersHandler(RequestHandler, OAuthMixin):

    def initialize(self, version):

        self._OAUTH_VERSION = version



    def _oauth_consumer_token(self):

        return dict(key="asdf", secret="qwer")



    def get(self):

        params = self._oauth_request_parameters(

            "http://www.example.com/api/asdf",

            dict(key="uiop", secret="5678"),

            parameters=dict(foo="bar"),

        )

        self.write(params)





class OAuth1ServerRequestTokenHandler(RequestHandler):

    def get(self):

        self.write("oauth_token=zxcv&oauth_token_secret=1234")





class OAuth1ServerAccessTokenHandler(RequestHandler):

    def get(self):

        self.write("oauth_token=uiop&oauth_token_secret=5678")





class OAuth2ClientLoginHandler(RequestHandler, OAuth2Mixin):

    def initialize(self, test):

        self._OAUTH_AUTHORIZE_URL = test.get_url("/oauth2/server/authorize")



    def get(self):

        res = self.authorize_redirect()

        assert res is None





class FacebookClientLoginHandler(RequestHandler, FacebookGraphMixin):

    def initialize(self, test):

        self._OAUTH_AUTHORIZE_URL = test.get_url("/facebook/server/authorize")

        self._OAUTH_ACCESS_TOKEN_URL = test.get_url("/facebook/server/access_token")

        self._FACEBOOK_BASE_URL = test.get_url("/facebook/server")



    @gen.coroutine

    def get(self):

        if self.get_argument("code", None):

            user = yield self.get_authenticated_user(

                redirect_uri=self.request.full_url(),

                client_id=self.settings["facebook_api_key"],

                client_secret=self.settings["facebook_secret"],

                code=self.get_argument("code"),

            )

            self.write(user)

        else:

            yield self.authorize_redirect(

                redirect_uri=self.request.full_url(),

                client_id=self.settings["facebook_api_key"],

                extra_params={"scope": "read_stream,offline_access"},

            )





class FacebookServerAccessTokenHandler(RequestHandler):

    def get(self):

        self.write(dict(access_token="asdf", expires_in=3600))





class FacebookServerMeHandler(RequestHandler):

    def get(self):

        self.write("{}")





class TwitterClientHandler(RequestHandler, TwitterMixin):

    def initialize(self, test):

        self._OAUTH_REQUEST_TOKEN_URL = test.get_url("/oauth1/server/request_token")

        self._OAUTH_ACCESS_TOKEN_URL = test.get_url("/twitter/server/access_token")

        self._OAUTH_AUTHORIZE_URL = test.get_url("/oauth1/server/authorize")

        self._OAUTH_AUTHENTICATE_URL = test.get_url("/twitter/server/authenticate")

        self._TWITTER_BASE_URL = test.get_url("/twitter/api")



    def get_auth_http_client(self):

        return self.settings["http_client"]





class TwitterClientLoginHandler(TwitterClientHandler):

    @gen.coroutine

    def get(self):

        if self.get_argument("oauth_token", None):

            user = yield self.get_authenticated_user()

            if user is None:

                raise Exception("user is None")

            self.finish(user)

            return

        yield self.authorize_redirect()





class TwitterClientAuthenticateHandler(TwitterClientHandler):

    # Like TwitterClientLoginHandler, but uses authenticate_redirect

    # instead of authorize_redirect.

    @gen.coroutine

    def get(self):

        if self.get_argument("oauth_token", None):

            user = yield self.get_authenticated_user()

            if user is None:

                raise Exception("user is None")

            self.finish(user)

            return

        yield self.authenticate_redirect()





class TwitterClientLoginGenCoroutineHandler(TwitterClientHandler):

    @gen.coroutine

    def get(self):

        if self.get_argument("oauth_token", None):

            user = yield self.get_authenticated_user()

            self.finish(user)

        else:

            # New style: with @gen.coroutine the result must be yielded

            # or else the request will be auto-finished too soon.

            yield self.authorize_redirect()





class TwitterClientShowUserHandler(TwitterClientHandler):

    @gen.coroutine

    def get(self):

        # TODO: would be nice to go through the login flow instead of

        # cheating with a hard-coded access token.

        try:

            response = yield self.twitter_request(

                "/users/show/%s" % self.get_argument("name"),

                access_token=dict(key="hjkl", secret="vbnm"),

            )

        except HTTPClientError:

            # TODO(bdarnell): Should we catch HTTP errors and

            # transform some of them (like 403s) into AuthError?

            self.set_status(500)

            self.finish("error from twitter request")

        else:

            self.finish(response)





class TwitterServerAccessTokenHandler(RequestHandler):

    def get(self):

        self.write("oauth_token=hjkl&oauth_token_secret=vbnm&screen_name=foo")





class TwitterServerShowUserHandler(RequestHandler):

    def get(self, screen_name):

        if screen_name == "error":

            raise HTTPError(500)

        assert "oauth_nonce" in self.request.arguments

        assert "oauth_timestamp" in self.request.arguments

        assert "oauth_signature" in self.request.arguments

        assert self.get_argument("oauth_consumer_key") == "test_twitter_consumer_key"

        assert self.get_argument("oauth_signature_method") == "HMAC-SHA1"

        assert self.get_argument("oauth_version") == "1.0"

        assert self.get_argument("oauth_token") == "hjkl"

        self.write(dict(screen_name=screen_name, name=screen_name.capitalize()))





class TwitterServerVerifyCredentialsHandler(RequestHandler):

    def get(self):

        assert "oauth_nonce" in self.request.arguments

        assert "oauth_timestamp" in self.request.arguments

        assert "oauth_signature" in self.request.arguments

        assert self.get_argument("oauth_consumer_key") == "test_twitter_consumer_key"

        assert self.get_argument("oauth_signature_method") == "HMAC-SHA1"

        assert self.get_argument("oauth_version") == "1.0"

        assert self.get_argument("oauth_token") == "hjkl"

        self.write(dict(screen_name="foo", name="Foo"))





class AuthTest(AsyncHTTPTestCase):

    def get_app(self):

        return Application(

            [

                # test endpoints

                ("/openid/client/login", OpenIdClientLoginHandler, dict(test=self)),

                (

                    "/oauth10/client/login",

                    OAuth1ClientLoginHandler,

                    dict(test=self, version="1.0"),

                ),

                (

                    "/oauth10/client/request_params",

                    OAuth1ClientRequestParametersHandler,

                    dict(version="1.0"),

                ),

                (

                    "/oauth10a/client/login",

                    OAuth1ClientLoginHandler,

                    dict(test=self, version="1.0a"),

                ),

                (

                    "/oauth10a/client/login_coroutine",

                    OAuth1ClientLoginCoroutineHandler,

                    dict(test=self, version="1.0a"),

                ),

                (

                    "/oauth10a/client/request_params",

                    OAuth1ClientRequestParametersHandler,

                    dict(version="1.0a"),

                ),

                ("/oauth2/client/login", OAuth2ClientLoginHandler, dict(test=self)),

                ("/facebook/client/login", FacebookClientLoginHandler, dict(test=self)),

                ("/twitter/client/login", TwitterClientLoginHandler, dict(test=self)),

                (

                    "/twitter/client/authenticate",

                    TwitterClientAuthenticateHandler,

                    dict(test=self),

                ),

                (

                    "/twitter/client/login_gen_coroutine",

                    TwitterClientLoginGenCoroutineHandler,

                    dict(test=self),

                ),

                (

                    "/twitter/client/show_user",

                    TwitterClientShowUserHandler,

                    dict(test=self),

                ),

                # simulated servers

                ("/openid/server/authenticate", OpenIdServerAuthenticateHandler),

                ("/oauth1/server/request_token", OAuth1ServerRequestTokenHandler),

                ("/oauth1/server/access_token", OAuth1ServerAccessTokenHandler),

                ("/facebook/server/access_token", FacebookServerAccessTokenHandler),

                ("/facebook/server/me", FacebookServerMeHandler),

                ("/twitter/server/access_token", TwitterServerAccessTokenHandler),

                (r"/twitter/api/users/show/(.*)\.json", TwitterServerShowUserHandler),

                (

                    r"/twitter/api/account/verify_credentials\.json",

                    TwitterServerVerifyCredentialsHandler,

                ),

            ],

            http_client=self.http_client,

            twitter_consumer_key="test_twitter_consumer_key",

            twitter_consumer_secret="test_twitter_consumer_secret",

            facebook_api_key="test_facebook_api_key",

            facebook_secret="test_facebook_secret",

        )



    def test_openid_redirect(self):

        response = self.fetch("/openid/client/login", follow_redirects=False)

        self.assertEqual(response.code, 302)

        self.assertTrue("/openid/server/authenticate?" in response.headers["Location"])



    def test_openid_get_user(self):

        response = self.fetch(

            "/openid/client/login?openid.mode=blah"

            "&openid.ns.ax=http://openid.net/srv/ax/1.0"

            "&openid.ax.type.email=http://axschema.org/contact/email"

            "&openid.ax.value.email=foo@example.com"

        )

        response.rethrow()

        parsed = json_decode(response.body)

        self.assertEqual(parsed["email"], "foo@example.com")



    def test_oauth10_redirect(self):

        response = self.fetch("/oauth10/client/login", follow_redirects=False)

        self.assertEqual(response.code, 302)

        self.assertTrue(

            response.headers["Location"].endswith(

                "/oauth1/server/authorize?oauth_token=zxcv"

            )

        )

        # the cookie is base64('zxcv')|base64('1234')

        self.assertTrue(

            '_oauth_request_token="enhjdg==|MTIzNA=="'

            in response.headers["Set-Cookie"],

            response.headers["Set-Cookie"],

        )



    def test_oauth10_get_user(self):

        response = self.fetch(

            "/oauth10/client/login?oauth_token=zxcv",

            headers={"Cookie": "_oauth_request_token=enhjdg==|MTIzNA=="},

        )

        response.rethrow()

        parsed = json_decode(response.body)

        self.assertEqual(parsed["email"], "foo@example.com")

        self.assertEqual(parsed["access_token"], dict(key="uiop", secret="5678"))



    def test_oauth10_request_parameters(self):

        response = self.fetch("/oauth10/client/request_params")

        response.rethrow()

        parsed = json_decode(response.body)

        self.assertEqual(parsed["oauth_consumer_key"], "asdf")

        self.assertEqual(parsed["oauth_token"], "uiop")

        self.assertTrue("oauth_nonce" in parsed)

        self.assertTrue("oauth_signature" in parsed)



    def test_oauth10a_redirect(self):

        response = self.fetch("/oauth10a/client/login", follow_redirects=False)

        self.assertEqual(response.code, 302)

        self.assertTrue(

            response.headers["Location"].endswith(

                "/oauth1/server/authorize?oauth_token=zxcv"

            )

        )

        # the cookie is base64('zxcv')|base64('1234')

        self.assertTrue(

            '_oauth_request_token="enhjdg==|MTIzNA=="'

            in response.headers["Set-Cookie"],

            response.headers["Set-Cookie"],

        )



    @unittest.skipIf(mock is None, "mock package not present")

    def test_oauth10a_redirect_error(self):

        with mock.patch.object(OAuth1ServerRequestTokenHandler, "get") as get:

            get.side_effect = Exception("boom")

            with ExpectLog(app_log, "Uncaught exception"):

                response = self.fetch("/oauth10a/client/login", follow_redirects=False)

            self.assertEqual(response.code, 500)



    def test_oauth10a_get_user(self):

        response = self.fetch(

            "/oauth10a/client/login?oauth_token=zxcv",

            headers={"Cookie": "_oauth_request_token=enhjdg==|MTIzNA=="},

        )

        response.rethrow()

        parsed = json_decode(response.body)

        self.assertEqual(parsed["email"], "foo@example.com")

        self.assertEqual(parsed["access_token"], dict(key="uiop", secret="5678"))



    def test_oauth10a_request_parameters(self):

        response = self.fetch("/oauth10a/client/request_params")

        response.rethrow()

        parsed = json_decode(response.body)

        self.assertEqual(parsed["oauth_consumer_key"], "asdf")

        self.assertEqual(parsed["oauth_token"], "uiop")

        self.assertTrue("oauth_nonce" in parsed)

        self.assertTrue("oauth_signature" in parsed)



    def test_oauth10a_get_user_coroutine_exception(self):

        response = self.fetch(

            "/oauth10a/client/login_coroutine?oauth_token=zxcv&fail_in_get_user=true",

            headers={"Cookie": "_oauth_request_token=enhjdg==|MTIzNA=="},

        )

        self.assertEqual(response.code, 503)



    def test_oauth2_redirect(self):

        response = self.fetch("/oauth2/client/login", follow_redirects=False)

        self.assertEqual(response.code, 302)

        self.assertTrue("/oauth2/server/authorize?" in response.headers["Location"])



    def test_facebook_login(self):

        response = self.fetch("/facebook/client/login", follow_redirects=False)

        self.assertEqual(response.code, 302)

        self.assertTrue("/facebook/server/authorize?" in response.headers["Location"])

        response = self.fetch(

            "/facebook/client/login?code=1234", follow_redirects=False

        )

        self.assertEqual(response.code, 200)

        user = json_decode(response.body)

        self.assertEqual(user["access_token"], "asdf")

        self.assertEqual(user["session_expires"], "3600")



    def base_twitter_redirect(self, url):

        # Same as test_oauth10a_redirect

        response = self.fetch(url, follow_redirects=False)

        self.assertEqual(response.code, 302)

        self.assertTrue(

            response.headers["Location"].endswith(

                "/oauth1/server/authorize?oauth_token=zxcv"

            )

        )

        # the cookie is base64('zxcv')|base64('1234')

        self.assertTrue(

            '_oauth_request_token="enhjdg==|MTIzNA=="'

            in response.headers["Set-Cookie"],

            response.headers["Set-Cookie"],

        )



    def test_twitter_redirect(self):

        self.base_twitter_redirect("/twitter/client/login")



    def test_twitter_redirect_gen_coroutine(self):

        self.base_twitter_redirect("/twitter/client/login_gen_coroutine")



    def test_twitter_authenticate_redirect(self):

        response = self.fetch("/twitter/client/authenticate", follow_redirects=False)

        self.assertEqual(response.code, 302)

        self.assertTrue(

            response.headers["Location"].endswith(

                "/twitter/server/authenticate?oauth_token=zxcv"

            ),

            response.headers["Location"],

        )

        # the cookie is base64('zxcv')|base64('1234')

        self.assertTrue(

            '_oauth_request_token="enhjdg==|MTIzNA=="'

            in response.headers["Set-Cookie"],

            response.headers["Set-Cookie"],

        )



    def test_twitter_get_user(self):

        response = self.fetch(

            "/twitter/client/login?oauth_token=zxcv",

            headers={"Cookie": "_oauth_request_token=enhjdg==|MTIzNA=="},

        )

        response.rethrow()

        parsed = json_decode(response.body)

        self.assertEqual(

            parsed,

            {

                u"access_token": {

                    u"key": u"hjkl",

                    u"screen_name": u"foo",

                    u"secret": u"vbnm",

                },

                u"name": u"Foo",

                u"screen_name": u"foo",

                u"username": u"foo",

            },

        )



    def test_twitter_show_user(self):

        response = self.fetch("/twitter/client/show_user?name=somebody")

        response.rethrow()

        self.assertEqual(

            json_decode(response.body), {"name": "Somebody", "screen_name": "somebody"}

        )



    def test_twitter_show_user_error(self):

        response = self.fetch("/twitter/client/show_user?name=error")

        self.assertEqual(response.code, 500)

        self.assertEqual(response.body, b"error from twitter request")





class GoogleLoginHandler(RequestHandler, GoogleOAuth2Mixin):

    def initialize(self, test):

        self.test = test

        self._OAUTH_REDIRECT_URI = test.get_url("/client/login")

        self._OAUTH_AUTHORIZE_URL = test.get_url("/google/oauth2/authorize")

        self._OAUTH_ACCESS_TOKEN_URL = test.get_url("/google/oauth2/token")



    @gen.coroutine

    def get(self):

        code = self.get_argument("code", None)

        if code is not None:

            # retrieve authenticate google user

            access = yield self.get_authenticated_user(self._OAUTH_REDIRECT_URI, code)

            user = yield self.oauth2_request(

                self.test.get_url("/google/oauth2/userinfo"),

                access_token=access["access_token"],

            )

            # return the user and access token as json

            user["access_token"] = access["access_token"]

            self.write(user)

        else:

            yield self.authorize_redirect(

                redirect_uri=self._OAUTH_REDIRECT_URI,

                client_id=self.settings["google_oauth"]["key"],

                client_secret=self.settings["google_oauth"]["secret"],

                scope=["profile", "email"],

                response_type="code",

                extra_params={"prompt": "select_account"},

            )





class GoogleOAuth2AuthorizeHandler(RequestHandler):

    def get(self):

        # issue a fake auth code and redirect to redirect_uri

        code = "fake-authorization-code"

        self.redirect(url_concat(self.get_argument("redirect_uri"), dict(code=code)))





class GoogleOAuth2TokenHandler(RequestHandler):

    def post(self):

        assert self.get_argument("code") == "fake-authorization-code"

        # issue a fake token

        self.finish(

            {"access_token": "fake-access-token", "expires_in": "never-expires"}

        )





class GoogleOAuth2UserinfoHandler(RequestHandler):

    def get(self):

        assert self.get_argument("access_token") == "fake-access-token"

        # return a fake user

        self.finish({"name": "Foo", "email": "foo@example.com"})





class GoogleOAuth2Test(AsyncHTTPTestCase):

    def get_app(self):

        return Application(

            [

                # test endpoints

                ("/client/login", GoogleLoginHandler, dict(test=self)),

                # simulated google authorization server endpoints

                ("/google/oauth2/authorize", GoogleOAuth2AuthorizeHandler),

                ("/google/oauth2/token", GoogleOAuth2TokenHandler),

                ("/google/oauth2/userinfo", GoogleOAuth2UserinfoHandler),

            ],

            google_oauth={

                "key": "fake_google_client_id",

                "secret": "fake_google_client_secret",

            },

        )



    def test_google_login(self):

        response = self.fetch("/client/login")

        self.assertDictEqual(

            {

                u"name": u"Foo",

                u"email": u"foo@example.com",

                u"access_token": u"fake-access-token",

            },

            json_decode(response.body),

        )

import os

import shutil

import subprocess

from subprocess import Popen

import sys

from tempfile import mkdtemp

import time

import unittest





class AutoreloadTest(unittest.TestCase):

    def setUp(self):

        self.path = mkdtemp()



    def tearDown(self):

        try:

            shutil.rmtree(self.path)

        except OSError:

            # Windows disallows deleting files that are in use by

            # another process, and even though we've waited for our

            # child process below, it appears that its lock on these

            # files is not guaranteed to be released by this point.

            # Sleep and try again (once).

            time.sleep(1)

            shutil.rmtree(self.path)



    def test_reload_module(self):

        main = """\

import os

import sys



from tornado import autoreload



# This import will fail if path is not set up correctly

import testapp



print('Starting')

if 'TESTAPP_STARTED' not in os.environ:

    os.environ['TESTAPP_STARTED'] = '1'

    sys.stdout.flush()

    autoreload._reload()

"""



        # Create temporary test application

        os.mkdir(os.path.join(self.path, "testapp"))

        open(os.path.join(self.path, "testapp/__init__.py"), "w").close()

        with open(os.path.join(self.path, "testapp/__main__.py"), "w") as f:

            f.write(main)



        # Make sure the tornado module under test is available to the test

        # application

        pythonpath = os.getcwd()

        if "PYTHONPATH" in os.environ:

            pythonpath += os.pathsep + os.environ["PYTHONPATH"]



        p = Popen(

            [sys.executable, "-m", "testapp"],

            stdout=subprocess.PIPE,

            cwd=self.path,

            env=dict(os.environ, PYTHONPATH=pythonpath),

            universal_newlines=True,

        )

        out = p.communicate()[0]

        self.assertEqual(out, "Starting\nStarting\n")



    def test_reload_wrapper_preservation(self):

        # This test verifies that when `python -m tornado.autoreload`

        # is used on an application that also has an internal

        # autoreload, the reload wrapper is preserved on restart.

        main = """\

import os

import sys



# This import will fail if path is not set up correctly

import testapp



if 'tornado.autoreload' not in sys.modules:

    raise Exception('started without autoreload wrapper')



import tornado.autoreload



print('Starting')

sys.stdout.flush()

if 'TESTAPP_STARTED' not in os.environ:

    os.environ['TESTAPP_STARTED'] = '1'

    # Simulate an internal autoreload (one not caused

    # by the wrapper).

    tornado.autoreload._reload()

else:

    # Exit directly so autoreload doesn't catch it.

    os._exit(0)

"""



        # Create temporary test application

        os.mkdir(os.path.join(self.path, "testapp"))

        init_file = os.path.join(self.path, "testapp", "__init__.py")

        open(init_file, "w").close()

        main_file = os.path.join(self.path, "testapp", "__main__.py")

        with open(main_file, "w") as f:

            f.write(main)



        # Make sure the tornado module under test is available to the test

        # application

        pythonpath = os.getcwd()

        if "PYTHONPATH" in os.environ:

            pythonpath += os.pathsep + os.environ["PYTHONPATH"]



        autoreload_proc = Popen(

            [sys.executable, "-m", "tornado.autoreload", "-m", "testapp"],

            stdout=subprocess.PIPE,

            cwd=self.path,

            env=dict(os.environ, PYTHONPATH=pythonpath),

            universal_newlines=True,

        )



        # This timeout needs to be fairly generous for pypy due to jit

        # warmup costs.

        for i in range(40):

            if autoreload_proc.poll() is not None:

                break

            time.sleep(0.1)

        else:

            autoreload_proc.kill()

            raise Exception("subprocess failed to terminate")



        out = autoreload_proc.communicate()[0]

        self.assertEqual(out, "Starting\n" * 2)

#

# Copyright 2012 Facebook

#

# Licensed under the Apache License, Version 2.0 (the "License"); you may

# not use this file except in compliance with the License. You may obtain

# a copy of the License at

#

#     http://www.apache.org/licenses/LICENSE-2.0

#

# Unless required by applicable law or agreed to in writing, software

# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT

# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the

# License for the specific language governing permissions and limitations

# under the License.

from concurrent import futures

import logging

import re

import socket

import unittest



from tornado.concurrent import (

    Future,

    run_on_executor,

    future_set_result_unless_cancelled,

)

from tornado.escape import utf8, to_unicode

from tornado import gen

from tornado.iostream import IOStream

from tornado.tcpserver import TCPServer

from tornado.testing import AsyncTestCase, bind_unused_port, gen_test





class MiscFutureTest(AsyncTestCase):

    def test_future_set_result_unless_cancelled(self):

        fut = Future()  # type: Future[int]

        future_set_result_unless_cancelled(fut, 42)

        self.assertEqual(fut.result(), 42)

        self.assertFalse(fut.cancelled())



        fut = Future()

        fut.cancel()

        is_cancelled = fut.cancelled()

        future_set_result_unless_cancelled(fut, 42)

        self.assertEqual(fut.cancelled(), is_cancelled)

        if not is_cancelled:

            self.assertEqual(fut.result(), 42)





# The following series of classes demonstrate and test various styles

# of use, with and without generators and futures.





class CapServer(TCPServer):

    @gen.coroutine

    def handle_stream(self, stream, address):

        data = yield stream.read_until(b"\n")

        data = to_unicode(data)

        if data == data.upper():

            stream.write(b"error\talready capitalized\n")

        else:

            # data already has \n

            stream.write(utf8("ok\t%s" % data.upper()))

        stream.close()





class CapError(Exception):

    pass





class BaseCapClient(object):

    def __init__(self, port):

        self.port = port



    def process_response(self, data):

        m = re.match("(.*)\t(.*)\n", to_unicode(data))

        if m is None:

            raise Exception("did not match")

        status, message = m.groups()

        if status == "ok":

            return message

        else:

            raise CapError(message)





class GeneratorCapClient(BaseCapClient):

    @gen.coroutine

    def capitalize(self, request_data):

        logging.debug("capitalize")

        stream = IOStream(socket.socket())

        logging.debug("connecting")

        yield stream.connect(("127.0.0.1", self.port))

        stream.write(utf8(request_data + "\n"))

        logging.debug("reading")

        data = yield stream.read_until(b"\n")

        logging.debug("returning")

        stream.close()

        raise gen.Return(self.process_response(data))





class ClientTestMixin(object):

    def setUp(self):

        super(ClientTestMixin, self).setUp()  # type: ignore

        self.server = CapServer()

        sock, port = bind_unused_port()

        self.server.add_sockets([sock])

        self.client = self.client_class(port=port)



    def tearDown(self):

        self.server.stop()

        super(ClientTestMixin, self).tearDown()  # type: ignore



    def test_future(self):

        future = self.client.capitalize("hello")

        self.io_loop.add_future(future, self.stop)

        self.wait()

        self.assertEqual(future.result(), "HELLO")



    def test_future_error(self):

        future = self.client.capitalize("HELLO")

        self.io_loop.add_future(future, self.stop)

        self.wait()

        self.assertRaisesRegexp(CapError, "already capitalized", future.result)



    def test_generator(self):

        @gen.coroutine

        def f():

            result = yield self.client.capitalize("hello")

            self.assertEqual(result, "HELLO")



        self.io_loop.run_sync(f)



    def test_generator_error(self):

        @gen.coroutine

        def f():

            with self.assertRaisesRegexp(CapError, "already capitalized"):

                yield self.client.capitalize("HELLO")



        self.io_loop.run_sync(f)





class GeneratorClientTest(ClientTestMixin, AsyncTestCase):

    client_class = GeneratorCapClient





class RunOnExecutorTest(AsyncTestCase):

    @gen_test

    def test_no_calling(self):

        class Object(object):

            def __init__(self):

                self.executor = futures.thread.ThreadPoolExecutor(1)



            @run_on_executor

            def f(self):

                return 42



        o = Object()

        answer = yield o.f()

        self.assertEqual(answer, 42)



    @gen_test

    def test_call_with_no_args(self):

        class Object(object):

            def __init__(self):

                self.executor = futures.thread.ThreadPoolExecutor(1)



            @run_on_executor()

            def f(self):

                return 42



        o = Object()

        answer = yield o.f()

        self.assertEqual(answer, 42)



    @gen_test

    def test_call_with_executor(self):

        class Object(object):

            def __init__(self):

                self.__executor = futures.thread.ThreadPoolExecutor(1)



            @run_on_executor(executor="_Object__executor")

            def f(self):

                return 42



        o = Object()

        answer = yield o.f()

        self.assertEqual(answer, 42)



    @gen_test

    def test_async_await(self):

        class Object(object):

            def __init__(self):

                self.executor = futures.thread.ThreadPoolExecutor(1)



            @run_on_executor()

            def f(self):

                return 42



        o = Object()



        async def f():

            answer = await o.f()

            return answer



        result = yield f()

        self.assertEqual(result, 42)





if __name__ == "__main__":

    unittest.main()

# coding: utf-8

from hashlib import md5

import unittest



from tornado.escape import utf8

from tornado.testing import AsyncHTTPTestCase

from tornado.test import httpclient_test

from tornado.web import Application, RequestHandler





try:

    import pycurl  # type: ignore

except ImportError:

    pycurl = None



if pycurl is not None:

    from tornado.curl_httpclient import CurlAsyncHTTPClient





@unittest.skipIf(pycurl is None, "pycurl module not present")

class CurlHTTPClientCommonTestCase(httpclient_test.HTTPClientCommonTestCase):

    def get_http_client(self):

        client = CurlAsyncHTTPClient(defaults=dict(allow_ipv6=False))

        # make sure AsyncHTTPClient magic doesn't give us the wrong class

        self.assertTrue(isinstance(client, CurlAsyncHTTPClient))

        return client





class DigestAuthHandler(RequestHandler):

    def initialize(self, username, password):

        self.username = username

        self.password = password



    def get(self):

        realm = "test"

        opaque = "asdf"

        # Real implementations would use a random nonce.

        nonce = "1234"



        auth_header = self.request.headers.get("Authorization", None)

        if auth_header is not None:

            auth_mode, params = auth_header.split(" ", 1)

            assert auth_mode == "Digest"

            param_dict = {}

            for pair in params.split(","):

                k, v = pair.strip().split("=", 1)

                if v[0] == '"' and v[-1] == '"':

                    v = v[1:-1]

                param_dict[k] = v

            assert param_dict["realm"] == realm

            assert param_dict["opaque"] == opaque

            assert param_dict["nonce"] == nonce

            assert param_dict["username"] == self.username

            assert param_dict["uri"] == self.request.path

            h1 = md5(

                utf8("%s:%s:%s" % (self.username, realm, self.password))

            ).hexdigest()

            h2 = md5(

                utf8("%s:%s" % (self.request.method, self.request.path))

            ).hexdigest()

            digest = md5(utf8("%s:%s:%s" % (h1, nonce, h2))).hexdigest()

            if digest == param_dict["response"]:

                self.write("ok")

            else:

                self.write("fail")

        else:

            self.set_status(401)

            self.set_header(

                "WWW-Authenticate",

                'Digest realm="%s", nonce="%s", opaque="%s"' % (realm, nonce, opaque),

            )





class CustomReasonHandler(RequestHandler):

    def get(self):

        self.set_status(200, "Custom reason")





class CustomFailReasonHandler(RequestHandler):

    def get(self):

        self.set_status(400, "Custom reason")





@unittest.skipIf(pycurl is None, "pycurl module not present")

class CurlHTTPClientTestCase(AsyncHTTPTestCase):

    def setUp(self):

        super(CurlHTTPClientTestCase, self).setUp()

        self.http_client = self.create_client()



    def get_app(self):

        return Application(

            [

                ("/digest", DigestAuthHandler, {"username": "foo", "password": "bar"}),

                (

                    "/digest_non_ascii",

                    DigestAuthHandler,

                    {"username": "foo", "password": "barユ£"},

                ),

                ("/custom_reason", CustomReasonHandler),

                ("/custom_fail_reason", CustomFailReasonHandler),

            ]

        )



    def create_client(self, **kwargs):

        return CurlAsyncHTTPClient(

            force_instance=True, defaults=dict(allow_ipv6=False), **kwargs

        )



    def test_digest_auth(self):

        response = self.fetch(

            "/digest", auth_mode="digest", auth_username="foo", auth_password="bar"

        )

        self.assertEqual(response.body, b"ok")



    def test_custom_reason(self):

        response = self.fetch("/custom_reason")

        self.assertEqual(response.reason, "Custom reason")



    def test_fail_custom_reason(self):

        response = self.fetch("/custom_fail_reason")

        self.assertEqual(str(response.error), "HTTP 400: Custom reason")



    def test_digest_auth_non_ascii(self):

        response = self.fetch(

            "/digest_non_ascii",

            auth_mode="digest",

            auth_username="foo",

            auth_password="barユ£",

        )

        self.assertEqual(response.body, b"ok")

import unittest



import tornado.escape

from tornado.escape import (

    utf8,

    xhtml_escape,

    xhtml_unescape,

    url_escape,

    url_unescape,

    to_unicode,

    json_decode,

    json_encode,

    squeeze,

    recursive_unicode,

)

from tornado.util import unicode_type



from typing import List, Tuple, Union, Dict, Any  # noqa: F401



linkify_tests = [

    # (input, linkify_kwargs, expected_output)

    (

        "hello http://world.com/!",

        {},

        u'hello <a href="http://world.com/">http://world.com/</a>!',

    ),

    (

        "hello http://world.com/with?param=true&stuff=yes",

        {},

        u'hello <a href="http://world.com/with?param=true&amp;stuff=yes">http://world.com/with?param=true&amp;stuff=yes</a>',  # noqa: E501

    ),

    # an opened paren followed by many chars killed Gruber's regex

    (

        "http://url.com/w(aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa",

        {},

        u'<a href="http://url.com/w">http://url.com/w</a>(aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',  # noqa: E501

    ),

    # as did too many dots at the end

    (

        "http://url.com/withmany.......................................",

        {},

        u'<a href="http://url.com/withmany">http://url.com/withmany</a>.......................................',  # noqa: E501

    ),

    (

        "http://url.com/withmany((((((((((((((((((((((((((((((((((a)",

        {},

        u'<a href="http://url.com/withmany">http://url.com/withmany</a>((((((((((((((((((((((((((((((((((a)',  # noqa: E501

    ),

    # some examples from http://daringfireball.net/2009/11/liberal_regex_for_matching_urls

    # plus a fex extras (such as multiple parentheses).

    (

        "http://foo.com/blah_blah",

        {},

        u'<a href="http://foo.com/blah_blah">http://foo.com/blah_blah</a>',

    ),

    (

        "http://foo.com/blah_blah/",

        {},

        u'<a href="http://foo.com/blah_blah/">http://foo.com/blah_blah/</a>',

    ),

    (

        "(Something like http://foo.com/blah_blah)",

        {},

        u'(Something like <a href="http://foo.com/blah_blah">http://foo.com/blah_blah</a>)',

    ),

    (

        "http://foo.com/blah_blah_(wikipedia)",

        {},

        u'<a href="http://foo.com/blah_blah_(wikipedia)">http://foo.com/blah_blah_(wikipedia)</a>',

    ),

    (

        "http://foo.com/blah_(blah)_(wikipedia)_blah",

        {},

        u'<a href="http://foo.com/blah_(blah)_(wikipedia)_blah">http://foo.com/blah_(blah)_(wikipedia)_blah</a>',  # noqa: E501

    ),

    (

        "(Something like http://foo.com/blah_blah_(wikipedia))",

        {},

        u'(Something like <a href="http://foo.com/blah_blah_(wikipedia)">http://foo.com/blah_blah_(wikipedia)</a>)',  # noqa: E501

    ),

    (

        "http://foo.com/blah_blah.",

        {},

        u'<a href="http://foo.com/blah_blah">http://foo.com/blah_blah</a>.',

    ),

    (

        "http://foo.com/blah_blah/.",

        {},

        u'<a href="http://foo.com/blah_blah/">http://foo.com/blah_blah/</a>.',

    ),

    (

        "<http://foo.com/blah_blah>",

        {},

        u'&lt;<a href="http://foo.com/blah_blah">http://foo.com/blah_blah</a>&gt;',

    ),

    (

        "<http://foo.com/blah_blah/>",

        {},

        u'&lt;<a href="http://foo.com/blah_blah/">http://foo.com/blah_blah/</a>&gt;',

    ),

    (

        "http://foo.com/blah_blah,",

        {},

        u'<a href="http://foo.com/blah_blah">http://foo.com/blah_blah</a>,',

    ),

    (

        "http://www.example.com/wpstyle/?p=364.",

        {},

        u'<a href="http://www.example.com/wpstyle/?p=364">http://www.example.com/wpstyle/?p=364</a>.',  # noqa: E501

    ),

    (

        "rdar://1234",

        {"permitted_protocols": ["http", "rdar"]},

        u'<a href="rdar://1234">rdar://1234</a>',

    ),

    (

        "rdar:/1234",

        {"permitted_protocols": ["rdar"]},

        u'<a href="rdar:/1234">rdar:/1234</a>',

    ),

    (

        "http://userid:password@example.com:8080",

        {},

        u'<a href="http://userid:password@example.com:8080">http://userid:password@example.com:8080</a>',  # noqa: E501

    ),

    (

        "http://userid@example.com",

        {},

        u'<a href="http://userid@example.com">http://userid@example.com</a>',

    ),

    (

        "http://userid@example.com:8080",

        {},

        u'<a href="http://userid@example.com:8080">http://userid@example.com:8080</a>',

    ),

    (

        "http://userid:password@example.com",

        {},

        u'<a href="http://userid:password@example.com">http://userid:password@example.com</a>',

    ),

    (

        "message://%3c330e7f8409726r6a4ba78dkf1fd71420c1bf6ff@mail.gmail.com%3e",

        {"permitted_protocols": ["http", "message"]},

        u'<a href="message://%3c330e7f8409726r6a4ba78dkf1fd71420c1bf6ff@mail.gmail.com%3e">'

        u"message://%3c330e7f8409726r6a4ba78dkf1fd71420c1bf6ff@mail.gmail.com%3e</a>",

    ),

    (

        u"http://\u27a1.ws/\u4a39",

        {},

        u'<a href="http://\u27a1.ws/\u4a39">http://\u27a1.ws/\u4a39</a>',

    ),

    (

        "<tag>http://example.com</tag>",

        {},

        u'&lt;tag&gt;<a href="http://example.com">http://example.com</a>&lt;/tag&gt;',

    ),

    (

        "Just a www.example.com link.",

        {},

        u'Just a <a href="http://www.example.com">www.example.com</a> link.',

    ),

    (

        "Just a www.example.com link.",

        {"require_protocol": True},

        u"Just a www.example.com link.",

    ),

    (

        "A http://reallylong.com/link/that/exceedsthelenglimit.html",

        {"require_protocol": True, "shorten": True},

        u'A <a href="http://reallylong.com/link/that/exceedsthelenglimit.html"'

        u' title="http://reallylong.com/link/that/exceedsthelenglimit.html">http://reallylong.com/link...</a>',  # noqa: E501

    ),

    (

        "A http://reallylongdomainnamethatwillbetoolong.com/hi!",

        {"shorten": True},

        u'A <a href="http://reallylongdomainnamethatwillbetoolong.com/hi"'

        u' title="http://reallylongdomainnamethatwillbetoolong.com/hi">http://reallylongdomainnametha...</a>!',  # noqa: E501

    ),

    (

        "A file:///passwords.txt and http://web.com link",

        {},

        u'A file:///passwords.txt and <a href="http://web.com">http://web.com</a> link',

    ),

    (

        "A file:///passwords.txt and http://web.com link",

        {"permitted_protocols": ["file"]},

        u'A <a href="file:///passwords.txt">file:///passwords.txt</a> and http://web.com link',

    ),

    (

        "www.external-link.com",

        {"extra_params": 'rel="nofollow" class="external"'},

        u'<a href="http://www.external-link.com" rel="nofollow" class="external">www.external-link.com</a>',  # noqa: E501

    ),

    (

        "www.external-link.com and www.internal-link.com/blogs extra",

        {

            "extra_params": lambda href: 'class="internal"'

            if href.startswith("http://www.internal-link.com")

            else 'rel="nofollow" class="external"'

        },

        u'<a href="http://www.external-link.com" rel="nofollow" class="external">www.external-link.com</a>'  # noqa: E501

        u' and <a href="http://www.internal-link.com/blogs" class="internal">www.internal-link.com/blogs</a> extra',  # noqa: E501

    ),

    (

        "www.external-link.com",

        {"extra_params": lambda href: '    rel="nofollow" class="external"  '},

        u'<a href="http://www.external-link.com" rel="nofollow" class="external">www.external-link.com</a>',  # noqa: E501

    ),

]  # type: List[Tuple[Union[str, bytes], Dict[str, Any], str]]





class EscapeTestCase(unittest.TestCase):

    def test_linkify(self):

        for text, kwargs, html in linkify_tests:

            linked = tornado.escape.linkify(text, **kwargs)

            self.assertEqual(linked, html)



    def test_xhtml_escape(self):

        tests = [

            ("<foo>", "&lt;foo&gt;"),

            (u"<foo>", u"&lt;foo&gt;"),

            (b"<foo>", b"&lt;foo&gt;"),

            ("<>&\"'", "&lt;&gt;&amp;&quot;&#39;"),

            ("&amp;", "&amp;amp;"),

            (u"<\u00e9>", u"&lt;\u00e9&gt;"),

            (b"<\xc3\xa9>", b"&lt;\xc3\xa9&gt;"),

        ]  # type: List[Tuple[Union[str, bytes], Union[str, bytes]]]

        for unescaped, escaped in tests:

            self.assertEqual(utf8(xhtml_escape(unescaped)), utf8(escaped))

            self.assertEqual(utf8(unescaped), utf8(xhtml_unescape(escaped)))



    def test_xhtml_unescape_numeric(self):

        tests = [

            ("foo&#32;bar", "foo bar"),

            ("foo&#x20;bar", "foo bar"),

            ("foo&#X20;bar", "foo bar"),

            ("foo&#xabc;bar", u"foo\u0abcbar"),

            ("foo&#xyz;bar", "foo&#xyz;bar"),  # invalid encoding

            ("foo&#;bar", "foo&#;bar"),  # invalid encoding

            ("foo&#x;bar", "foo&#x;bar"),  # invalid encoding

        ]

        for escaped, unescaped in tests:

            self.assertEqual(unescaped, xhtml_unescape(escaped))



    def test_url_escape_unicode(self):

        tests = [

            # byte strings are passed through as-is

            (u"\u00e9".encode("utf8"), "%C3%A9"),

            (u"\u00e9".encode("latin1"), "%E9"),

            # unicode strings become utf8

            (u"\u00e9", "%C3%A9"),

        ]  # type: List[Tuple[Union[str, bytes], str]]

        for unescaped, escaped in tests:

            self.assertEqual(url_escape(unescaped), escaped)



    def test_url_unescape_unicode(self):

        tests = [

            ("%C3%A9", u"\u00e9", "utf8"),

            ("%C3%A9", u"\u00c3\u00a9", "latin1"),

            ("%C3%A9", utf8(u"\u00e9"), None),

        ]

        for escaped, unescaped, encoding in tests:

            # input strings to url_unescape should only contain ascii

            # characters, but make sure the function accepts both byte

            # and unicode strings.

            self.assertEqual(url_unescape(to_unicode(escaped), encoding), unescaped)

            self.assertEqual(url_unescape(utf8(escaped), encoding), unescaped)



    def test_url_escape_quote_plus(self):

        unescaped = "+ #%"

        plus_escaped = "%2B+%23%25"

        escaped = "%2B%20%23%25"

        self.assertEqual(url_escape(unescaped), plus_escaped)

        self.assertEqual(url_escape(unescaped, plus=False), escaped)

        self.assertEqual(url_unescape(plus_escaped), unescaped)

        self.assertEqual(url_unescape(escaped, plus=False), unescaped)

        self.assertEqual(url_unescape(plus_escaped, encoding=None), utf8(unescaped))

        self.assertEqual(

            url_unescape(escaped, encoding=None, plus=False), utf8(unescaped)

        )



    def test_escape_return_types(self):

        # On python2 the escape methods should generally return the same

        # type as their argument

        self.assertEqual(type(xhtml_escape("foo")), str)

        self.assertEqual(type(xhtml_escape(u"foo")), unicode_type)



    def test_json_decode(self):

        # json_decode accepts both bytes and unicode, but strings it returns

        # are always unicode.

        self.assertEqual(json_decode(b'"foo"'), u"foo")

        self.assertEqual(json_decode(u'"foo"'), u"foo")



        # Non-ascii bytes are interpreted as utf8

        self.assertEqual(json_decode(utf8(u'"\u00e9"')), u"\u00e9")



    def test_json_encode(self):

        # json deals with strings, not bytes.  On python 2 byte strings will

        # convert automatically if they are utf8; on python 3 byte strings

        # are not allowed.

        self.assertEqual(json_decode(json_encode(u"\u00e9")), u"\u00e9")

        if bytes is str:

            self.assertEqual(json_decode(json_encode(utf8(u"\u00e9"))), u"\u00e9")

            self.assertRaises(UnicodeDecodeError, json_encode, b"\xe9")



    def test_squeeze(self):

        self.assertEqual(

            squeeze(u"sequences     of    whitespace   chars"),

            u"sequences of whitespace chars",

        )



    def test_recursive_unicode(self):

        tests = {

            "dict": {b"foo": b"bar"},

            "list": [b"foo", b"bar"],

            "tuple": (b"foo", b"bar"),

            "bytes": b"foo",

        }

        self.assertEqual(recursive_unicode(tests["dict"]), {u"foo": u"bar"})

        self.assertEqual(recursive_unicode(tests["list"]), [u"foo", u"bar"])

        self.assertEqual(recursive_unicode(tests["tuple"]), (u"foo", u"bar"))

        self.assertEqual(recursive_unicode(tests["bytes"]), u"foo")

import asyncio

from concurrent import futures

import gc

import datetime

import platform

import sys

import time

import weakref

import unittest



from tornado.concurrent import Future

from tornado.log import app_log

from tornado.testing import AsyncHTTPTestCase, AsyncTestCase, ExpectLog, gen_test

from tornado.test.util import skipOnTravis, skipNotCPython

from tornado.web import Application, RequestHandler, HTTPError



from tornado import gen



try:

    import contextvars

except ImportError:

    contextvars = None  # type: ignore



import typing



if typing.TYPE_CHECKING:

    from typing import List, Optional  # noqa: F401





class GenBasicTest(AsyncTestCase):

    @gen.coroutine

    def delay(self, iterations, arg):

        """Returns arg after a number of IOLoop iterations."""

        for i in range(iterations):

            yield gen.moment

        raise gen.Return(arg)



    @gen.coroutine

    def async_future(self, result):

        yield gen.moment

        return result



    @gen.coroutine

    def async_exception(self, e):

        yield gen.moment

        raise e



    @gen.coroutine

    def add_one_async(self, x):

        yield gen.moment

        raise gen.Return(x + 1)



    def test_no_yield(self):

        @gen.coroutine

        def f():

            pass



        self.io_loop.run_sync(f)



    def test_exception_phase1(self):

        @gen.coroutine

        def f():

            1 / 0



        self.assertRaises(ZeroDivisionError, self.io_loop.run_sync, f)



    def test_exception_phase2(self):

        @gen.coroutine

        def f():

            yield gen.moment

            1 / 0



        self.assertRaises(ZeroDivisionError, self.io_loop.run_sync, f)



    def test_bogus_yield(self):

        @gen.coroutine

        def f():

            yield 42



        self.assertRaises(gen.BadYieldError, self.io_loop.run_sync, f)



    def test_bogus_yield_tuple(self):

        @gen.coroutine

        def f():

            yield (1, 2)



        self.assertRaises(gen.BadYieldError, self.io_loop.run_sync, f)



    def test_reuse(self):

        @gen.coroutine

        def f():

            yield gen.moment



        self.io_loop.run_sync(f)

        self.io_loop.run_sync(f)



    def test_none(self):

        @gen.coroutine

        def f():

            yield None



        self.io_loop.run_sync(f)



    def test_multi(self):

        @gen.coroutine

        def f():

            results = yield [self.add_one_async(1), self.add_one_async(2)]

            self.assertEqual(results, [2, 3])



        self.io_loop.run_sync(f)



    def test_multi_dict(self):

        @gen.coroutine

        def f():

            results = yield dict(foo=self.add_one_async(1), bar=self.add_one_async(2))

            self.assertEqual(results, dict(foo=2, bar=3))



        self.io_loop.run_sync(f)



    def test_multi_delayed(self):

        @gen.coroutine

        def f():

            # callbacks run at different times

            responses = yield gen.multi_future(

                [self.delay(3, "v1"), self.delay(1, "v2")]

            )

            self.assertEqual(responses, ["v1", "v2"])



        self.io_loop.run_sync(f)



    def test_multi_dict_delayed(self):

        @gen.coroutine

        def f():

            # callbacks run at different times

            responses = yield gen.multi_future(

                dict(foo=self.delay(3, "v1"), bar=self.delay(1, "v2"))

            )

            self.assertEqual(responses, dict(foo="v1", bar="v2"))



        self.io_loop.run_sync(f)



    @skipOnTravis

    @gen_test

    def test_multi_performance(self):

        # Yielding a list used to have quadratic performance; make

        # sure a large list stays reasonable.  On my laptop a list of

        # 2000 used to take 1.8s, now it takes 0.12.

        start = time.time()

        yield [gen.moment for i in range(2000)]

        end = time.time()

        self.assertLess(end - start, 1.0)



    @gen_test

    def test_multi_empty(self):

        # Empty lists or dicts should return the same type.

        x = yield []

        self.assertTrue(isinstance(x, list))

        y = yield {}

        self.assertTrue(isinstance(y, dict))



    @gen_test

    def test_future(self):

        result = yield self.async_future(1)

        self.assertEqual(result, 1)



    @gen_test

    def test_multi_future(self):

        results = yield [self.async_future(1), self.async_future(2)]

        self.assertEqual(results, [1, 2])



    @gen_test

    def test_multi_future_duplicate(self):

        # Note that this doesn't work with native corotines, only with

        # decorated coroutines.

        f = self.async_future(2)

        results = yield [self.async_future(1), f, self.async_future(3), f]

        self.assertEqual(results, [1, 2, 3, 2])



    @gen_test

    def test_multi_dict_future(self):

        results = yield dict(foo=self.async_future(1), bar=self.async_future(2))

        self.assertEqual(results, dict(foo=1, bar=2))



    @gen_test

    def test_multi_exceptions(self):

        with ExpectLog(app_log, "Multiple exceptions in yield list"):

            with self.assertRaises(RuntimeError) as cm:

                yield gen.Multi(

                    [

                        self.async_exception(RuntimeError("error 1")),

                        self.async_exception(RuntimeError("error 2")),

                    ]

                )

        self.assertEqual(str(cm.exception), "error 1")



        # With only one exception, no error is logged.

        with self.assertRaises(RuntimeError):

            yield gen.Multi(

                [self.async_exception(RuntimeError("error 1")), self.async_future(2)]

            )



        # Exception logging may be explicitly quieted.

        with self.assertRaises(RuntimeError):

            yield gen.Multi(

                [

                    self.async_exception(RuntimeError("error 1")),

                    self.async_exception(RuntimeError("error 2")),

                ],

                quiet_exceptions=RuntimeError,

            )



    @gen_test

    def test_multi_future_exceptions(self):

        with ExpectLog(app_log, "Multiple exceptions in yield list"):

            with self.assertRaises(RuntimeError) as cm:

                yield [

                    self.async_exception(RuntimeError("error 1")),

                    self.async_exception(RuntimeError("error 2")),

                ]

        self.assertEqual(str(cm.exception), "error 1")



        # With only one exception, no error is logged.

        with self.assertRaises(RuntimeError):

            yield [self.async_exception(RuntimeError("error 1")), self.async_future(2)]



        # Exception logging may be explicitly quieted.

        with self.assertRaises(RuntimeError):

            yield gen.multi_future(

                [

                    self.async_exception(RuntimeError("error 1")),

                    self.async_exception(RuntimeError("error 2")),

                ],

                quiet_exceptions=RuntimeError,

            )



    def test_sync_raise_return(self):

        @gen.coroutine

        def f():

            raise gen.Return()



        self.io_loop.run_sync(f)



    def test_async_raise_return(self):

        @gen.coroutine

        def f():

            yield gen.moment

            raise gen.Return()



        self.io_loop.run_sync(f)



    def test_sync_raise_return_value(self):

        @gen.coroutine

        def f():

            raise gen.Return(42)



        self.assertEqual(42, self.io_loop.run_sync(f))



    def test_sync_raise_return_value_tuple(self):

        @gen.coroutine

        def f():

            raise gen.Return((1, 2))



        self.assertEqual((1, 2), self.io_loop.run_sync(f))



    def test_async_raise_return_value(self):

        @gen.coroutine

        def f():

            yield gen.moment

            raise gen.Return(42)



        self.assertEqual(42, self.io_loop.run_sync(f))



    def test_async_raise_return_value_tuple(self):

        @gen.coroutine

        def f():

            yield gen.moment

            raise gen.Return((1, 2))



        self.assertEqual((1, 2), self.io_loop.run_sync(f))





class GenCoroutineTest(AsyncTestCase):

    def setUp(self):

        # Stray StopIteration exceptions can lead to tests exiting prematurely,

        # so we need explicit checks here to make sure the tests run all

        # the way through.

        self.finished = False

        super(GenCoroutineTest, self).setUp()



    def tearDown(self):

        super(GenCoroutineTest, self).tearDown()

        assert self.finished



    def test_attributes(self):

        self.finished = True



        def f():

            yield gen.moment



        coro = gen.coroutine(f)

        self.assertEqual(coro.__name__, f.__name__)

        self.assertEqual(coro.__module__, f.__module__)

        self.assertIs(coro.__wrapped__, f)  # type: ignore



    def test_is_coroutine_function(self):

        self.finished = True



        def f():

            yield gen.moment



        coro = gen.coroutine(f)

        self.assertFalse(gen.is_coroutine_function(f))

        self.assertTrue(gen.is_coroutine_function(coro))

        self.assertFalse(gen.is_coroutine_function(coro()))



    @gen_test

    def test_sync_gen_return(self):

        @gen.coroutine

        def f():

            raise gen.Return(42)



        result = yield f()

        self.assertEqual(result, 42)

        self.finished = True



    @gen_test

    def test_async_gen_return(self):

        @gen.coroutine

        def f():

            yield gen.moment

            raise gen.Return(42)



        result = yield f()

        self.assertEqual(result, 42)

        self.finished = True



    @gen_test

    def test_sync_return(self):

        @gen.coroutine

        def f():

            return 42



        result = yield f()

        self.assertEqual(result, 42)

        self.finished = True



    @gen_test

    def test_async_return(self):

        @gen.coroutine

        def f():

            yield gen.moment

            return 42



        result = yield f()

        self.assertEqual(result, 42)

        self.finished = True



    @gen_test

    def test_async_early_return(self):

        # A yield statement exists but is not executed, which means

        # this function "returns" via an exception.  This exception

        # doesn't happen before the exception handling is set up.

        @gen.coroutine

        def f():

            if True:

                return 42

            yield gen.Task(self.io_loop.add_callback)



        result = yield f()

        self.assertEqual(result, 42)

        self.finished = True



    @gen_test

    def test_async_await(self):

        @gen.coroutine

        def f1():

            yield gen.moment

            raise gen.Return(42)



        # This test verifies that an async function can await a

        # yield-based gen.coroutine, and that a gen.coroutine

        # (the test method itself) can yield an async function.

        async def f2():

            result = await f1()

            return result



        result = yield f2()

        self.assertEqual(result, 42)

        self.finished = True



    @gen_test

    def test_asyncio_sleep_zero(self):

        # asyncio.sleep(0) turns into a special case (equivalent to

        # `yield None`)

        async def f():

            import asyncio



            await asyncio.sleep(0)

            return 42



        result = yield f()

        self.assertEqual(result, 42)

        self.finished = True



    @gen_test

    def test_async_await_mixed_multi_native_future(self):

        @gen.coroutine

        def f1():

            yield gen.moment



        async def f2():

            await f1()

            return 42



        @gen.coroutine

        def f3():

            yield gen.moment

            raise gen.Return(43)



        results = yield [f2(), f3()]

        self.assertEqual(results, [42, 43])

        self.finished = True



    @gen_test

    def test_async_with_timeout(self):

        async def f1():

            return 42



        result = yield gen.with_timeout(datetime.timedelta(hours=1), f1())

        self.assertEqual(result, 42)

        self.finished = True



    @gen_test

    def test_sync_return_no_value(self):

        @gen.coroutine

        def f():

            return



        result = yield f()

        self.assertEqual(result, None)

        self.finished = True



    @gen_test

    def test_async_return_no_value(self):

        # Without a return value we don't need python 3.3.

        @gen.coroutine

        def f():

            yield gen.moment

            return



        result = yield f()

        self.assertEqual(result, None)

        self.finished = True



    @gen_test

    def test_sync_raise(self):

        @gen.coroutine

        def f():

            1 / 0



        # The exception is raised when the future is yielded

        # (or equivalently when its result method is called),

        # not when the function itself is called).

        future = f()

        with self.assertRaises(ZeroDivisionError):

            yield future

        self.finished = True



    @gen_test

    def test_async_raise(self):

        @gen.coroutine

        def f():

            yield gen.moment

            1 / 0



        future = f()

        with self.assertRaises(ZeroDivisionError):

            yield future

        self.finished = True



    @gen_test

    def test_replace_yieldpoint_exception(self):

        # Test exception handling: a coroutine can catch one exception

        # raised by a yield point and raise a different one.

        @gen.coroutine

        def f1():

            1 / 0



        @gen.coroutine

        def f2():

            try:

                yield f1()

            except ZeroDivisionError:

                raise KeyError()



        future = f2()

        with self.assertRaises(KeyError):

            yield future

        self.finished = True



    @gen_test

    def test_swallow_yieldpoint_exception(self):

        # Test exception handling: a coroutine can catch an exception

        # raised by a yield point and not raise a different one.

        @gen.coroutine

        def f1():

            1 / 0



        @gen.coroutine

        def f2():

            try:

                yield f1()

            except ZeroDivisionError:

                raise gen.Return(42)



        result = yield f2()

        self.assertEqual(result, 42)

        self.finished = True



    @gen_test

    def test_moment(self):

        calls = []



        @gen.coroutine

        def f(name, yieldable):

            for i in range(5):

                calls.append(name)

                yield yieldable



        # First, confirm the behavior without moment: each coroutine

        # monopolizes the event loop until it finishes.

        immediate = Future()  # type: Future[None]

        immediate.set_result(None)

        yield [f("a", immediate), f("b", immediate)]

        self.assertEqual("".join(calls), "aaaaabbbbb")



        # With moment, they take turns.

        calls = []

        yield [f("a", gen.moment), f("b", gen.moment)]

        self.assertEqual("".join(calls), "ababababab")

        self.finished = True



        calls = []

        yield [f("a", gen.moment), f("b", immediate)]

        self.assertEqual("".join(calls), "abbbbbaaaa")



    @gen_test

    def test_sleep(self):

        yield gen.sleep(0.01)

        self.finished = True



    @gen_test

    def test_py3_leak_exception_context(self):

        class LeakedException(Exception):

            pass



        @gen.coroutine

        def inner(iteration):

            raise LeakedException(iteration)



        try:

            yield inner(1)

        except LeakedException as e:

            self.assertEqual(str(e), "1")

            self.assertIsNone(e.__context__)



        try:

            yield inner(2)

        except LeakedException as e:

            self.assertEqual(str(e), "2")

            self.assertIsNone(e.__context__)



        self.finished = True



    @skipNotCPython

    @unittest.skipIf(

        (3,) < sys.version_info < (3, 6), "asyncio.Future has reference cycles"

    )

    def test_coroutine_refcounting(self):

        # On CPython, tasks and their arguments should be released immediately

        # without waiting for garbage collection.

        @gen.coroutine

        def inner():

            class Foo(object):

                pass



            local_var = Foo()

            self.local_ref = weakref.ref(local_var)



            def dummy():

                pass



            yield gen.coroutine(dummy)()

            raise ValueError("Some error")



        @gen.coroutine

        def inner2():

            try:

                yield inner()

            except ValueError:

                pass



        self.io_loop.run_sync(inner2, timeout=3)



        self.assertIs(self.local_ref(), None)

        self.finished = True



    def test_asyncio_future_debug_info(self):

        self.finished = True

        # Enable debug mode

        asyncio_loop = asyncio.get_event_loop()

        self.addCleanup(asyncio_loop.set_debug, asyncio_loop.get_debug())

        asyncio_loop.set_debug(True)



        def f():

            yield gen.moment



        coro = gen.coroutine(f)()

        self.assertIsInstance(coro, asyncio.Future)

        # We expect the coroutine repr() to show the place where

        # it was instantiated

        expected = "created at %s:%d" % (__file__, f.__code__.co_firstlineno + 3)

        actual = repr(coro)

        self.assertIn(expected, actual)



    @gen_test

    def test_asyncio_gather(self):

        # This demonstrates that tornado coroutines can be understood

        # by asyncio (This failed prior to Tornado 5.0).

        @gen.coroutine

        def f():

            yield gen.moment

            raise gen.Return(1)



        ret = yield asyncio.gather(f(), f())

        self.assertEqual(ret, [1, 1])

        self.finished = True





class GenCoroutineSequenceHandler(RequestHandler):

    @gen.coroutine

    def get(self):

        yield gen.moment

        self.write("1")

        yield gen.moment

        self.write("2")

        yield gen.moment

        self.finish("3")





class GenCoroutineUnfinishedSequenceHandler(RequestHandler):

    @gen.coroutine

    def get(self):

        yield gen.moment

        self.write("1")

        yield gen.moment

        self.write("2")

        yield gen.moment

        # just write, don't finish

        self.write("3")





# "Undecorated" here refers to the absence of @asynchronous.

class UndecoratedCoroutinesHandler(RequestHandler):

    @gen.coroutine

    def prepare(self):

        self.chunks = []  # type: List[str]

        yield gen.moment

        self.chunks.append("1")



    @gen.coroutine

    def get(self):

        self.chunks.append("2")

        yield gen.moment

        self.chunks.append("3")

        yield gen.moment

        self.write("".join(self.chunks))





class AsyncPrepareErrorHandler(RequestHandler):

    @gen.coroutine

    def prepare(self):

        yield gen.moment

        raise HTTPError(403)



    def get(self):

        self.finish("ok")





class NativeCoroutineHandler(RequestHandler):

    async def get(self):

        await asyncio.sleep(0)

        self.write("ok")





class GenWebTest(AsyncHTTPTestCase):

    def get_app(self):

        return Application(

            [

                ("/coroutine_sequence", GenCoroutineSequenceHandler),

                (

                    "/coroutine_unfinished_sequence",

                    GenCoroutineUnfinishedSequenceHandler,

                ),

                ("/undecorated_coroutine", UndecoratedCoroutinesHandler),

                ("/async_prepare_error", AsyncPrepareErrorHandler),

                ("/native_coroutine", NativeCoroutineHandler),

            ]

        )



    def test_coroutine_sequence_handler(self):

        response = self.fetch("/coroutine_sequence")

        self.assertEqual(response.body, b"123")



    def test_coroutine_unfinished_sequence_handler(self):

        response = self.fetch("/coroutine_unfinished_sequence")

        self.assertEqual(response.body, b"123")



    def test_undecorated_coroutines(self):

        response = self.fetch("/undecorated_coroutine")

        self.assertEqual(response.body, b"123")



    def test_async_prepare_error_handler(self):

        response = self.fetch("/async_prepare_error")

        self.assertEqual(response.code, 403)



    def test_native_coroutine_handler(self):

        response = self.fetch("/native_coroutine")

        self.assertEqual(response.code, 200)

        self.assertEqual(response.body, b"ok")





class WithTimeoutTest(AsyncTestCase):

    @gen_test

    def test_timeout(self):

        with self.assertRaises(gen.TimeoutError):

            yield gen.with_timeout(datetime.timedelta(seconds=0.1), Future())



    @gen_test

    def test_completes_before_timeout(self):

        future = Future()  # type: Future[str]

        self.io_loop.add_timeout(

            datetime.timedelta(seconds=0.1), lambda: future.set_result("asdf")

        )

        result = yield gen.with_timeout(datetime.timedelta(seconds=3600), future)

        self.assertEqual(result, "asdf")



    @gen_test

    def test_fails_before_timeout(self):

        future = Future()  # type: Future[str]

        self.io_loop.add_timeout(

            datetime.timedelta(seconds=0.1),

            lambda: future.set_exception(ZeroDivisionError()),

        )

        with self.assertRaises(ZeroDivisionError):

            yield gen.with_timeout(datetime.timedelta(seconds=3600), future)



    @gen_test

    def test_already_resolved(self):

        future = Future()  # type: Future[str]

        future.set_result("asdf")

        result = yield gen.with_timeout(datetime.timedelta(seconds=3600), future)

        self.assertEqual(result, "asdf")



    @gen_test

    def test_timeout_concurrent_future(self):

        # A concurrent future that does not resolve before the timeout.

        with futures.ThreadPoolExecutor(1) as executor:

            with self.assertRaises(gen.TimeoutError):

                yield gen.with_timeout(

                    self.io_loop.time(), executor.submit(time.sleep, 0.1)

                )



    @gen_test

    def test_completed_concurrent_future(self):

        # A concurrent future that is resolved before we even submit it

        # to with_timeout.

        with futures.ThreadPoolExecutor(1) as executor:



            def dummy():

                pass



            f = executor.submit(dummy)

            f.result()  # wait for completion

            yield gen.with_timeout(datetime.timedelta(seconds=3600), f)



    @gen_test

    def test_normal_concurrent_future(self):

        # A conccurrent future that resolves while waiting for the timeout.

        with futures.ThreadPoolExecutor(1) as executor:

            yield gen.with_timeout(

                datetime.timedelta(seconds=3600),

                executor.submit(lambda: time.sleep(0.01)),

            )





class WaitIteratorTest(AsyncTestCase):

    @gen_test

    def test_empty_iterator(self):

        g = gen.WaitIterator()

        self.assertTrue(g.done(), "empty generator iterated")



        with self.assertRaises(ValueError):

            g = gen.WaitIterator(Future(), bar=Future())



        self.assertEqual(g.current_index, None, "bad nil current index")

        self.assertEqual(g.current_future, None, "bad nil current future")



    @gen_test

    def test_already_done(self):

        f1 = Future()  # type: Future[int]

        f2 = Future()  # type: Future[int]

        f3 = Future()  # type: Future[int]

        f1.set_result(24)

        f2.set_result(42)

        f3.set_result(84)



        g = gen.WaitIterator(f1, f2, f3)

        i = 0

        while not g.done():

            r = yield g.next()

            # Order is not guaranteed, but the current implementation

            # preserves ordering of already-done Futures.

            if i == 0:

                self.assertEqual(g.current_index, 0)

                self.assertIs(g.current_future, f1)

                self.assertEqual(r, 24)

            elif i == 1:

                self.assertEqual(g.current_index, 1)

                self.assertIs(g.current_future, f2)

                self.assertEqual(r, 42)

            elif i == 2:

                self.assertEqual(g.current_index, 2)

                self.assertIs(g.current_future, f3)

                self.assertEqual(r, 84)

            i += 1



        self.assertEqual(g.current_index, None, "bad nil current index")

        self.assertEqual(g.current_future, None, "bad nil current future")



        dg = gen.WaitIterator(f1=f1, f2=f2)



        while not dg.done():

            dr = yield dg.next()

            if dg.current_index == "f1":

                self.assertTrue(

                    dg.current_future == f1 and dr == 24,

                    "WaitIterator dict status incorrect",

                )

            elif dg.current_index == "f2":

                self.assertTrue(

                    dg.current_future == f2 and dr == 42,

                    "WaitIterator dict status incorrect",

                )

            else:

                self.fail("got bad WaitIterator index {}".format(dg.current_index))



            i += 1



        self.assertEqual(dg.current_index, None, "bad nil current index")

        self.assertEqual(dg.current_future, None, "bad nil current future")



    def finish_coroutines(self, iteration, futures):

        if iteration == 3:

            futures[2].set_result(24)

        elif iteration == 5:

            futures[0].set_exception(ZeroDivisionError())

        elif iteration == 8:

            futures[1].set_result(42)

            futures[3].set_result(84)



        if iteration < 8:

            self.io_loop.add_callback(self.finish_coroutines, iteration + 1, futures)



    @gen_test

    def test_iterator(self):

        futures = [Future(), Future(), Future(), Future()]  # type: List[Future[int]]



        self.finish_coroutines(0, futures)



        g = gen.WaitIterator(*futures)



        i = 0

        while not g.done():

            try:

                r = yield g.next()

            except ZeroDivisionError:

                self.assertIs(g.current_future, futures[0], "exception future invalid")

            else:

                if i == 0:

                    self.assertEqual(r, 24, "iterator value incorrect")

                    self.assertEqual(g.current_index, 2, "wrong index")

                elif i == 2:

                    self.assertEqual(r, 42, "iterator value incorrect")

                    self.assertEqual(g.current_index, 1, "wrong index")

                elif i == 3:

                    self.assertEqual(r, 84, "iterator value incorrect")

                    self.assertEqual(g.current_index, 3, "wrong index")

            i += 1



    @gen_test

    def test_iterator_async_await(self):

        # Recreate the previous test with py35 syntax. It's a little clunky

        # because of the way the previous test handles an exception on

        # a single iteration.

        futures = [Future(), Future(), Future(), Future()]  # type: List[Future[int]]

        self.finish_coroutines(0, futures)

        self.finished = False



        async def f():

            i = 0

            g = gen.WaitIterator(*futures)

            try:

                async for r in g:

                    if i == 0:

                        self.assertEqual(r, 24, "iterator value incorrect")

                        self.assertEqual(g.current_index, 2, "wrong index")

                    else:

                        raise Exception("expected exception on iteration 1")

                    i += 1

            except ZeroDivisionError:

                i += 1

            async for r in g:

                if i == 2:

                    self.assertEqual(r, 42, "iterator value incorrect")

                    self.assertEqual(g.current_index, 1, "wrong index")

                elif i == 3:

                    self.assertEqual(r, 84, "iterator value incorrect")

                    self.assertEqual(g.current_index, 3, "wrong index")

                else:

                    raise Exception("didn't expect iteration %d" % i)

                i += 1

            self.finished = True



        yield f()

        self.assertTrue(self.finished)



    @gen_test

    def test_no_ref(self):

        # In this usage, there is no direct hard reference to the

        # WaitIterator itself, only the Future it returns. Since

        # WaitIterator uses weak references internally to improve GC

        # performance, this used to cause problems.

        yield gen.with_timeout(

            datetime.timedelta(seconds=0.1), gen.WaitIterator(gen.sleep(0)).next()

        )





class RunnerGCTest(AsyncTestCase):

    def is_pypy3(self):

        return platform.python_implementation() == "PyPy" and sys.version_info > (3,)



    @gen_test

    def test_gc(self):

        # Github issue 1769: Runner objects can get GCed unexpectedly

        # while their future is alive.

        weakref_scope = [None]  # type: List[Optional[weakref.ReferenceType]]



        def callback():

            gc.collect(2)

            weakref_scope[0]().set_result(123)  # type: ignore



        @gen.coroutine

        def tester():

            fut = Future()  # type: Future[int]

            weakref_scope[0] = weakref.ref(fut)

            self.io_loop.add_callback(callback)

            yield fut



        yield gen.with_timeout(datetime.timedelta(seconds=0.2), tester())



    def test_gc_infinite_coro(self):

        # Github issue 2229: suspended coroutines should be GCed when

        # their loop is closed, even if they're involved in a reference

        # cycle.

        loop = self.get_new_ioloop()

        result = []  # type: List[Optional[bool]]

        wfut = []



        @gen.coroutine

        def infinite_coro():

            try:

                while True:

                    yield gen.sleep(1e-3)

                    result.append(True)

            finally:

                # coroutine finalizer

                result.append(None)



        @gen.coroutine

        def do_something():

            fut = infinite_coro()

            fut._refcycle = fut  # type: ignore

            wfut.append(weakref.ref(fut))

            yield gen.sleep(0.2)



        loop.run_sync(do_something)

        loop.close()

        gc.collect()

        # Future was collected

        self.assertIs(wfut[0](), None)

        # At least one wakeup

        self.assertGreaterEqual(len(result), 2)

        if not self.is_pypy3():

            # coroutine finalizer was called (not on PyPy3 apparently)

            self.assertIs(result[-1], None)



    def test_gc_infinite_async_await(self):

        # Same as test_gc_infinite_coro, but with a `async def` function

        import asyncio



        async def infinite_coro(result):

            try:

                while True:

                    await gen.sleep(1e-3)

                    result.append(True)

            finally:

                # coroutine finalizer

                result.append(None)



        loop = self.get_new_ioloop()

        result = []  # type: List[Optional[bool]]

        wfut = []



        @gen.coroutine

        def do_something():

            fut = asyncio.get_event_loop().create_task(infinite_coro(result))

            fut._refcycle = fut  # type: ignore

            wfut.append(weakref.ref(fut))

            yield gen.sleep(0.2)



        loop.run_sync(do_something)

        with ExpectLog("asyncio", "Task was destroyed but it is pending"):

            loop.close()

            gc.collect()

        # Future was collected

        self.assertIs(wfut[0](), None)

        # At least one wakeup and one finally

        self.assertGreaterEqual(len(result), 2)

        if not self.is_pypy3():

            # coroutine finalizer was called (not on PyPy3 apparently)

            self.assertIs(result[-1], None)



    def test_multi_moment(self):

        # Test gen.multi with moment

        # now that it's not a real Future

        @gen.coroutine

        def wait_a_moment():

            result = yield gen.multi([gen.moment, gen.moment])

            raise gen.Return(result)



        loop = self.get_new_ioloop()

        result = loop.run_sync(wait_a_moment)

        self.assertEqual(result, [None, None])





if contextvars is not None:

    ctx_var = contextvars.ContextVar("ctx_var")  # type: contextvars.ContextVar[int]





@unittest.skipIf(contextvars is None, "contextvars module not present")

class ContextVarsTest(AsyncTestCase):

    async def native_root(self, x):

        ctx_var.set(x)

        await self.inner(x)



    @gen.coroutine

    def gen_root(self, x):

        ctx_var.set(x)

        yield

        yield self.inner(x)



    async def inner(self, x):

        self.assertEqual(ctx_var.get(), x)

        await self.gen_inner(x)

        self.assertEqual(ctx_var.get(), x)



        # IOLoop.run_in_executor doesn't automatically copy context

        ctx = contextvars.copy_context()

        await self.io_loop.run_in_executor(None, lambda: ctx.run(self.thread_inner, x))

        self.assertEqual(ctx_var.get(), x)



        # Neither does asyncio's run_in_executor.

        await asyncio.get_event_loop().run_in_executor(

            None, lambda: ctx.run(self.thread_inner, x)

        )

        self.assertEqual(ctx_var.get(), x)



    @gen.coroutine

    def gen_inner(self, x):

        self.assertEqual(ctx_var.get(), x)

        yield

        self.assertEqual(ctx_var.get(), x)



    def thread_inner(self, x):

        self.assertEqual(ctx_var.get(), x)



    @gen_test

    def test_propagate(self):

        # Verify that context vars get propagated across various

        # combinations of native and decorated coroutines.

        yield [

            self.native_root(1),

            self.native_root(2),

            self.gen_root(3),

            self.gen_root(4),

        ]





if __name__ == "__main__":

    unittest.main()

import socket



from tornado.http1connection import HTTP1Connection

from tornado.httputil import HTTPMessageDelegate

from tornado.iostream import IOStream

from tornado.locks import Event

from tornado.netutil import add_accept_handler

from tornado.testing import AsyncTestCase, bind_unused_port, gen_test





class HTTP1ConnectionTest(AsyncTestCase):

    def setUp(self):

        super(HTTP1ConnectionTest, self).setUp()

        self.asyncSetUp()



    @gen_test

    def asyncSetUp(self):

        listener, port = bind_unused_port()

        event = Event()



        def accept_callback(conn, addr):

            self.server_stream = IOStream(conn)

            self.addCleanup(self.server_stream.close)

            event.set()



        add_accept_handler(listener, accept_callback)

        self.client_stream = IOStream(socket.socket())

        self.addCleanup(self.client_stream.close)

        yield [self.client_stream.connect(("127.0.0.1", port)), event.wait()]

        self.io_loop.remove_handler(listener)

        listener.close()



    @gen_test

    def test_http10_no_content_length(self):

        # Regression test for a bug in which can_keep_alive would crash

        # for an HTTP/1.0 (not 1.1) response with no content-length.

        conn = HTTP1Connection(self.client_stream, True)

        self.server_stream.write(b"HTTP/1.0 200 Not Modified\r\n\r\nhello")

        self.server_stream.close()



        event = Event()

        test = self

        body = []



        class Delegate(HTTPMessageDelegate):

            def headers_received(self, start_line, headers):

                test.code = start_line.code



            def data_received(self, data):

                body.append(data)



            def finish(self):

                event.set()



        yield conn.read_response(Delegate())

        yield event.wait()

        self.assertEqual(self.code, 200)

        self.assertEqual(b"".join(body), b"hello")

# -*- coding: utf-8 -*-

import base64

import binascii

from contextlib import closing

import copy

import threading

import datetime

from io import BytesIO

import subprocess

import sys

import time

import typing  # noqa: F401

import unicodedata

import unittest



from tornado.escape import utf8, native_str, to_unicode

from tornado import gen

from tornado.httpclient import (

    HTTPRequest,

    HTTPResponse,

    _RequestProxy,

    HTTPError,

    HTTPClient,

)

from tornado.httpserver import HTTPServer

from tornado.ioloop import IOLoop

from tornado.iostream import IOStream

from tornado.log import gen_log, app_log

from tornado import netutil

from tornado.testing import AsyncHTTPTestCase, bind_unused_port, gen_test, ExpectLog

from tornado.test.util import skipOnTravis

from tornado.web import Application, RequestHandler, url

from tornado.httputil import format_timestamp, HTTPHeaders





class HelloWorldHandler(RequestHandler):

    def get(self):

        name = self.get_argument("name", "world")

        self.set_header("Content-Type", "text/plain")

        self.finish("Hello %s!" % name)





class PostHandler(RequestHandler):

    def post(self):

        self.finish(

            "Post arg1: %s, arg2: %s"

            % (self.get_argument("arg1"), self.get_argument("arg2"))

        )





class PutHandler(RequestHandler):

    def put(self):

        self.write("Put body: ")

        self.write(self.request.body)





class RedirectHandler(RequestHandler):

    def prepare(self):

        self.write("redirects can have bodies too")

        self.redirect(

            self.get_argument("url"), status=int(self.get_argument("status", "302"))

        )





class RedirectWithoutLocationHandler(RequestHandler):

    def prepare(self):

        # For testing error handling of a redirect with no location header.

        self.set_status(301)

        self.finish()





class ChunkHandler(RequestHandler):

    @gen.coroutine

    def get(self):

        self.write("asdf")

        self.flush()

        # Wait a bit to ensure the chunks are sent and received separately.

        yield gen.sleep(0.01)

        self.write("qwer")





class AuthHandler(RequestHandler):

    def get(self):

        self.finish(self.request.headers["Authorization"])





class CountdownHandler(RequestHandler):

    def get(self, count):

        count = int(count)

        if count > 0:

            self.redirect(self.reverse_url("countdown", count - 1))

        else:

            self.write("Zero")





class EchoPostHandler(RequestHandler):

    def post(self):

        self.write(self.request.body)





class UserAgentHandler(RequestHandler):

    def get(self):

        self.write(self.request.headers.get("User-Agent", "User agent not set"))





class ContentLength304Handler(RequestHandler):

    def get(self):

        self.set_status(304)

        self.set_header("Content-Length", 42)



    def _clear_headers_for_304(self):

        # Tornado strips content-length from 304 responses, but here we

        # want to simulate servers that include the headers anyway.

        pass





class PatchHandler(RequestHandler):

    def patch(self):

        "Return the request payload - so we can check it is being kept"

        self.write(self.request.body)





class AllMethodsHandler(RequestHandler):

    SUPPORTED_METHODS = RequestHandler.SUPPORTED_METHODS + ("OTHER",)  # type: ignore



    def method(self):

        self.write(self.request.method)



    get = head = post = put = delete = options = patch = other = method  # type: ignore





class SetHeaderHandler(RequestHandler):

    def get(self):

        # Use get_arguments for keys to get strings, but

        # request.arguments for values to get bytes.

        for k, v in zip(self.get_arguments("k"), self.request.arguments["v"]):

            self.set_header(k, v)





# These tests end up getting run redundantly: once here with the default

# HTTPClient implementation, and then again in each implementation's own

# test suite.





class HTTPClientCommonTestCase(AsyncHTTPTestCase):

    def get_app(self):

        return Application(

            [

                url("/hello", HelloWorldHandler),

                url("/post", PostHandler),

                url("/put", PutHandler),

                url("/redirect", RedirectHandler),

                url("/redirect_without_location", RedirectWithoutLocationHandler),

                url("/chunk", ChunkHandler),

                url("/auth", AuthHandler),

                url("/countdown/([0-9]+)", CountdownHandler, name="countdown"),

                url("/echopost", EchoPostHandler),

                url("/user_agent", UserAgentHandler),

                url("/304_with_content_length", ContentLength304Handler),

                url("/all_methods", AllMethodsHandler),

                url("/patch", PatchHandler),

                url("/set_header", SetHeaderHandler),

            ],

            gzip=True,

        )



    def test_patch_receives_payload(self):

        body = b"some patch data"

        response = self.fetch("/patch", method="PATCH", body=body)

        self.assertEqual(response.code, 200)

        self.assertEqual(response.body, body)



    @skipOnTravis

    def test_hello_world(self):

        response = self.fetch("/hello")

        self.assertEqual(response.code, 200)

        self.assertEqual(response.headers["Content-Type"], "text/plain")

        self.assertEqual(response.body, b"Hello world!")

        self.assertEqual(int(response.request_time), 0)



        response = self.fetch("/hello?name=Ben")

        self.assertEqual(response.body, b"Hello Ben!")



    def test_streaming_callback(self):

        # streaming_callback is also tested in test_chunked

        chunks = []  # type: typing.List[bytes]

        response = self.fetch("/hello", streaming_callback=chunks.append)

        # with streaming_callback, data goes to the callback and not response.body

        self.assertEqual(chunks, [b"Hello world!"])

        self.assertFalse(response.body)



    def test_post(self):

        response = self.fetch("/post", method="POST", body="arg1=foo&arg2=bar")

        self.assertEqual(response.code, 200)

        self.assertEqual(response.body, b"Post arg1: foo, arg2: bar")



    def test_chunked(self):

        response = self.fetch("/chunk")

        self.assertEqual(response.body, b"asdfqwer")



        chunks = []  # type: typing.List[bytes]

        response = self.fetch("/chunk", streaming_callback=chunks.append)

        self.assertEqual(chunks, [b"asdf", b"qwer"])

        self.assertFalse(response.body)



    def test_chunked_close(self):

        # test case in which chunks spread read-callback processing

        # over several ioloop iterations, but the connection is already closed.

        sock, port = bind_unused_port()

        with closing(sock):



            @gen.coroutine

            def accept_callback(conn, address):

                # fake an HTTP server using chunked encoding where the final chunks

                # and connection close all happen at once

                stream = IOStream(conn)

                request_data = yield stream.read_until(b"\r\n\r\n")

                if b"HTTP/1." not in request_data:

                    self.skipTest("requires HTTP/1.x")

                yield stream.write(

                    b"""\

HTTP/1.1 200 OK

Transfer-Encoding: chunked



1

1

1

2

0



""".replace(

                        b"\n", b"\r\n"

                    )

                )

                stream.close()



            netutil.add_accept_handler(sock, accept_callback)  # type: ignore

            resp = self.fetch("http://127.0.0.1:%d/" % port)

            resp.rethrow()

            self.assertEqual(resp.body, b"12")

            self.io_loop.remove_handler(sock.fileno())



    def test_basic_auth(self):

        # This test data appears in section 2 of RFC 7617.

        self.assertEqual(

            self.fetch(

                "/auth", auth_username="Aladdin", auth_password="open sesame"

            ).body,

            b"Basic QWxhZGRpbjpvcGVuIHNlc2FtZQ==",

        )



    def test_basic_auth_explicit_mode(self):

        self.assertEqual(

            self.fetch(

                "/auth",

                auth_username="Aladdin",

                auth_password="open sesame",

                auth_mode="basic",

            ).body,

            b"Basic QWxhZGRpbjpvcGVuIHNlc2FtZQ==",

        )



    def test_basic_auth_unicode(self):

        # This test data appears in section 2.1 of RFC 7617.

        self.assertEqual(

            self.fetch("/auth", auth_username="test", auth_password="123£").body,

            b"Basic dGVzdDoxMjPCow==",

        )



        # The standard mandates NFC. Give it a decomposed username

        # and ensure it is normalized to composed form.

        username = unicodedata.normalize("NFD", u"josé")

        self.assertEqual(

            self.fetch("/auth", auth_username=username, auth_password="səcrət").body,

            b"Basic am9zw6k6c8mZY3LJmXQ=",

        )



    def test_unsupported_auth_mode(self):

        # curl and simple clients handle errors a bit differently; the

        # important thing is that they don't fall back to basic auth

        # on an unknown mode.

        with ExpectLog(gen_log, "uncaught exception", required=False):

            with self.assertRaises((ValueError, HTTPError)):

                self.fetch(

                    "/auth",

                    auth_username="Aladdin",

                    auth_password="open sesame",

                    auth_mode="asdf",

                    raise_error=True,

                )



    def test_follow_redirect(self):

        response = self.fetch("/countdown/2", follow_redirects=False)

        self.assertEqual(302, response.code)

        self.assertTrue(response.headers["Location"].endswith("/countdown/1"))



        response = self.fetch("/countdown/2")

        self.assertEqual(200, response.code)

        self.assertTrue(response.effective_url.endswith("/countdown/0"))

        self.assertEqual(b"Zero", response.body)



    def test_redirect_without_location(self):

        response = self.fetch("/redirect_without_location", follow_redirects=True)

        # If there is no location header, the redirect response should

        # just be returned as-is. (This should arguably raise an

        # error, but libcurl doesn't treat this as an error, so we

        # don't either).

        self.assertEqual(301, response.code)



    def test_redirect_put_with_body(self):

        response = self.fetch(

            "/redirect?url=/put&status=307", method="PUT", body="hello"

        )

        self.assertEqual(response.body, b"Put body: hello")



    def test_redirect_put_without_body(self):

        # This "without body" edge case is similar to what happens with body_producer.

        response = self.fetch(

            "/redirect?url=/put&status=307",

            method="PUT",

            allow_nonstandard_methods=True,

        )

        self.assertEqual(response.body, b"Put body: ")



    def test_method_after_redirect(self):

        # Legacy redirect codes (301, 302) convert POST requests to GET.

        for status in [301, 302, 303]:

            url = "/redirect?url=/all_methods&status=%d" % status

            resp = self.fetch(url, method="POST", body=b"")

            self.assertEqual(b"GET", resp.body)



            # Other methods are left alone.

            for method in ["GET", "OPTIONS", "PUT", "DELETE"]:

                resp = self.fetch(url, method=method, allow_nonstandard_methods=True)

                self.assertEqual(utf8(method), resp.body)

            # HEAD is different so check it separately.

            resp = self.fetch(url, method="HEAD")

            self.assertEqual(200, resp.code)

            self.assertEqual(b"", resp.body)



        # Newer redirects always preserve the original method.

        for status in [307, 308]:

            url = "/redirect?url=/all_methods&status=307"

            for method in ["GET", "OPTIONS", "POST", "PUT", "DELETE"]:

                resp = self.fetch(url, method=method, allow_nonstandard_methods=True)

                self.assertEqual(method, to_unicode(resp.body))

            resp = self.fetch(url, method="HEAD")

            self.assertEqual(200, resp.code)

            self.assertEqual(b"", resp.body)



    def test_credentials_in_url(self):

        url = self.get_url("/auth").replace("http://", "http://me:secret@")

        response = self.fetch(url)

        self.assertEqual(b"Basic " + base64.b64encode(b"me:secret"), response.body)



    def test_body_encoding(self):

        unicode_body = u"\xe9"

        byte_body = binascii.a2b_hex(b"e9")



        # unicode string in body gets converted to utf8

        response = self.fetch(

            "/echopost",

            method="POST",

            body=unicode_body,

            headers={"Content-Type": "application/blah"},

        )

        self.assertEqual(response.headers["Content-Length"], "2")

        self.assertEqual(response.body, utf8(unicode_body))



        # byte strings pass through directly

        response = self.fetch(

            "/echopost",

            method="POST",

            body=byte_body,

            headers={"Content-Type": "application/blah"},

        )

        self.assertEqual(response.headers["Content-Length"], "1")

        self.assertEqual(response.body, byte_body)



        # Mixing unicode in headers and byte string bodies shouldn't

        # break anything

        response = self.fetch(

            "/echopost",

            method="POST",

            body=byte_body,

            headers={"Content-Type": "application/blah"},

            user_agent=u"foo",

        )

        self.assertEqual(response.headers["Content-Length"], "1")

        self.assertEqual(response.body, byte_body)



    def test_types(self):

        response = self.fetch("/hello")

        self.assertEqual(type(response.body), bytes)

        self.assertEqual(type(response.headers["Content-Type"]), str)

        self.assertEqual(type(response.code), int)

        self.assertEqual(type(response.effective_url), str)



    def test_header_callback(self):

        first_line = []

        headers = {}

        chunks = []



        def header_callback(header_line):

            if header_line.startswith("HTTP/1.1 101"):

                # Upgrading to HTTP/2

                pass

            elif header_line.startswith("HTTP/"):

                first_line.append(header_line)

            elif header_line != "\r\n":

                k, v = header_line.split(":", 1)

                headers[k.lower()] = v.strip()



        def streaming_callback(chunk):

            # All header callbacks are run before any streaming callbacks,

            # so the header data is available to process the data as it

            # comes in.

            self.assertEqual(headers["content-type"], "text/html; charset=UTF-8")

            chunks.append(chunk)



        self.fetch(

            "/chunk",

            header_callback=header_callback,

            streaming_callback=streaming_callback,

        )

        self.assertEqual(len(first_line), 1, first_line)

        self.assertRegexpMatches(first_line[0], "HTTP/[0-9]\\.[0-9] 200.*\r\n")

        self.assertEqual(chunks, [b"asdf", b"qwer"])



    @gen_test

    def test_configure_defaults(self):

        defaults = dict(user_agent="TestDefaultUserAgent", allow_ipv6=False)

        # Construct a new instance of the configured client class

        client = self.http_client.__class__(force_instance=True, defaults=defaults)

        try:

            response = yield client.fetch(self.get_url("/user_agent"))

            self.assertEqual(response.body, b"TestDefaultUserAgent")

        finally:

            client.close()



    def test_header_types(self):

        # Header values may be passed as character or utf8 byte strings,

        # in a plain dictionary or an HTTPHeaders object.

        # Keys must always be the native str type.

        # All combinations should have the same results on the wire.

        for value in [u"MyUserAgent", b"MyUserAgent"]:

            for container in [dict, HTTPHeaders]:

                headers = container()

                headers["User-Agent"] = value

                resp = self.fetch("/user_agent", headers=headers)

                self.assertEqual(

                    resp.body,

                    b"MyUserAgent",

                    "response=%r, value=%r, container=%r"

                    % (resp.body, value, container),

                )



    def test_multi_line_headers(self):

        # Multi-line http headers are rare but rfc-allowed

        # http://www.w3.org/Protocols/rfc2616/rfc2616-sec4.html#sec4.2

        sock, port = bind_unused_port()

        with closing(sock):



            @gen.coroutine

            def accept_callback(conn, address):

                stream = IOStream(conn)

                request_data = yield stream.read_until(b"\r\n\r\n")

                if b"HTTP/1." not in request_data:

                    self.skipTest("requires HTTP/1.x")

                yield stream.write(

                    b"""\

HTTP/1.1 200 OK

X-XSS-Protection: 1;

\tmode=block



""".replace(

                        b"\n", b"\r\n"

                    )

                )

                stream.close()



            netutil.add_accept_handler(sock, accept_callback)  # type: ignore

            resp = self.fetch("http://127.0.0.1:%d/" % port)

            resp.rethrow()

            self.assertEqual(resp.headers["X-XSS-Protection"], "1; mode=block")

            self.io_loop.remove_handler(sock.fileno())



    def test_304_with_content_length(self):

        # According to the spec 304 responses SHOULD NOT include

        # Content-Length or other entity headers, but some servers do it

        # anyway.

        # http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html#sec10.3.5

        response = self.fetch("/304_with_content_length")

        self.assertEqual(response.code, 304)

        self.assertEqual(response.headers["Content-Length"], "42")



    @gen_test

    def test_future_interface(self):

        response = yield self.http_client.fetch(self.get_url("/hello"))

        self.assertEqual(response.body, b"Hello world!")



    @gen_test

    def test_future_http_error(self):

        with self.assertRaises(HTTPError) as context:

            yield self.http_client.fetch(self.get_url("/notfound"))

        self.assertEqual(context.exception.code, 404)

        self.assertEqual(context.exception.response.code, 404)



    @gen_test

    def test_future_http_error_no_raise(self):

        response = yield self.http_client.fetch(

            self.get_url("/notfound"), raise_error=False

        )

        self.assertEqual(response.code, 404)



    @gen_test

    def test_reuse_request_from_response(self):

        # The response.request attribute should be an HTTPRequest, not

        # a _RequestProxy.

        # This test uses self.http_client.fetch because self.fetch calls

        # self.get_url on the input unconditionally.

        url = self.get_url("/hello")

        response = yield self.http_client.fetch(url)

        self.assertEqual(response.request.url, url)

        self.assertTrue(isinstance(response.request, HTTPRequest))

        response2 = yield self.http_client.fetch(response.request)

        self.assertEqual(response2.body, b"Hello world!")



    @gen_test

    def test_bind_source_ip(self):

        url = self.get_url("/hello")

        request = HTTPRequest(url, network_interface="127.0.0.1")

        response = yield self.http_client.fetch(request)

        self.assertEqual(response.code, 200)



        with self.assertRaises((ValueError, HTTPError)) as context:

            request = HTTPRequest(url, network_interface="not-interface-or-ip")

            yield self.http_client.fetch(request)

        self.assertIn("not-interface-or-ip", str(context.exception))



    def test_all_methods(self):

        for method in ["GET", "DELETE", "OPTIONS"]:

            response = self.fetch("/all_methods", method=method)

            self.assertEqual(response.body, utf8(method))

        for method in ["POST", "PUT", "PATCH"]:

            response = self.fetch("/all_methods", method=method, body=b"")

            self.assertEqual(response.body, utf8(method))

        response = self.fetch("/all_methods", method="HEAD")

        self.assertEqual(response.body, b"")

        response = self.fetch(

            "/all_methods", method="OTHER", allow_nonstandard_methods=True

        )

        self.assertEqual(response.body, b"OTHER")



    def test_body_sanity_checks(self):

        # These methods require a body.

        for method in ("POST", "PUT", "PATCH"):

            with self.assertRaises(ValueError) as context:

                self.fetch("/all_methods", method=method, raise_error=True)

            self.assertIn("must not be None", str(context.exception))



            resp = self.fetch(

                "/all_methods", method=method, allow_nonstandard_methods=True

            )

            self.assertEqual(resp.code, 200)



        # These methods don't allow a body.

        for method in ("GET", "DELETE", "OPTIONS"):

            with self.assertRaises(ValueError) as context:

                self.fetch(

                    "/all_methods", method=method, body=b"asdf", raise_error=True

                )

            self.assertIn("must be None", str(context.exception))



            # In most cases this can be overridden, but curl_httpclient

            # does not allow body with a GET at all.

            if method != "GET":

                self.fetch(

                    "/all_methods",

                    method=method,

                    body=b"asdf",

                    allow_nonstandard_methods=True,

                    raise_error=True,

                )

                self.assertEqual(resp.code, 200)



    # This test causes odd failures with the combination of

    # curl_httpclient (at least with the version of libcurl available

    # on ubuntu 12.04), TwistedIOLoop, and epoll.  For POST (but not PUT),

    # curl decides the response came back too soon and closes the connection

    # to start again.  It does this *before* telling the socket callback to

    # unregister the FD.  Some IOLoop implementations have special kernel

    # integration to discover this immediately.  Tornado's IOLoops

    # ignore errors on remove_handler to accommodate this behavior, but

    # Twisted's reactor does not.  The removeReader call fails and so

    # do all future removeAll calls (which our tests do at cleanup).

    #

    # def test_post_307(self):

    #    response = self.fetch("/redirect?status=307&url=/post",

    #                          method="POST", body=b"arg1=foo&arg2=bar")

    #    self.assertEqual(response.body, b"Post arg1: foo, arg2: bar")



    def test_put_307(self):

        response = self.fetch(

            "/redirect?status=307&url=/put", method="PUT", body=b"hello"

        )

        response.rethrow()

        self.assertEqual(response.body, b"Put body: hello")



    def test_non_ascii_header(self):

        # Non-ascii headers are sent as latin1.

        response = self.fetch("/set_header?k=foo&v=%E9")

        response.rethrow()

        self.assertEqual(response.headers["Foo"], native_str(u"\u00e9"))



    def test_response_times(self):

        # A few simple sanity checks of the response time fields to

        # make sure they're using the right basis (between the

        # wall-time and monotonic clocks).

        start_time = time.time()

        response = self.fetch("/hello")

        response.rethrow()

        self.assertGreaterEqual(response.request_time, 0)

        self.assertLess(response.request_time, 1.0)

        # A very crude check to make sure that start_time is based on

        # wall time and not the monotonic clock.

        self.assertLess(abs(response.start_time - start_time), 1.0)



        for k, v in response.time_info.items():

            self.assertTrue(0 <= v < 1.0, "time_info[%s] out of bounds: %s" % (k, v))



    @gen_test

    def test_error_after_cancel(self):

        fut = self.http_client.fetch(self.get_url("/404"))

        self.assertTrue(fut.cancel())

        with ExpectLog(app_log, "Exception after Future was cancelled") as el:

            # We can't wait on the cancelled Future any more, so just

            # let the IOLoop run until the exception gets logged (or

            # not, in which case we exit the loop and ExpectLog will

            # raise).

            for i in range(100):

                yield gen.sleep(0.01)

                if el.logged_stack:

                    break





class RequestProxyTest(unittest.TestCase):

    def test_request_set(self):

        proxy = _RequestProxy(

            HTTPRequest("http://example.com/", user_agent="foo"), dict()

        )

        self.assertEqual(proxy.user_agent, "foo")



    def test_default_set(self):

        proxy = _RequestProxy(

            HTTPRequest("http://example.com/"), dict(network_interface="foo")

        )

        self.assertEqual(proxy.network_interface, "foo")



    def test_both_set(self):

        proxy = _RequestProxy(

            HTTPRequest("http://example.com/", proxy_host="foo"), dict(proxy_host="bar")

        )

        self.assertEqual(proxy.proxy_host, "foo")



    def test_neither_set(self):

        proxy = _RequestProxy(HTTPRequest("http://example.com/"), dict())

        self.assertIs(proxy.auth_username, None)



    def test_bad_attribute(self):

        proxy = _RequestProxy(HTTPRequest("http://example.com/"), dict())

        with self.assertRaises(AttributeError):

            proxy.foo



    def test_defaults_none(self):

        proxy = _RequestProxy(HTTPRequest("http://example.com/"), None)

        self.assertIs(proxy.auth_username, None)





class HTTPResponseTestCase(unittest.TestCase):

    def test_str(self):

        response = HTTPResponse(  # type: ignore

            HTTPRequest("http://example.com"), 200, headers={}, buffer=BytesIO()

        )

        s = str(response)

        self.assertTrue(s.startswith("HTTPResponse("))

        self.assertIn("code=200", s)





class SyncHTTPClientTest(unittest.TestCase):

    def setUp(self):

        self.server_ioloop = IOLoop()

        event = threading.Event()



        @gen.coroutine

        def init_server():

            sock, self.port = bind_unused_port()

            app = Application([("/", HelloWorldHandler)])

            self.server = HTTPServer(app)

            self.server.add_socket(sock)

            event.set()



        def start():

            self.server_ioloop.run_sync(init_server)

            self.server_ioloop.start()



        self.server_thread = threading.Thread(target=start)

        self.server_thread.start()

        event.wait()



        self.http_client = HTTPClient()



    def tearDown(self):

        def stop_server():

            self.server.stop()

            # Delay the shutdown of the IOLoop by several iterations because

            # the server may still have some cleanup work left when

            # the client finishes with the response (this is noticeable

            # with http/2, which leaves a Future with an unexamined

            # StreamClosedError on the loop).



            @gen.coroutine

            def slow_stop():

                yield self.server.close_all_connections()

                # The number of iterations is difficult to predict. Typically,

                # one is sufficient, although sometimes it needs more.

                for i in range(5):

                    yield

                self.server_ioloop.stop()



            self.server_ioloop.add_callback(slow_stop)



        self.server_ioloop.add_callback(stop_server)

        self.server_thread.join()

        self.http_client.close()

        self.server_ioloop.close(all_fds=True)



    def get_url(self, path):

        return "http://127.0.0.1:%d%s" % (self.port, path)



    def test_sync_client(self):

        response = self.http_client.fetch(self.get_url("/"))

        self.assertEqual(b"Hello world!", response.body)



    def test_sync_client_error(self):

        # Synchronous HTTPClient raises errors directly; no need for

        # response.rethrow()

        with self.assertRaises(HTTPError) as assertion:

            self.http_client.fetch(self.get_url("/notfound"))

        self.assertEqual(assertion.exception.code, 404)





class SyncHTTPClientSubprocessTest(unittest.TestCase):

    def test_destructor_log(self):

        # Regression test for

        # https://github.com/tornadoweb/tornado/issues/2539

        #

        # In the past, the following program would log an

        # "inconsistent AsyncHTTPClient cache" error from a destructor

        # when the process is shutting down. The shutdown process is

        # subtle and I don't fully understand it; the failure does not

        # manifest if that lambda isn't there or is a simpler object

        # like an int (nor does it manifest in the tornado test suite

        # as a whole, which is why we use this subprocess).

        proc = subprocess.run(

            [

                sys.executable,

                "-c",

                "from tornado.httpclient import HTTPClient; f = lambda: None; c = HTTPClient()",

            ],

            stdout=subprocess.PIPE,

            stderr=subprocess.STDOUT,

            check=True,

        )

        if proc.stdout:

            print("STDOUT:")

            print(to_unicode(proc.stdout))

        if proc.stdout:

            self.fail("subprocess produced unexpected output")





class HTTPRequestTestCase(unittest.TestCase):

    def test_headers(self):

        request = HTTPRequest("http://example.com", headers={"foo": "bar"})

        self.assertEqual(request.headers, {"foo": "bar"})



    def test_headers_setter(self):

        request = HTTPRequest("http://example.com")

        request.headers = {"bar": "baz"}  # type: ignore

        self.assertEqual(request.headers, {"bar": "baz"})



    def test_null_headers_setter(self):

        request = HTTPRequest("http://example.com")

        request.headers = None  # type: ignore

        self.assertEqual(request.headers, {})



    def test_body(self):

        request = HTTPRequest("http://example.com", body="foo")

        self.assertEqual(request.body, utf8("foo"))



    def test_body_setter(self):

        request = HTTPRequest("http://example.com")

        request.body = "foo"  # type: ignore

        self.assertEqual(request.body, utf8("foo"))



    def test_if_modified_since(self):

        http_date = datetime.datetime.utcnow()

        request = HTTPRequest("http://example.com", if_modified_since=http_date)

        self.assertEqual(

            request.headers, {"If-Modified-Since": format_timestamp(http_date)}

        )





class HTTPErrorTestCase(unittest.TestCase):

    def test_copy(self):

        e = HTTPError(403)

        e2 = copy.copy(e)

        self.assertIsNot(e, e2)

        self.assertEqual(e.code, e2.code)



    def test_plain_error(self):

        e = HTTPError(403)

        self.assertEqual(str(e), "HTTP 403: Forbidden")

        self.assertEqual(repr(e), "HTTP 403: Forbidden")



    def test_error_with_response(self):

        resp = HTTPResponse(HTTPRequest("http://example.com/"), 403)

        with self.assertRaises(HTTPError) as cm:

            resp.rethrow()

        e = cm.exception

        self.assertEqual(str(e), "HTTP 403: Forbidden")

        self.assertEqual(repr(e), "HTTP 403: Forbidden")

from tornado import gen, netutil

from tornado.escape import (

    json_decode,

    json_encode,

    utf8,

    _unicode,

    recursive_unicode,

    native_str,

)

from tornado.http1connection import HTTP1Connection

from tornado.httpclient import HTTPError

from tornado.httpserver import HTTPServer

from tornado.httputil import (

    HTTPHeaders,

    HTTPMessageDelegate,

    HTTPServerConnectionDelegate,

    ResponseStartLine,

)

from tornado.iostream import IOStream

from tornado.locks import Event

from tornado.log import gen_log

from tornado.netutil import ssl_options_to_context

from tornado.simple_httpclient import SimpleAsyncHTTPClient

from tornado.testing import (

    AsyncHTTPTestCase,

    AsyncHTTPSTestCase,

    AsyncTestCase,

    ExpectLog,

    gen_test,

)

from tornado.test.util import skipOnTravis

from tornado.web import Application, RequestHandler, stream_request_body



from contextlib import closing

import datetime

import gzip

import os

import shutil

import socket

import ssl

import sys

import tempfile

import unittest

from io import BytesIO



import typing



if typing.TYPE_CHECKING:

    from typing import Dict, List  # noqa: F401





async def read_stream_body(stream):

    """Reads an HTTP response from `stream` and returns a tuple of its

    start_line, headers and body."""

    chunks = []



    class Delegate(HTTPMessageDelegate):

        def headers_received(self, start_line, headers):

            self.headers = headers

            self.start_line = start_line



        def data_received(self, chunk):

            chunks.append(chunk)



        def finish(self):

            conn.detach()  # type: ignore



    conn = HTTP1Connection(stream, True)

    delegate = Delegate()

    await conn.read_response(delegate)

    return delegate.start_line, delegate.headers, b"".join(chunks)





class HandlerBaseTestCase(AsyncHTTPTestCase):

    def get_app(self):

        return Application([("/", self.__class__.Handler)])



    def fetch_json(self, *args, **kwargs):

        response = self.fetch(*args, **kwargs)

        response.rethrow()

        return json_decode(response.body)





class HelloWorldRequestHandler(RequestHandler):

    def initialize(self, protocol="http"):

        self.expected_protocol = protocol



    def get(self):

        if self.request.protocol != self.expected_protocol:

            raise Exception("unexpected protocol")

        self.finish("Hello world")



    def post(self):

        self.finish("Got %d bytes in POST" % len(self.request.body))





# In pre-1.0 versions of openssl, SSLv23 clients always send SSLv2

# ClientHello messages, which are rejected by SSLv3 and TLSv1

# servers.  Note that while the OPENSSL_VERSION_INFO was formally

# introduced in python3.2, it was present but undocumented in

# python 2.7

skipIfOldSSL = unittest.skipIf(

    getattr(ssl, "OPENSSL_VERSION_INFO", (0, 0)) < (1, 0),

    "old version of ssl module and/or openssl",

)





class BaseSSLTest(AsyncHTTPSTestCase):

    def get_app(self):

        return Application([("/", HelloWorldRequestHandler, dict(protocol="https"))])





class SSLTestMixin(object):

    def get_ssl_options(self):

        return dict(

            ssl_version=self.get_ssl_version(),

            **AsyncHTTPSTestCase.default_ssl_options()

        )



    def get_ssl_version(self):

        raise NotImplementedError()



    def test_ssl(self):

        response = self.fetch("/")

        self.assertEqual(response.body, b"Hello world")



    def test_large_post(self):

        response = self.fetch("/", method="POST", body="A" * 5000)

        self.assertEqual(response.body, b"Got 5000 bytes in POST")



    def test_non_ssl_request(self):

        # Make sure the server closes the connection when it gets a non-ssl

        # connection, rather than waiting for a timeout or otherwise

        # misbehaving.

        with ExpectLog(gen_log, "(SSL Error|uncaught exception)"):

            with ExpectLog(gen_log, "Uncaught exception", required=False):

                with self.assertRaises((IOError, HTTPError)):

                    self.fetch(

                        self.get_url("/").replace("https:", "http:"),

                        request_timeout=3600,

                        connect_timeout=3600,

                        raise_error=True,

                    )



    def test_error_logging(self):

        # No stack traces are logged for SSL errors.

        with ExpectLog(gen_log, "SSL Error") as expect_log:

            with self.assertRaises((IOError, HTTPError)):

                self.fetch(

                    self.get_url("/").replace("https:", "http:"), raise_error=True

                )

        self.assertFalse(expect_log.logged_stack)





# Python's SSL implementation differs significantly between versions.

# For example, SSLv3 and TLSv1 throw an exception if you try to read

# from the socket before the handshake is complete, but the default

# of SSLv23 allows it.





class SSLv23Test(BaseSSLTest, SSLTestMixin):

    def get_ssl_version(self):

        return ssl.PROTOCOL_SSLv23





@skipIfOldSSL

class SSLv3Test(BaseSSLTest, SSLTestMixin):

    def get_ssl_version(self):

        return ssl.PROTOCOL_SSLv3





@skipIfOldSSL

class TLSv1Test(BaseSSLTest, SSLTestMixin):

    def get_ssl_version(self):

        return ssl.PROTOCOL_TLSv1





class SSLContextTest(BaseSSLTest, SSLTestMixin):

    def get_ssl_options(self):

        context = ssl_options_to_context(AsyncHTTPSTestCase.get_ssl_options(self))

        assert isinstance(context, ssl.SSLContext)

        return context





class BadSSLOptionsTest(unittest.TestCase):

    def test_missing_arguments(self):

        application = Application()

        self.assertRaises(

            KeyError,

            HTTPServer,

            application,

            ssl_options={"keyfile": "/__missing__.crt"},

        )



    def test_missing_key(self):

        """A missing SSL key should cause an immediate exception."""



        application = Application()

        module_dir = os.path.dirname(__file__)

        existing_certificate = os.path.join(module_dir, "test.crt")

        existing_key = os.path.join(module_dir, "test.key")



        self.assertRaises(

            (ValueError, IOError),

            HTTPServer,

            application,

            ssl_options={"certfile": "/__mising__.crt"},

        )

        self.assertRaises(

            (ValueError, IOError),

            HTTPServer,

            application,

            ssl_options={

                "certfile": existing_certificate,

                "keyfile": "/__missing__.key",

            },

        )



        # This actually works because both files exist

        HTTPServer(

            application,

            ssl_options={"certfile": existing_certificate, "keyfile": existing_key},

        )





class MultipartTestHandler(RequestHandler):

    def post(self):

        self.finish(

            {

                "header": self.request.headers["X-Header-Encoding-Test"],

                "argument": self.get_argument("argument"),

                "filename": self.request.files["files"][0].filename,

                "filebody": _unicode(self.request.files["files"][0]["body"]),

            }

        )





# This test is also called from wsgi_test

class HTTPConnectionTest(AsyncHTTPTestCase):

    def get_handlers(self):

        return [

            ("/multipart", MultipartTestHandler),

            ("/hello", HelloWorldRequestHandler),

        ]



    def get_app(self):

        return Application(self.get_handlers())



    def raw_fetch(self, headers, body, newline=b"\r\n"):

        with closing(IOStream(socket.socket())) as stream:

            self.io_loop.run_sync(

                lambda: stream.connect(("127.0.0.1", self.get_http_port()))

            )

            stream.write(

                newline.join(headers + [utf8("Content-Length: %d" % len(body))])

                + newline

                + newline

                + body

            )

            start_line, headers, body = self.io_loop.run_sync(

                lambda: read_stream_body(stream)

            )

            return body



    def test_multipart_form(self):

        # Encodings here are tricky:  Headers are latin1, bodies can be

        # anything (we use utf8 by default).

        response = self.raw_fetch(

            [

                b"POST /multipart HTTP/1.0",

                b"Content-Type: multipart/form-data; boundary=1234567890",

                b"X-Header-encoding-test: \xe9",

            ],

            b"\r\n".join(

                [

                    b"Content-Disposition: form-data; name=argument",

                    b"",

                    u"\u00e1".encode("utf-8"),

                    b"--1234567890",

                    u'Content-Disposition: form-data; name="files"; filename="\u00f3"'.encode(

                        "utf8"

                    ),

                    b"",

                    u"\u00fa".encode("utf-8"),

                    b"--1234567890--",

                    b"",

                ]

            ),

        )

        data = json_decode(response)

        self.assertEqual(u"\u00e9", data["header"])

        self.assertEqual(u"\u00e1", data["argument"])

        self.assertEqual(u"\u00f3", data["filename"])

        self.assertEqual(u"\u00fa", data["filebody"])



    def test_newlines(self):

        # We support both CRLF and bare LF as line separators.

        for newline in (b"\r\n", b"\n"):

            response = self.raw_fetch([b"GET /hello HTTP/1.0"], b"", newline=newline)

            self.assertEqual(response, b"Hello world")



    @gen_test

    def test_100_continue(self):

        # Run through a 100-continue interaction by hand:

        # When given Expect: 100-continue, we get a 100 response after the

        # headers, and then the real response after the body.

        stream = IOStream(socket.socket())

        yield stream.connect(("127.0.0.1", self.get_http_port()))

        yield stream.write(

            b"\r\n".join(

                [

                    b"POST /hello HTTP/1.1",

                    b"Content-Length: 1024",

                    b"Expect: 100-continue",

                    b"Connection: close",

                    b"\r\n",

                ]

            )

        )

        data = yield stream.read_until(b"\r\n\r\n")

        self.assertTrue(data.startswith(b"HTTP/1.1 100 "), data)

        stream.write(b"a" * 1024)

        first_line = yield stream.read_until(b"\r\n")

        self.assertTrue(first_line.startswith(b"HTTP/1.1 200"), first_line)

        header_data = yield stream.read_until(b"\r\n\r\n")

        headers = HTTPHeaders.parse(native_str(header_data.decode("latin1")))

        body = yield stream.read_bytes(int(headers["Content-Length"]))

        self.assertEqual(body, b"Got 1024 bytes in POST")

        stream.close()





class EchoHandler(RequestHandler):

    def get(self):

        self.write(recursive_unicode(self.request.arguments))



    def post(self):

        self.write(recursive_unicode(self.request.arguments))





class TypeCheckHandler(RequestHandler):

    def prepare(self):

        self.errors = {}  # type: Dict[str, str]

        fields = [

            ("method", str),

            ("uri", str),

            ("version", str),

            ("remote_ip", str),

            ("protocol", str),

            ("host", str),

            ("path", str),

            ("query", str),

        ]

        for field, expected_type in fields:

            self.check_type(field, getattr(self.request, field), expected_type)



        self.check_type("header_key", list(self.request.headers.keys())[0], str)

        self.check_type("header_value", list(self.request.headers.values())[0], str)



        self.check_type("cookie_key", list(self.request.cookies.keys())[0], str)

        self.check_type(

            "cookie_value", list(self.request.cookies.values())[0].value, str

        )

        # secure cookies



        self.check_type("arg_key", list(self.request.arguments.keys())[0], str)

        self.check_type("arg_value", list(self.request.arguments.values())[0][0], bytes)



    def post(self):

        self.check_type("body", self.request.body, bytes)

        self.write(self.errors)



    def get(self):

        self.write(self.errors)



    def check_type(self, name, obj, expected_type):

        actual_type = type(obj)

        if expected_type != actual_type:

            self.errors[name] = "expected %s, got %s" % (expected_type, actual_type)





class HTTPServerTest(AsyncHTTPTestCase):

    def get_app(self):

        return Application(

            [

                ("/echo", EchoHandler),

                ("/typecheck", TypeCheckHandler),

                ("//doubleslash", EchoHandler),

            ]

        )



    def test_query_string_encoding(self):

        response = self.fetch("/echo?foo=%C3%A9")

        data = json_decode(response.body)

        self.assertEqual(data, {u"foo": [u"\u00e9"]})



    def test_empty_query_string(self):

        response = self.fetch("/echo?foo=&foo=")

        data = json_decode(response.body)

        self.assertEqual(data, {u"foo": [u"", u""]})



    def test_empty_post_parameters(self):

        response = self.fetch("/echo", method="POST", body="foo=&bar=")

        data = json_decode(response.body)

        self.assertEqual(data, {u"foo": [u""], u"bar": [u""]})



    def test_types(self):

        headers = {"Cookie": "foo=bar"}

        response = self.fetch("/typecheck?foo=bar", headers=headers)

        data = json_decode(response.body)

        self.assertEqual(data, {})



        response = self.fetch(

            "/typecheck", method="POST", body="foo=bar", headers=headers

        )

        data = json_decode(response.body)

        self.assertEqual(data, {})



    def test_double_slash(self):

        # urlparse.urlsplit (which tornado.httpserver used to use

        # incorrectly) would parse paths beginning with "//" as

        # protocol-relative urls.

        response = self.fetch("//doubleslash")

        self.assertEqual(200, response.code)

        self.assertEqual(json_decode(response.body), {})



    def test_malformed_body(self):

        # parse_qs is pretty forgiving, but it will fail on python 3

        # if the data is not utf8.

        with ExpectLog(gen_log, "Invalid x-www-form-urlencoded body"):

            response = self.fetch(

                "/echo",

                method="POST",

                headers={"Content-Type": "application/x-www-form-urlencoded"},

                body=b"\xe9",

            )

        self.assertEqual(200, response.code)

        self.assertEqual(b"{}", response.body)





class HTTPServerRawTest(AsyncHTTPTestCase):

    def get_app(self):

        return Application([("/echo", EchoHandler)])



    def setUp(self):

        super(HTTPServerRawTest, self).setUp()

        self.stream = IOStream(socket.socket())

        self.io_loop.run_sync(

            lambda: self.stream.connect(("127.0.0.1", self.get_http_port()))

        )



    def tearDown(self):

        self.stream.close()

        super(HTTPServerRawTest, self).tearDown()



    def test_empty_request(self):

        self.stream.close()

        self.io_loop.add_timeout(datetime.timedelta(seconds=0.001), self.stop)

        self.wait()



    def test_malformed_first_line_response(self):

        with ExpectLog(gen_log, ".*Malformed HTTP request line"):

            self.stream.write(b"asdf\r\n\r\n")

            start_line, headers, response = self.io_loop.run_sync(

                lambda: read_stream_body(self.stream)

            )

            self.assertEqual("HTTP/1.1", start_line.version)

            self.assertEqual(400, start_line.code)

            self.assertEqual("Bad Request", start_line.reason)



    def test_malformed_first_line_log(self):

        with ExpectLog(gen_log, ".*Malformed HTTP request line"):

            self.stream.write(b"asdf\r\n\r\n")

            # TODO: need an async version of ExpectLog so we don't need

            # hard-coded timeouts here.

            self.io_loop.add_timeout(datetime.timedelta(seconds=0.05), self.stop)

            self.wait()



    def test_malformed_headers(self):

        with ExpectLog(gen_log, ".*Malformed HTTP message.*no colon in header line"):

            self.stream.write(b"GET / HTTP/1.0\r\nasdf\r\n\r\n")

            self.io_loop.add_timeout(datetime.timedelta(seconds=0.05), self.stop)

            self.wait()



    def test_chunked_request_body(self):

        # Chunked requests are not widely supported and we don't have a way

        # to generate them in AsyncHTTPClient, but HTTPServer will read them.

        self.stream.write(

            b"""\

POST /echo HTTP/1.1

Transfer-Encoding: chunked

Content-Type: application/x-www-form-urlencoded



4

foo=

3

bar

0



""".replace(

                b"\n", b"\r\n"

            )

        )

        start_line, headers, response = self.io_loop.run_sync(

            lambda: read_stream_body(self.stream)

        )

        self.assertEqual(json_decode(response), {u"foo": [u"bar"]})



    def test_chunked_request_uppercase(self):

        # As per RFC 2616 section 3.6, "Transfer-Encoding" header's value is

        # case-insensitive.

        self.stream.write(

            b"""\

POST /echo HTTP/1.1

Transfer-Encoding: Chunked

Content-Type: application/x-www-form-urlencoded



4

foo=

3

bar

0



""".replace(

                b"\n", b"\r\n"

            )

        )

        start_line, headers, response = self.io_loop.run_sync(

            lambda: read_stream_body(self.stream)

        )

        self.assertEqual(json_decode(response), {u"foo": [u"bar"]})



    @gen_test

    def test_invalid_content_length(self):

        with ExpectLog(gen_log, ".*Only integer Content-Length is allowed"):

            self.stream.write(

                b"""\

POST /echo HTTP/1.1

Content-Length: foo



bar



""".replace(

                    b"\n", b"\r\n"

                )

            )

            yield self.stream.read_until_close()





class XHeaderTest(HandlerBaseTestCase):

    class Handler(RequestHandler):

        def get(self):

            self.set_header("request-version", self.request.version)

            self.write(

                dict(

                    remote_ip=self.request.remote_ip,

                    remote_protocol=self.request.protocol,

                )

            )



    def get_httpserver_options(self):

        return dict(xheaders=True, trusted_downstream=["5.5.5.5"])



    def test_ip_headers(self):

        self.assertEqual(self.fetch_json("/")["remote_ip"], "127.0.0.1")



        valid_ipv4 = {"X-Real-IP": "4.4.4.4"}

        self.assertEqual(

            self.fetch_json("/", headers=valid_ipv4)["remote_ip"], "4.4.4.4"

        )



        valid_ipv4_list = {"X-Forwarded-For": "127.0.0.1, 4.4.4.4"}

        self.assertEqual(

            self.fetch_json("/", headers=valid_ipv4_list)["remote_ip"], "4.4.4.4"

        )



        valid_ipv6 = {"X-Real-IP": "2620:0:1cfe:face:b00c::3"}

        self.assertEqual(

            self.fetch_json("/", headers=valid_ipv6)["remote_ip"],

            "2620:0:1cfe:face:b00c::3",

        )



        valid_ipv6_list = {"X-Forwarded-For": "::1, 2620:0:1cfe:face:b00c::3"}

        self.assertEqual(

            self.fetch_json("/", headers=valid_ipv6_list)["remote_ip"],

            "2620:0:1cfe:face:b00c::3",

        )



        invalid_chars = {"X-Real-IP": "4.4.4.4<script>"}

        self.assertEqual(

            self.fetch_json("/", headers=invalid_chars)["remote_ip"], "127.0.0.1"

        )



        invalid_chars_list = {"X-Forwarded-For": "4.4.4.4, 5.5.5.5<script>"}

        self.assertEqual(

            self.fetch_json("/", headers=invalid_chars_list)["remote_ip"], "127.0.0.1"

        )



        invalid_host = {"X-Real-IP": "www.google.com"}

        self.assertEqual(

            self.fetch_json("/", headers=invalid_host)["remote_ip"], "127.0.0.1"

        )



    def test_trusted_downstream(self):

        valid_ipv4_list = {"X-Forwarded-For": "127.0.0.1, 4.4.4.4, 5.5.5.5"}

        resp = self.fetch("/", headers=valid_ipv4_list)

        if resp.headers["request-version"].startswith("HTTP/2"):

            # This is a hack - there's nothing that fundamentally requires http/1

            # here but tornado_http2 doesn't support it yet.

            self.skipTest("requires HTTP/1.x")

        result = json_decode(resp.body)

        self.assertEqual(result["remote_ip"], "4.4.4.4")



    def test_scheme_headers(self):

        self.assertEqual(self.fetch_json("/")["remote_protocol"], "http")



        https_scheme = {"X-Scheme": "https"}

        self.assertEqual(

            self.fetch_json("/", headers=https_scheme)["remote_protocol"], "https"

        )



        https_forwarded = {"X-Forwarded-Proto": "https"}

        self.assertEqual(

            self.fetch_json("/", headers=https_forwarded)["remote_protocol"], "https"

        )



        https_multi_forwarded = {"X-Forwarded-Proto": "https , http"}

        self.assertEqual(

            self.fetch_json("/", headers=https_multi_forwarded)["remote_protocol"],

            "http",

        )



        http_multi_forwarded = {"X-Forwarded-Proto": "http,https"}

        self.assertEqual(

            self.fetch_json("/", headers=http_multi_forwarded)["remote_protocol"],

            "https",

        )



        bad_forwarded = {"X-Forwarded-Proto": "unknown"}

        self.assertEqual(

            self.fetch_json("/", headers=bad_forwarded)["remote_protocol"], "http"

        )





class SSLXHeaderTest(AsyncHTTPSTestCase, HandlerBaseTestCase):

    def get_app(self):

        return Application([("/", XHeaderTest.Handler)])



    def get_httpserver_options(self):

        output = super(SSLXHeaderTest, self).get_httpserver_options()

        output["xheaders"] = True

        return output



    def test_request_without_xprotocol(self):

        self.assertEqual(self.fetch_json("/")["remote_protocol"], "https")



        http_scheme = {"X-Scheme": "http"}

        self.assertEqual(

            self.fetch_json("/", headers=http_scheme)["remote_protocol"], "http"

        )



        bad_scheme = {"X-Scheme": "unknown"}

        self.assertEqual(

            self.fetch_json("/", headers=bad_scheme)["remote_protocol"], "https"

        )





class ManualProtocolTest(HandlerBaseTestCase):

    class Handler(RequestHandler):

        def get(self):

            self.write(dict(protocol=self.request.protocol))



    def get_httpserver_options(self):

        return dict(protocol="https")



    def test_manual_protocol(self):

        self.assertEqual(self.fetch_json("/")["protocol"], "https")





@unittest.skipIf(

    not hasattr(socket, "AF_UNIX") or sys.platform == "cygwin",

    "unix sockets not supported on this platform",

)

class UnixSocketTest(AsyncTestCase):

    """HTTPServers can listen on Unix sockets too.



    Why would you want to do this?  Nginx can proxy to backends listening

    on unix sockets, for one thing (and managing a namespace for unix

    sockets can be easier than managing a bunch of TCP port numbers).



    Unfortunately, there's no way to specify a unix socket in a url for

    an HTTP client, so we have to test this by hand.

    """



    def setUp(self):

        super(UnixSocketTest, self).setUp()

        self.tmpdir = tempfile.mkdtemp()

        self.sockfile = os.path.join(self.tmpdir, "test.sock")

        sock = netutil.bind_unix_socket(self.sockfile)

        app = Application([("/hello", HelloWorldRequestHandler)])

        self.server = HTTPServer(app)

        self.server.add_socket(sock)

        self.stream = IOStream(socket.socket(socket.AF_UNIX))

        self.io_loop.run_sync(lambda: self.stream.connect(self.sockfile))



    def tearDown(self):

        self.stream.close()

        self.io_loop.run_sync(self.server.close_all_connections)

        self.server.stop()

        shutil.rmtree(self.tmpdir)

        super(UnixSocketTest, self).tearDown()



    @gen_test

    def test_unix_socket(self):

        self.stream.write(b"GET /hello HTTP/1.0\r\n\r\n")

        response = yield self.stream.read_until(b"\r\n")

        self.assertEqual(response, b"HTTP/1.1 200 OK\r\n")

        header_data = yield self.stream.read_until(b"\r\n\r\n")

        headers = HTTPHeaders.parse(header_data.decode("latin1"))

        body = yield self.stream.read_bytes(int(headers["Content-Length"]))

        self.assertEqual(body, b"Hello world")



    @gen_test

    def test_unix_socket_bad_request(self):

        # Unix sockets don't have remote addresses so they just return an

        # empty string.

        with ExpectLog(gen_log, "Malformed HTTP message from"):

            self.stream.write(b"garbage\r\n\r\n")

            response = yield self.stream.read_until_close()

        self.assertEqual(response, b"HTTP/1.1 400 Bad Request\r\n\r\n")





class KeepAliveTest(AsyncHTTPTestCase):

    """Tests various scenarios for HTTP 1.1 keep-alive support.



    These tests don't use AsyncHTTPClient because we want to control

    connection reuse and closing.

    """



    def get_app(self):

        class HelloHandler(RequestHandler):

            def get(self):

                self.finish("Hello world")



            def post(self):

                self.finish("Hello world")



        class LargeHandler(RequestHandler):

            def get(self):

                # 512KB should be bigger than the socket buffers so it will

                # be written out in chunks.

                self.write("".join(chr(i % 256) * 1024 for i in range(512)))



        class FinishOnCloseHandler(RequestHandler):

            def initialize(self, cleanup_event):

                self.cleanup_event = cleanup_event



            @gen.coroutine

            def get(self):

                self.flush()

                yield self.cleanup_event.wait()



            def on_connection_close(self):

                # This is not very realistic, but finishing the request

                # from the close callback has the right timing to mimic

                # some errors seen in the wild.

                self.finish("closed")



        self.cleanup_event = Event()

        return Application(

            [

                ("/", HelloHandler),

                ("/large", LargeHandler),

                (

                    "/finish_on_close",

                    FinishOnCloseHandler,

                    dict(cleanup_event=self.cleanup_event),

                ),

            ]

        )



    def setUp(self):

        super(KeepAliveTest, self).setUp()

        self.http_version = b"HTTP/1.1"



    def tearDown(self):

        # We just closed the client side of the socket; let the IOLoop run

        # once to make sure the server side got the message.

        self.io_loop.add_timeout(datetime.timedelta(seconds=0.001), self.stop)

        self.wait()



        if hasattr(self, "stream"):

            self.stream.close()

        super(KeepAliveTest, self).tearDown()



    # The next few methods are a crude manual http client

    @gen.coroutine

    def connect(self):

        self.stream = IOStream(socket.socket())

        yield self.stream.connect(("127.0.0.1", self.get_http_port()))



    @gen.coroutine

    def read_headers(self):

        first_line = yield self.stream.read_until(b"\r\n")

        self.assertTrue(first_line.startswith(b"HTTP/1.1 200"), first_line)

        header_bytes = yield self.stream.read_until(b"\r\n\r\n")

        headers = HTTPHeaders.parse(header_bytes.decode("latin1"))

        raise gen.Return(headers)



    @gen.coroutine

    def read_response(self):

        self.headers = yield self.read_headers()

        body = yield self.stream.read_bytes(int(self.headers["Content-Length"]))

        self.assertEqual(b"Hello world", body)



    def close(self):

        self.stream.close()

        del self.stream



    @gen_test

    def test_two_requests(self):

        yield self.connect()

        self.stream.write(b"GET / HTTP/1.1\r\n\r\n")

        yield self.read_response()

        self.stream.write(b"GET / HTTP/1.1\r\n\r\n")

        yield self.read_response()

        self.close()



    @gen_test

    def test_request_close(self):

        yield self.connect()

        self.stream.write(b"GET / HTTP/1.1\r\nConnection: close\r\n\r\n")

        yield self.read_response()

        data = yield self.stream.read_until_close()

        self.assertTrue(not data)

        self.assertEqual(self.headers["Connection"], "close")

        self.close()



    # keepalive is supported for http 1.0 too, but it's opt-in

    @gen_test

    def test_http10(self):

        self.http_version = b"HTTP/1.0"

        yield self.connect()

        self.stream.write(b"GET / HTTP/1.0\r\n\r\n")

        yield self.read_response()

        data = yield self.stream.read_until_close()

        self.assertTrue(not data)

        self.assertTrue("Connection" not in self.headers)

        self.close()



    @gen_test

    def test_http10_keepalive(self):

        self.http_version = b"HTTP/1.0"

        yield self.connect()

        self.stream.write(b"GET / HTTP/1.0\r\nConnection: keep-alive\r\n\r\n")

        yield self.read_response()

        self.assertEqual(self.headers["Connection"], "Keep-Alive")

        self.stream.write(b"GET / HTTP/1.0\r\nConnection: keep-alive\r\n\r\n")

        yield self.read_response()

        self.assertEqual(self.headers["Connection"], "Keep-Alive")

        self.close()



    @gen_test

    def test_http10_keepalive_extra_crlf(self):

        self.http_version = b"HTTP/1.0"

        yield self.connect()

        self.stream.write(b"GET / HTTP/1.0\r\nConnection: keep-alive\r\n\r\n\r\n")

        yield self.read_response()

        self.assertEqual(self.headers["Connection"], "Keep-Alive")

        self.stream.write(b"GET / HTTP/1.0\r\nConnection: keep-alive\r\n\r\n")

        yield self.read_response()

        self.assertEqual(self.headers["Connection"], "Keep-Alive")

        self.close()



    @gen_test

    def test_pipelined_requests(self):

        yield self.connect()

        self.stream.write(b"GET / HTTP/1.1\r\n\r\nGET / HTTP/1.1\r\n\r\n")

        yield self.read_response()

        yield self.read_response()

        self.close()



    @gen_test

    def test_pipelined_cancel(self):

        yield self.connect()

        self.stream.write(b"GET / HTTP/1.1\r\n\r\nGET / HTTP/1.1\r\n\r\n")

        # only read once

        yield self.read_response()

        self.close()



    @gen_test

    def test_cancel_during_download(self):

        yield self.connect()

        self.stream.write(b"GET /large HTTP/1.1\r\n\r\n")

        yield self.read_headers()

        yield self.stream.read_bytes(1024)

        self.close()



    @gen_test

    def test_finish_while_closed(self):

        yield self.connect()

        self.stream.write(b"GET /finish_on_close HTTP/1.1\r\n\r\n")

        yield self.read_headers()

        self.close()

        # Let the hanging coroutine clean up after itself

        self.cleanup_event.set()



    @gen_test

    def test_keepalive_chunked(self):

        self.http_version = b"HTTP/1.0"

        yield self.connect()

        self.stream.write(

            b"POST / HTTP/1.0\r\n"

            b"Connection: keep-alive\r\n"

            b"Transfer-Encoding: chunked\r\n"

            b"\r\n"

            b"0\r\n"

            b"\r\n"

        )

        yield self.read_response()

        self.assertEqual(self.headers["Connection"], "Keep-Alive")

        self.stream.write(b"GET / HTTP/1.0\r\nConnection: keep-alive\r\n\r\n")

        yield self.read_response()

        self.assertEqual(self.headers["Connection"], "Keep-Alive")

        self.close()





class GzipBaseTest(object):

    def get_app(self):

        return Application([("/", EchoHandler)])



    def post_gzip(self, body):

        bytesio = BytesIO()

        gzip_file = gzip.GzipFile(mode="w", fileobj=bytesio)

        gzip_file.write(utf8(body))

        gzip_file.close()

        compressed_body = bytesio.getvalue()

        return self.fetch(

            "/",

            method="POST",

            body=compressed_body,

            headers={"Content-Encoding": "gzip"},

        )



    def test_uncompressed(self):

        response = self.fetch("/", method="POST", body="foo=bar")

        self.assertEquals(json_decode(response.body), {u"foo": [u"bar"]})





class GzipTest(GzipBaseTest, AsyncHTTPTestCase):

    def get_httpserver_options(self):

        return dict(decompress_request=True)



    def test_gzip(self):

        response = self.post_gzip("foo=bar")

        self.assertEquals(json_decode(response.body), {u"foo": [u"bar"]})





class GzipUnsupportedTest(GzipBaseTest, AsyncHTTPTestCase):

    def test_gzip_unsupported(self):

        # Gzip support is opt-in; without it the server fails to parse

        # the body (but parsing form bodies is currently just a log message,

        # not a fatal error).

        with ExpectLog(gen_log, "Unsupported Content-Encoding"):

            response = self.post_gzip("foo=bar")

        self.assertEquals(json_decode(response.body), {})





class StreamingChunkSizeTest(AsyncHTTPTestCase):

    # 50 characters long, and repetitive so it can be compressed.

    BODY = b"01234567890123456789012345678901234567890123456789"

    CHUNK_SIZE = 16



    def get_http_client(self):

        # body_producer doesn't work on curl_httpclient, so override the

        # configured AsyncHTTPClient implementation.

        return SimpleAsyncHTTPClient()



    def get_httpserver_options(self):

        return dict(chunk_size=self.CHUNK_SIZE, decompress_request=True)



    class MessageDelegate(HTTPMessageDelegate):

        def __init__(self, connection):

            self.connection = connection



        def headers_received(self, start_line, headers):

            self.chunk_lengths = []  # type: List[int]



        def data_received(self, chunk):

            self.chunk_lengths.append(len(chunk))



        def finish(self):

            response_body = utf8(json_encode(self.chunk_lengths))

            self.connection.write_headers(

                ResponseStartLine("HTTP/1.1", 200, "OK"),

                HTTPHeaders({"Content-Length": str(len(response_body))}),

            )

            self.connection.write(response_body)

            self.connection.finish()



    def get_app(self):

        class App(HTTPServerConnectionDelegate):

            def start_request(self, server_conn, request_conn):

                return StreamingChunkSizeTest.MessageDelegate(request_conn)



        return App()



    def fetch_chunk_sizes(self, **kwargs):

        response = self.fetch("/", method="POST", **kwargs)

        response.rethrow()

        chunks = json_decode(response.body)

        self.assertEqual(len(self.BODY), sum(chunks))

        for chunk_size in chunks:

            self.assertLessEqual(

                chunk_size, self.CHUNK_SIZE, "oversized chunk: " + str(chunks)

            )

            self.assertGreater(chunk_size, 0, "empty chunk: " + str(chunks))

        return chunks



    def compress(self, body):

        bytesio = BytesIO()

        gzfile = gzip.GzipFile(mode="w", fileobj=bytesio)

        gzfile.write(body)

        gzfile.close()

        compressed = bytesio.getvalue()

        if len(compressed) >= len(body):

            raise Exception("body did not shrink when compressed")

        return compressed



    def test_regular_body(self):

        chunks = self.fetch_chunk_sizes(body=self.BODY)

        # Without compression we know exactly what to expect.

        self.assertEqual([16, 16, 16, 2], chunks)



    def test_compressed_body(self):

        self.fetch_chunk_sizes(

            body=self.compress(self.BODY), headers={"Content-Encoding": "gzip"}

        )

        # Compression creates irregular boundaries so the assertions

        # in fetch_chunk_sizes are as specific as we can get.



    def test_chunked_body(self):

        def body_producer(write):

            write(self.BODY[:20])

            write(self.BODY[20:])



        chunks = self.fetch_chunk_sizes(body_producer=body_producer)

        # HTTP chunk boundaries translate to application-visible breaks

        self.assertEqual([16, 4, 16, 14], chunks)



    def test_chunked_compressed(self):

        compressed = self.compress(self.BODY)

        self.assertGreater(len(compressed), 20)



        def body_producer(write):

            write(compressed[:20])

            write(compressed[20:])



        self.fetch_chunk_sizes(

            body_producer=body_producer, headers={"Content-Encoding": "gzip"}

        )





class MaxHeaderSizeTest(AsyncHTTPTestCase):

    def get_app(self):

        return Application([("/", HelloWorldRequestHandler)])



    def get_httpserver_options(self):

        return dict(max_header_size=1024)



    def test_small_headers(self):

        response = self.fetch("/", headers={"X-Filler": "a" * 100})

        response.rethrow()

        self.assertEqual(response.body, b"Hello world")



    def test_large_headers(self):

        with ExpectLog(gen_log, "Unsatisfiable read", required=False):

            try:

                self.fetch("/", headers={"X-Filler": "a" * 1000}, raise_error=True)

                self.fail("did not raise expected exception")

            except HTTPError as e:

                # 431 is "Request Header Fields Too Large", defined in RFC

                # 6585. However, many implementations just close the

                # connection in this case, resulting in a missing response.

                if e.response is not None:

                    self.assertIn(e.response.code, (431, 599))





@skipOnTravis

class IdleTimeoutTest(AsyncHTTPTestCase):

    def get_app(self):

        return Application([("/", HelloWorldRequestHandler)])



    def get_httpserver_options(self):

        return dict(idle_connection_timeout=0.1)



    def setUp(self):

        super(IdleTimeoutTest, self).setUp()

        self.streams = []  # type: List[IOStream]



    def tearDown(self):

        super(IdleTimeoutTest, self).tearDown()

        for stream in self.streams:

            stream.close()



    @gen.coroutine

    def connect(self):

        stream = IOStream(socket.socket())

        yield stream.connect(("127.0.0.1", self.get_http_port()))

        self.streams.append(stream)

        raise gen.Return(stream)



    @gen_test

    def test_unused_connection(self):

        stream = yield self.connect()

        event = Event()

        stream.set_close_callback(event.set)

        yield event.wait()



    @gen_test

    def test_idle_after_use(self):

        stream = yield self.connect()

        event = Event()

        stream.set_close_callback(event.set)



        # Use the connection twice to make sure keep-alives are working

        for i in range(2):

            stream.write(b"GET / HTTP/1.1\r\n\r\n")

            yield stream.read_until(b"\r\n\r\n")

            data = yield stream.read_bytes(11)

            self.assertEqual(data, b"Hello world")



        # Now let the timeout trigger and close the connection.

        yield event.wait()





class BodyLimitsTest(AsyncHTTPTestCase):

    def get_app(self):

        class BufferedHandler(RequestHandler):

            def put(self):

                self.write(str(len(self.request.body)))



        @stream_request_body

        class StreamingHandler(RequestHandler):

            def initialize(self):

                self.bytes_read = 0



            def prepare(self):

                if "expected_size" in self.request.arguments:

                    self.request.connection.set_max_body_size(

                        int(self.get_argument("expected_size"))

                    )

                if "body_timeout" in self.request.arguments:

                    self.request.connection.set_body_timeout(

                        float(self.get_argument("body_timeout"))

                    )



            def data_received(self, data):

                self.bytes_read += len(data)



            def put(self):

                self.write(str(self.bytes_read))



        return Application(

            [("/buffered", BufferedHandler), ("/streaming", StreamingHandler)]

        )



    def get_httpserver_options(self):

        return dict(body_timeout=3600, max_body_size=4096)



    def get_http_client(self):

        # body_producer doesn't work on curl_httpclient, so override the

        # configured AsyncHTTPClient implementation.

        return SimpleAsyncHTTPClient()



    def test_small_body(self):

        response = self.fetch("/buffered", method="PUT", body=b"a" * 4096)

        self.assertEqual(response.body, b"4096")

        response = self.fetch("/streaming", method="PUT", body=b"a" * 4096)

        self.assertEqual(response.body, b"4096")



    def test_large_body_buffered(self):

        with ExpectLog(gen_log, ".*Content-Length too long"):

            response = self.fetch("/buffered", method="PUT", body=b"a" * 10240)

        self.assertEqual(response.code, 400)



    @unittest.skipIf(os.name == "nt", "flaky on windows")

    def test_large_body_buffered_chunked(self):

        # This test is flaky on windows for unknown reasons.

        with ExpectLog(gen_log, ".*chunked body too large"):

            response = self.fetch(

                "/buffered",

                method="PUT",

                body_producer=lambda write: write(b"a" * 10240),

            )

        self.assertEqual(response.code, 400)



    def test_large_body_streaming(self):

        with ExpectLog(gen_log, ".*Content-Length too long"):

            response = self.fetch("/streaming", method="PUT", body=b"a" * 10240)

        self.assertEqual(response.code, 400)



    @unittest.skipIf(os.name == "nt", "flaky on windows")

    def test_large_body_streaming_chunked(self):

        with ExpectLog(gen_log, ".*chunked body too large"):

            response = self.fetch(

                "/streaming",

                method="PUT",

                body_producer=lambda write: write(b"a" * 10240),

            )

        self.assertEqual(response.code, 400)



    def test_large_body_streaming_override(self):

        response = self.fetch(

            "/streaming?expected_size=10240", method="PUT", body=b"a" * 10240

        )

        self.assertEqual(response.body, b"10240")



    def test_large_body_streaming_chunked_override(self):

        response = self.fetch(

            "/streaming?expected_size=10240",

            method="PUT",

            body_producer=lambda write: write(b"a" * 10240),

        )

        self.assertEqual(response.body, b"10240")



    @gen_test

    def test_timeout(self):

        stream = IOStream(socket.socket())

        try:

            yield stream.connect(("127.0.0.1", self.get_http_port()))

            # Use a raw stream because AsyncHTTPClient won't let us read a

            # response without finishing a body.

            stream.write(

                b"PUT /streaming?body_timeout=0.1 HTTP/1.0\r\n"

                b"Content-Length: 42\r\n\r\n"

            )

            with ExpectLog(gen_log, "Timeout reading body"):

                response = yield stream.read_until_close()

            self.assertEqual(response, b"")

        finally:

            stream.close()



    @gen_test

    def test_body_size_override_reset(self):

        # The max_body_size override is reset between requests.

        stream = IOStream(socket.socket())

        try:

            yield stream.connect(("127.0.0.1", self.get_http_port()))

            # Use a raw stream so we can make sure it's all on one connection.

            stream.write(

                b"PUT /streaming?expected_size=10240 HTTP/1.1\r\n"

                b"Content-Length: 10240\r\n\r\n"

            )

            stream.write(b"a" * 10240)

            start_line, headers, response = yield read_stream_body(stream)

            self.assertEqual(response, b"10240")

            # Without the ?expected_size parameter, we get the old default value

            stream.write(

                b"PUT /streaming HTTP/1.1\r\n" b"Content-Length: 10240\r\n\r\n"

            )

            with ExpectLog(gen_log, ".*Content-Length too long"):

                data = yield stream.read_until_close()

            self.assertEqual(data, b"HTTP/1.1 400 Bad Request\r\n\r\n")

        finally:

            stream.close()





class LegacyInterfaceTest(AsyncHTTPTestCase):

    def get_app(self):

        # The old request_callback interface does not implement the

        # delegate interface, and writes its response via request.write

        # instead of request.connection.write_headers.

        def handle_request(request):

            self.http1 = request.version.startswith("HTTP/1.")

            if not self.http1:

                # This test will be skipped if we're using HTTP/2,

                # so just close it out cleanly using the modern interface.

                request.connection.write_headers(

                    ResponseStartLine("", 200, "OK"), HTTPHeaders()

                )

                request.connection.finish()

                return

            message = b"Hello world"

            request.connection.write(

                utf8("HTTP/1.1 200 OK\r\n" "Content-Length: %d\r\n\r\n" % len(message))

            )

            request.connection.write(message)

            request.connection.finish()



        return handle_request



    def test_legacy_interface(self):

        response = self.fetch("/")

        if not self.http1:

            self.skipTest("requires HTTP/1.x")

        self.assertEqual(response.body, b"Hello world")

# -*- coding: utf-8 -*-

from tornado.httputil import (

    url_concat,

    parse_multipart_form_data,

    HTTPHeaders,

    format_timestamp,

    HTTPServerRequest,

    parse_request_start_line,

    parse_cookie,

    qs_to_qsl,

    HTTPInputError,

    HTTPFile,

)

from tornado.escape import utf8, native_str

from tornado.log import gen_log

from tornado.testing import ExpectLog



import copy

import datetime

import logging

import pickle

import time

import urllib.parse

import unittest



from typing import Tuple, Dict, List





def form_data_args() -> Tuple[Dict[str, List[bytes]], Dict[str, List[HTTPFile]]]:

    """Return two empty dicts suitable for use with parse_multipart_form_data.



    mypy insists on type annotations for dict literals, so this lets us avoid

    the verbose types throughout this test.

    """

    return {}, {}





class TestUrlConcat(unittest.TestCase):

    def test_url_concat_no_query_params(self):

        url = url_concat("https://localhost/path", [("y", "y"), ("z", "z")])

        self.assertEqual(url, "https://localhost/path?y=y&z=z")



    def test_url_concat_encode_args(self):

        url = url_concat("https://localhost/path", [("y", "/y"), ("z", "z")])

        self.assertEqual(url, "https://localhost/path?y=%2Fy&z=z")



    def test_url_concat_trailing_q(self):

        url = url_concat("https://localhost/path?", [("y", "y"), ("z", "z")])

        self.assertEqual(url, "https://localhost/path?y=y&z=z")



    def test_url_concat_q_with_no_trailing_amp(self):

        url = url_concat("https://localhost/path?x", [("y", "y"), ("z", "z")])

        self.assertEqual(url, "https://localhost/path?x=&y=y&z=z")



    def test_url_concat_trailing_amp(self):

        url = url_concat("https://localhost/path?x&", [("y", "y"), ("z", "z")])

        self.assertEqual(url, "https://localhost/path?x=&y=y&z=z")



    def test_url_concat_mult_params(self):

        url = url_concat("https://localhost/path?a=1&b=2", [("y", "y"), ("z", "z")])

        self.assertEqual(url, "https://localhost/path?a=1&b=2&y=y&z=z")



    def test_url_concat_no_params(self):

        url = url_concat("https://localhost/path?r=1&t=2", [])

        self.assertEqual(url, "https://localhost/path?r=1&t=2")



    def test_url_concat_none_params(self):

        url = url_concat("https://localhost/path?r=1&t=2", None)

        self.assertEqual(url, "https://localhost/path?r=1&t=2")



    def test_url_concat_with_frag(self):

        url = url_concat("https://localhost/path#tab", [("y", "y")])

        self.assertEqual(url, "https://localhost/path?y=y#tab")



    def test_url_concat_multi_same_params(self):

        url = url_concat("https://localhost/path", [("y", "y1"), ("y", "y2")])

        self.assertEqual(url, "https://localhost/path?y=y1&y=y2")



    def test_url_concat_multi_same_query_params(self):

        url = url_concat("https://localhost/path?r=1&r=2", [("y", "y")])

        self.assertEqual(url, "https://localhost/path?r=1&r=2&y=y")



    def test_url_concat_dict_params(self):

        url = url_concat("https://localhost/path", dict(y="y"))

        self.assertEqual(url, "https://localhost/path?y=y")





class QsParseTest(unittest.TestCase):

    def test_parsing(self):

        qsstring = "a=1&b=2&a=3"

        qs = urllib.parse.parse_qs(qsstring)

        qsl = list(qs_to_qsl(qs))

        self.assertIn(("a", "1"), qsl)

        self.assertIn(("a", "3"), qsl)

        self.assertIn(("b", "2"), qsl)





class MultipartFormDataTest(unittest.TestCase):

    def test_file_upload(self):

        data = b"""\

--1234

Content-Disposition: form-data; name="files"; filename="ab.txt"



Foo

--1234--""".replace(

            b"\n", b"\r\n"

        )

        args, files = form_data_args()

        parse_multipart_form_data(b"1234", data, args, files)

        file = files["files"][0]

        self.assertEqual(file["filename"], "ab.txt")

        self.assertEqual(file["body"], b"Foo")



    def test_unquoted_names(self):

        # quotes are optional unless special characters are present

        data = b"""\

--1234

Content-Disposition: form-data; name=files; filename=ab.txt



Foo

--1234--""".replace(

            b"\n", b"\r\n"

        )

        args, files = form_data_args()

        parse_multipart_form_data(b"1234", data, args, files)

        file = files["files"][0]

        self.assertEqual(file["filename"], "ab.txt")

        self.assertEqual(file["body"], b"Foo")



    def test_special_filenames(self):

        filenames = [

            "a;b.txt",

            'a"b.txt',

            'a";b.txt',

            'a;"b.txt',

            'a";";.txt',

            'a\\"b.txt',

            "a\\b.txt",

        ]

        for filename in filenames:

            logging.debug("trying filename %r", filename)

            str_data = """\

--1234

Content-Disposition: form-data; name="files"; filename="%s"



Foo

--1234--""" % filename.replace(

                "\\", "\\\\"

            ).replace(

                '"', '\\"'

            )

            data = utf8(str_data.replace("\n", "\r\n"))

            args, files = form_data_args()

            parse_multipart_form_data(b"1234", data, args, files)

            file = files["files"][0]

            self.assertEqual(file["filename"], filename)

            self.assertEqual(file["body"], b"Foo")



    def test_non_ascii_filename(self):

        data = b"""\

--1234

Content-Disposition: form-data; name="files"; filename="ab.txt"; filename*=UTF-8''%C3%A1b.txt



Foo

--1234--""".replace(

            b"\n", b"\r\n"

        )

        args, files = form_data_args()

        parse_multipart_form_data(b"1234", data, args, files)

        file = files["files"][0]

        self.assertEqual(file["filename"], u"áb.txt")

        self.assertEqual(file["body"], b"Foo")



    def test_boundary_starts_and_ends_with_quotes(self):

        data = b"""\

--1234

Content-Disposition: form-data; name="files"; filename="ab.txt"



Foo

--1234--""".replace(

            b"\n", b"\r\n"

        )

        args, files = form_data_args()

        parse_multipart_form_data(b'"1234"', data, args, files)

        file = files["files"][0]

        self.assertEqual(file["filename"], "ab.txt")

        self.assertEqual(file["body"], b"Foo")



    def test_missing_headers(self):

        data = b"""\

--1234



Foo

--1234--""".replace(

            b"\n", b"\r\n"

        )

        args, files = form_data_args()

        with ExpectLog(gen_log, "multipart/form-data missing headers"):

            parse_multipart_form_data(b"1234", data, args, files)

        self.assertEqual(files, {})



    def test_invalid_content_disposition(self):

        data = b"""\

--1234

Content-Disposition: invalid; name="files"; filename="ab.txt"



Foo

--1234--""".replace(

            b"\n", b"\r\n"

        )

        args, files = form_data_args()

        with ExpectLog(gen_log, "Invalid multipart/form-data"):

            parse_multipart_form_data(b"1234", data, args, files)

        self.assertEqual(files, {})



    def test_line_does_not_end_with_correct_line_break(self):

        data = b"""\

--1234

Content-Disposition: form-data; name="files"; filename="ab.txt"



Foo--1234--""".replace(

            b"\n", b"\r\n"

        )

        args, files = form_data_args()

        with ExpectLog(gen_log, "Invalid multipart/form-data"):

            parse_multipart_form_data(b"1234", data, args, files)

        self.assertEqual(files, {})



    def test_content_disposition_header_without_name_parameter(self):

        data = b"""\

--1234

Content-Disposition: form-data; filename="ab.txt"



Foo

--1234--""".replace(

            b"\n", b"\r\n"

        )

        args, files = form_data_args()

        with ExpectLog(gen_log, "multipart/form-data value missing name"):

            parse_multipart_form_data(b"1234", data, args, files)

        self.assertEqual(files, {})



    def test_data_after_final_boundary(self):

        # The spec requires that data after the final boundary be ignored.

        # http://www.w3.org/Protocols/rfc1341/7_2_Multipart.html

        # In practice, some libraries include an extra CRLF after the boundary.

        data = b"""\

--1234

Content-Disposition: form-data; name="files"; filename="ab.txt"



Foo

--1234--

""".replace(

            b"\n", b"\r\n"

        )

        args, files = form_data_args()

        parse_multipart_form_data(b"1234", data, args, files)

        file = files["files"][0]

        self.assertEqual(file["filename"], "ab.txt")

        self.assertEqual(file["body"], b"Foo")





class HTTPHeadersTest(unittest.TestCase):

    def test_multi_line(self):

        # Lines beginning with whitespace are appended to the previous line

        # with any leading whitespace replaced by a single space.

        # Note that while multi-line headers are a part of the HTTP spec,

        # their use is strongly discouraged.

        data = """\

Foo: bar

 baz

Asdf: qwer

\tzxcv

Foo: even

     more

     lines

""".replace(

            "\n", "\r\n"

        )

        headers = HTTPHeaders.parse(data)

        self.assertEqual(headers["asdf"], "qwer zxcv")

        self.assertEqual(headers.get_list("asdf"), ["qwer zxcv"])

        self.assertEqual(headers["Foo"], "bar baz,even more lines")

        self.assertEqual(headers.get_list("foo"), ["bar baz", "even more lines"])

        self.assertEqual(

            sorted(list(headers.get_all())),

            [("Asdf", "qwer zxcv"), ("Foo", "bar baz"), ("Foo", "even more lines")],

        )



    def test_malformed_continuation(self):

        # If the first line starts with whitespace, it's a

        # continuation line with nothing to continue, so reject it

        # (with a proper error).

        data = " Foo: bar"

        self.assertRaises(HTTPInputError, HTTPHeaders.parse, data)



    def test_unicode_newlines(self):

        # Ensure that only \r\n is recognized as a header separator, and not

        # the other newline-like unicode characters.

        # Characters that are likely to be problematic can be found in

        # http://unicode.org/standard/reports/tr13/tr13-5.html

        # and cpython's unicodeobject.c (which defines the implementation

        # of unicode_type.splitlines(), and uses a different list than TR13).

        newlines = [

            u"\u001b",  # VERTICAL TAB

            u"\u001c",  # FILE SEPARATOR

            u"\u001d",  # GROUP SEPARATOR

            u"\u001e",  # RECORD SEPARATOR

            u"\u0085",  # NEXT LINE

            u"\u2028",  # LINE SEPARATOR

            u"\u2029",  # PARAGRAPH SEPARATOR

        ]

        for newline in newlines:

            # Try the utf8 and latin1 representations of each newline

            for encoding in ["utf8", "latin1"]:

                try:

                    try:

                        encoded = newline.encode(encoding)

                    except UnicodeEncodeError:

                        # Some chars cannot be represented in latin1

                        continue

                    data = b"Cookie: foo=" + encoded + b"bar"

                    # parse() wants a native_str, so decode through latin1

                    # in the same way the real parser does.

                    headers = HTTPHeaders.parse(native_str(data.decode("latin1")))

                    expected = [

                        (

                            "Cookie",

                            "foo=" + native_str(encoded.decode("latin1")) + "bar",

                        )

                    ]

                    self.assertEqual(expected, list(headers.get_all()))

                except Exception:

                    gen_log.warning("failed while trying %r in %s", newline, encoding)

                    raise



    def test_optional_cr(self):

        # Both CRLF and LF should be accepted as separators. CR should not be

        # part of the data when followed by LF, but it is a normal char

        # otherwise (or should bare CR be an error?)

        headers = HTTPHeaders.parse("CRLF: crlf\r\nLF: lf\nCR: cr\rMore: more\r\n")

        self.assertEqual(

            sorted(headers.get_all()),

            [("Cr", "cr\rMore: more"), ("Crlf", "crlf"), ("Lf", "lf")],

        )



    def test_copy(self):

        all_pairs = [("A", "1"), ("A", "2"), ("B", "c")]

        h1 = HTTPHeaders()

        for k, v in all_pairs:

            h1.add(k, v)

        h2 = h1.copy()

        h3 = copy.copy(h1)

        h4 = copy.deepcopy(h1)

        for headers in [h1, h2, h3, h4]:

            # All the copies are identical, no matter how they were

            # constructed.

            self.assertEqual(list(sorted(headers.get_all())), all_pairs)

        for headers in [h2, h3, h4]:

            # Neither the dict or its member lists are reused.

            self.assertIsNot(headers, h1)

            self.assertIsNot(headers.get_list("A"), h1.get_list("A"))



    def test_pickle_roundtrip(self):

        headers = HTTPHeaders()

        headers.add("Set-Cookie", "a=b")

        headers.add("Set-Cookie", "c=d")

        headers.add("Content-Type", "text/html")

        pickled = pickle.dumps(headers)

        unpickled = pickle.loads(pickled)

        self.assertEqual(sorted(headers.get_all()), sorted(unpickled.get_all()))

        self.assertEqual(sorted(headers.items()), sorted(unpickled.items()))



    def test_setdefault(self):

        headers = HTTPHeaders()

        headers["foo"] = "bar"

        # If a value is present, setdefault returns it without changes.

        self.assertEqual(headers.setdefault("foo", "baz"), "bar")

        self.assertEqual(headers["foo"], "bar")

        # If a value is not present, setdefault sets it for future use.

        self.assertEqual(headers.setdefault("quux", "xyzzy"), "xyzzy")

        self.assertEqual(headers["quux"], "xyzzy")

        self.assertEqual(sorted(headers.get_all()), [("Foo", "bar"), ("Quux", "xyzzy")])



    def test_string(self):

        headers = HTTPHeaders()

        headers.add("Foo", "1")

        headers.add("Foo", "2")

        headers.add("Foo", "3")

        headers2 = HTTPHeaders.parse(str(headers))

        self.assertEquals(headers, headers2)





class FormatTimestampTest(unittest.TestCase):

    # Make sure that all the input types are supported.

    TIMESTAMP = 1359312200.503611

    EXPECTED = "Sun, 27 Jan 2013 18:43:20 GMT"



    def check(self, value):

        self.assertEqual(format_timestamp(value), self.EXPECTED)



    def test_unix_time_float(self):

        self.check(self.TIMESTAMP)



    def test_unix_time_int(self):

        self.check(int(self.TIMESTAMP))



    def test_struct_time(self):

        self.check(time.gmtime(self.TIMESTAMP))



    def test_time_tuple(self):

        tup = tuple(time.gmtime(self.TIMESTAMP))

        self.assertEqual(9, len(tup))

        self.check(tup)



    def test_datetime(self):

        self.check(datetime.datetime.utcfromtimestamp(self.TIMESTAMP))





# HTTPServerRequest is mainly tested incidentally to the server itself,

# but this tests the parts of the class that can be tested in isolation.

class HTTPServerRequestTest(unittest.TestCase):

    def test_default_constructor(self):

        # All parameters are formally optional, but uri is required

        # (and has been for some time).  This test ensures that no

        # more required parameters slip in.

        HTTPServerRequest(uri="/")



    def test_body_is_a_byte_string(self):

        requets = HTTPServerRequest(uri="/")

        self.assertIsInstance(requets.body, bytes)



    def test_repr_does_not_contain_headers(self):

        request = HTTPServerRequest(

            uri="/", headers=HTTPHeaders({"Canary": ["Coal Mine"]})

        )

        self.assertTrue("Canary" not in repr(request))





class ParseRequestStartLineTest(unittest.TestCase):

    METHOD = "GET"

    PATH = "/foo"

    VERSION = "HTTP/1.1"



    def test_parse_request_start_line(self):

        start_line = " ".join([self.METHOD, self.PATH, self.VERSION])

        parsed_start_line = parse_request_start_line(start_line)

        self.assertEqual(parsed_start_line.method, self.METHOD)

        self.assertEqual(parsed_start_line.path, self.PATH)

        self.assertEqual(parsed_start_line.version, self.VERSION)





class ParseCookieTest(unittest.TestCase):

    # These tests copied from Django:

    # https://github.com/django/django/pull/6277/commits/da810901ada1cae9fc1f018f879f11a7fb467b28

    def test_python_cookies(self):

        """

        Test cases copied from Python's Lib/test/test_http_cookies.py

        """

        self.assertEqual(

            parse_cookie("chips=ahoy; vienna=finger"),

            {"chips": "ahoy", "vienna": "finger"},

        )

        # Here parse_cookie() differs from Python's cookie parsing in that it

        # treats all semicolons as delimiters, even within quotes.

        self.assertEqual(

            parse_cookie('keebler="E=mc2; L=\\"Loves\\"; fudge=\\012;"'),

            {"keebler": '"E=mc2', "L": '\\"Loves\\"', "fudge": "\\012", "": '"'},

        )

        # Illegal cookies that have an '=' char in an unquoted value.

        self.assertEqual(parse_cookie("keebler=E=mc2"), {"keebler": "E=mc2"})

        # Cookies with ':' character in their name.

        self.assertEqual(

            parse_cookie("key:term=value:term"), {"key:term": "value:term"}

        )

        # Cookies with '[' and ']'.

        self.assertEqual(

            parse_cookie("a=b; c=[; d=r; f=h"), {"a": "b", "c": "[", "d": "r", "f": "h"}

        )



    def test_cookie_edgecases(self):

        # Cookies that RFC6265 allows.

        self.assertEqual(

            parse_cookie("a=b; Domain=example.com"), {"a": "b", "Domain": "example.com"}

        )

        # parse_cookie() has historically kept only the last cookie with the

        # same name.

        self.assertEqual(parse_cookie("a=b; h=i; a=c"), {"a": "c", "h": "i"})



    def test_invalid_cookies(self):

        """

        Cookie strings that go against RFC6265 but browsers will send if set

        via document.cookie.

        """

        # Chunks without an equals sign appear as unnamed values per

        # https://bugzilla.mozilla.org/show_bug.cgi?id=169091

        self.assertIn(

            "django_language",

            parse_cookie("abc=def; unnamed; django_language=en").keys(),

        )

        # Even a double quote may be an unamed value.

        self.assertEqual(parse_cookie('a=b; "; c=d'), {"a": "b", "": '"', "c": "d"})

        # Spaces in names and values, and an equals sign in values.

        self.assertEqual(

            parse_cookie("a b c=d e = f; gh=i"), {"a b c": "d e = f", "gh": "i"}

        )

        # More characters the spec forbids.

        self.assertEqual(

            parse_cookie('a   b,c<>@:/[]?{}=d  "  =e,f g'),

            {"a   b,c<>@:/[]?{}": 'd  "  =e,f g'},

        )

        # Unicode characters. The spec only allows ASCII.

        self.assertEqual(

            parse_cookie("saint=André Bessette"),

            {"saint": native_str("André Bessette")},

        )

        # Browsers don't send extra whitespace or semicolons in Cookie headers,

        # but parse_cookie() should parse whitespace the same way

        # document.cookie parses whitespace.

        self.assertEqual(

            parse_cookie("  =  b  ;  ;  =  ;   c  =  ;  "), {"": "b", "c": ""}

        )

# flake8: noqa

import subprocess

import sys

import unittest



_import_everything = b"""

# The event loop is not fork-safe, and it's easy to initialize an asyncio.Future

# at startup, which in turn creates the default event loop and prevents forking.

# Explicitly disallow the default event loop so that an error will be raised

# if something tries to touch it.

import asyncio

asyncio.set_event_loop(None)



import tornado.auth

import tornado.autoreload

import tornado.concurrent

import tornado.escape

import tornado.gen

import tornado.http1connection

import tornado.httpclient

import tornado.httpserver

import tornado.httputil

import tornado.ioloop

import tornado.iostream

import tornado.locale

import tornado.log

import tornado.netutil

import tornado.options

import tornado.process

import tornado.simple_httpclient

import tornado.tcpserver

import tornado.tcpclient

import tornado.template

import tornado.testing

import tornado.util

import tornado.web

import tornado.websocket

import tornado.wsgi



try:

    import pycurl

except ImportError:

    pass

else:

    import tornado.curl_httpclient

"""





class ImportTest(unittest.TestCase):

    def test_import_everything(self):

        # Test that all Tornado modules can be imported without side effects,

        # specifically without initializing the default asyncio event loop.

        # Since we can't tell which modules may have already beein imported

        # in our process, do it in a subprocess for a clean slate.

        proc = subprocess.Popen([sys.executable], stdin=subprocess.PIPE)

        proc.communicate(_import_everything)

        self.assertEqual(proc.returncode, 0)



    def test_import_aliases(self):

        # Ensure we don't delete formerly-documented aliases accidentally.

        import tornado.ioloop

        import tornado.gen

        import tornado.util



        self.assertIs(tornado.ioloop.TimeoutError, tornado.util.TimeoutError)

        self.assertIs(tornado.gen.TimeoutError, tornado.util.TimeoutError)

from concurrent.futures import ThreadPoolExecutor

from concurrent import futures

import contextlib

import datetime

import functools

import socket

import subprocess

import sys

import threading

import time

import types

from unittest import mock

import unittest



from tornado.escape import native_str

from tornado import gen

from tornado.ioloop import IOLoop, TimeoutError, PeriodicCallback

from tornado.log import app_log

from tornado.testing import AsyncTestCase, bind_unused_port, ExpectLog, gen_test

from tornado.test.util import skipIfNonUnix, skipOnTravis



import typing



if typing.TYPE_CHECKING:

    from typing import List  # noqa: F401





class TestIOLoop(AsyncTestCase):

    def test_add_callback_return_sequence(self):

        # A callback returning {} or [] shouldn't spin the CPU, see Issue #1803.

        self.calls = 0



        loop = self.io_loop

        test = self

        old_add_callback = loop.add_callback



        def add_callback(self, callback, *args, **kwargs):

            test.calls += 1

            old_add_callback(callback, *args, **kwargs)



        loop.add_callback = types.MethodType(add_callback, loop)

        loop.add_callback(lambda: {})

        loop.add_callback(lambda: [])

        loop.add_timeout(datetime.timedelta(milliseconds=50), loop.stop)

        loop.start()

        self.assertLess(self.calls, 10)



    @skipOnTravis

    def test_add_callback_wakeup(self):

        # Make sure that add_callback from inside a running IOLoop

        # wakes up the IOLoop immediately instead of waiting for a timeout.

        def callback():

            self.called = True

            self.stop()



        def schedule_callback():

            self.called = False

            self.io_loop.add_callback(callback)

            # Store away the time so we can check if we woke up immediately

            self.start_time = time.time()



        self.io_loop.add_timeout(self.io_loop.time(), schedule_callback)

        self.wait()

        self.assertAlmostEqual(time.time(), self.start_time, places=2)

        self.assertTrue(self.called)



    @skipOnTravis

    def test_add_callback_wakeup_other_thread(self):

        def target():

            # sleep a bit to let the ioloop go into its poll loop

            time.sleep(0.01)

            self.stop_time = time.time()

            self.io_loop.add_callback(self.stop)



        thread = threading.Thread(target=target)

        self.io_loop.add_callback(thread.start)

        self.wait()

        delta = time.time() - self.stop_time

        self.assertLess(delta, 0.1)

        thread.join()



    def test_add_timeout_timedelta(self):

        self.io_loop.add_timeout(datetime.timedelta(microseconds=1), self.stop)

        self.wait()



    def test_multiple_add(self):

        sock, port = bind_unused_port()

        try:

            self.io_loop.add_handler(

                sock.fileno(), lambda fd, events: None, IOLoop.READ

            )

            # Attempting to add the same handler twice fails

            # (with a platform-dependent exception)

            self.assertRaises(

                Exception,

                self.io_loop.add_handler,

                sock.fileno(),

                lambda fd, events: None,

                IOLoop.READ,

            )

        finally:

            self.io_loop.remove_handler(sock.fileno())

            sock.close()



    def test_remove_without_add(self):

        # remove_handler should not throw an exception if called on an fd

        # was never added.

        sock, port = bind_unused_port()

        try:

            self.io_loop.remove_handler(sock.fileno())

        finally:

            sock.close()



    def test_add_callback_from_signal(self):

        # cheat a little bit and just run this normally, since we can't

        # easily simulate the races that happen with real signal handlers

        self.io_loop.add_callback_from_signal(self.stop)

        self.wait()



    def test_add_callback_from_signal_other_thread(self):

        # Very crude test, just to make sure that we cover this case.

        # This also happens to be the first test where we run an IOLoop in

        # a non-main thread.

        other_ioloop = IOLoop()

        thread = threading.Thread(target=other_ioloop.start)

        thread.start()

        other_ioloop.add_callback_from_signal(other_ioloop.stop)

        thread.join()

        other_ioloop.close()



    def test_add_callback_while_closing(self):

        # add_callback should not fail if it races with another thread

        # closing the IOLoop. The callbacks are dropped silently

        # without executing.

        closing = threading.Event()



        def target():

            other_ioloop.add_callback(other_ioloop.stop)

            other_ioloop.start()

            closing.set()

            other_ioloop.close(all_fds=True)



        other_ioloop = IOLoop()

        thread = threading.Thread(target=target)

        thread.start()

        closing.wait()

        for i in range(1000):

            other_ioloop.add_callback(lambda: None)



    @skipIfNonUnix  # just because socketpair is so convenient

    def test_read_while_writeable(self):

        # Ensure that write events don't come in while we're waiting for

        # a read and haven't asked for writeability. (the reverse is

        # difficult to test for)

        client, server = socket.socketpair()

        try:



            def handler(fd, events):

                self.assertEqual(events, IOLoop.READ)

                self.stop()



            self.io_loop.add_handler(client.fileno(), handler, IOLoop.READ)

            self.io_loop.add_timeout(

                self.io_loop.time() + 0.01, functools.partial(server.send, b"asdf")

            )

            self.wait()

            self.io_loop.remove_handler(client.fileno())

        finally:

            client.close()

            server.close()



    def test_remove_timeout_after_fire(self):

        # It is not an error to call remove_timeout after it has run.

        handle = self.io_loop.add_timeout(self.io_loop.time(), self.stop)

        self.wait()

        self.io_loop.remove_timeout(handle)



    def test_remove_timeout_cleanup(self):

        # Add and remove enough callbacks to trigger cleanup.

        # Not a very thorough test, but it ensures that the cleanup code

        # gets executed and doesn't blow up.  This test is only really useful

        # on PollIOLoop subclasses, but it should run silently on any

        # implementation.

        for i in range(2000):

            timeout = self.io_loop.add_timeout(self.io_loop.time() + 3600, lambda: None)

            self.io_loop.remove_timeout(timeout)

        # HACK: wait two IOLoop iterations for the GC to happen.

        self.io_loop.add_callback(lambda: self.io_loop.add_callback(self.stop))

        self.wait()



    def test_remove_timeout_from_timeout(self):

        calls = [False, False]



        # Schedule several callbacks and wait for them all to come due at once.

        # t2 should be cancelled by t1, even though it is already scheduled to

        # be run before the ioloop even looks at it.

        now = self.io_loop.time()



        def t1():

            calls[0] = True

            self.io_loop.remove_timeout(t2_handle)



        self.io_loop.add_timeout(now + 0.01, t1)



        def t2():

            calls[1] = True



        t2_handle = self.io_loop.add_timeout(now + 0.02, t2)

        self.io_loop.add_timeout(now + 0.03, self.stop)

        time.sleep(0.03)

        self.wait()

        self.assertEqual(calls, [True, False])



    def test_timeout_with_arguments(self):

        # This tests that all the timeout methods pass through *args correctly.

        results = []  # type: List[int]

        self.io_loop.add_timeout(self.io_loop.time(), results.append, 1)

        self.io_loop.add_timeout(datetime.timedelta(seconds=0), results.append, 2)

        self.io_loop.call_at(self.io_loop.time(), results.append, 3)

        self.io_loop.call_later(0, results.append, 4)

        self.io_loop.call_later(0, self.stop)

        self.wait()

        # The asyncio event loop does not guarantee the order of these

        # callbacks.

        self.assertEqual(sorted(results), [1, 2, 3, 4])



    def test_add_timeout_return(self):

        # All the timeout methods return non-None handles that can be

        # passed to remove_timeout.

        handle = self.io_loop.add_timeout(self.io_loop.time(), lambda: None)

        self.assertFalse(handle is None)

        self.io_loop.remove_timeout(handle)



    def test_call_at_return(self):

        handle = self.io_loop.call_at(self.io_loop.time(), lambda: None)

        self.assertFalse(handle is None)

        self.io_loop.remove_timeout(handle)



    def test_call_later_return(self):

        handle = self.io_loop.call_later(0, lambda: None)

        self.assertFalse(handle is None)

        self.io_loop.remove_timeout(handle)



    def test_close_file_object(self):

        """When a file object is used instead of a numeric file descriptor,

        the object should be closed (by IOLoop.close(all_fds=True),

        not just the fd.

        """

        # Use a socket since they are supported by IOLoop on all platforms.

        # Unfortunately, sockets don't support the .closed attribute for

        # inspecting their close status, so we must use a wrapper.

        class SocketWrapper(object):

            def __init__(self, sockobj):

                self.sockobj = sockobj

                self.closed = False



            def fileno(self):

                return self.sockobj.fileno()



            def close(self):

                self.closed = True

                self.sockobj.close()



        sockobj, port = bind_unused_port()

        socket_wrapper = SocketWrapper(sockobj)

        io_loop = IOLoop()

        io_loop.add_handler(socket_wrapper, lambda fd, events: None, IOLoop.READ)

        io_loop.close(all_fds=True)

        self.assertTrue(socket_wrapper.closed)



    def test_handler_callback_file_object(self):

        """The handler callback receives the same fd object it passed in."""

        server_sock, port = bind_unused_port()

        fds = []



        def handle_connection(fd, events):

            fds.append(fd)

            conn, addr = server_sock.accept()

            conn.close()

            self.stop()



        self.io_loop.add_handler(server_sock, handle_connection, IOLoop.READ)

        with contextlib.closing(socket.socket()) as client_sock:

            client_sock.connect(("127.0.0.1", port))

            self.wait()

        self.io_loop.remove_handler(server_sock)

        self.io_loop.add_handler(server_sock.fileno(), handle_connection, IOLoop.READ)

        with contextlib.closing(socket.socket()) as client_sock:

            client_sock.connect(("127.0.0.1", port))

            self.wait()

        self.assertIs(fds[0], server_sock)

        self.assertEqual(fds[1], server_sock.fileno())

        self.io_loop.remove_handler(server_sock.fileno())

        server_sock.close()



    def test_mixed_fd_fileobj(self):

        server_sock, port = bind_unused_port()



        def f(fd, events):

            pass



        self.io_loop.add_handler(server_sock, f, IOLoop.READ)

        with self.assertRaises(Exception):

            # The exact error is unspecified - some implementations use

            # IOError, others use ValueError.

            self.io_loop.add_handler(server_sock.fileno(), f, IOLoop.READ)

        self.io_loop.remove_handler(server_sock.fileno())

        server_sock.close()



    def test_reentrant(self):

        """Calling start() twice should raise an error, not deadlock."""

        returned_from_start = [False]

        got_exception = [False]



        def callback():

            try:

                self.io_loop.start()

                returned_from_start[0] = True

            except Exception:

                got_exception[0] = True

            self.stop()



        self.io_loop.add_callback(callback)

        self.wait()

        self.assertTrue(got_exception[0])

        self.assertFalse(returned_from_start[0])



    def test_exception_logging(self):

        """Uncaught exceptions get logged by the IOLoop."""

        self.io_loop.add_callback(lambda: 1 / 0)

        self.io_loop.add_callback(self.stop)

        with ExpectLog(app_log, "Exception in callback"):

            self.wait()



    def test_exception_logging_future(self):

        """The IOLoop examines exceptions from Futures and logs them."""



        @gen.coroutine

        def callback():

            self.io_loop.add_callback(self.stop)

            1 / 0



        self.io_loop.add_callback(callback)

        with ExpectLog(app_log, "Exception in callback"):

            self.wait()



    def test_exception_logging_native_coro(self):

        """The IOLoop examines exceptions from awaitables and logs them."""



        async def callback():

            # Stop the IOLoop two iterations after raising an exception

            # to give the exception time to be logged.

            self.io_loop.add_callback(self.io_loop.add_callback, self.stop)

            1 / 0



        self.io_loop.add_callback(callback)

        with ExpectLog(app_log, "Exception in callback"):

            self.wait()



    def test_spawn_callback(self):

        # Both add_callback and spawn_callback run directly on the IOLoop,

        # so their errors are logged without stopping the test.

        self.io_loop.add_callback(lambda: 1 / 0)

        self.io_loop.add_callback(self.stop)

        with ExpectLog(app_log, "Exception in callback"):

            self.wait()

        # A spawned callback is run directly on the IOLoop, so it will be

        # logged without stopping the test.

        self.io_loop.spawn_callback(lambda: 1 / 0)

        self.io_loop.add_callback(self.stop)

        with ExpectLog(app_log, "Exception in callback"):

            self.wait()



    @skipIfNonUnix

    def test_remove_handler_from_handler(self):

        # Create two sockets with simultaneous read events.

        client, server = socket.socketpair()

        try:

            client.send(b"abc")

            server.send(b"abc")



            # After reading from one fd, remove the other from the IOLoop.

            chunks = []



            def handle_read(fd, events):

                chunks.append(fd.recv(1024))

                if fd is client:

                    self.io_loop.remove_handler(server)

                else:

                    self.io_loop.remove_handler(client)



            self.io_loop.add_handler(client, handle_read, self.io_loop.READ)

            self.io_loop.add_handler(server, handle_read, self.io_loop.READ)

            self.io_loop.call_later(0.1, self.stop)

            self.wait()



            # Only one fd was read; the other was cleanly removed.

            self.assertEqual(chunks, [b"abc"])

        finally:

            client.close()

            server.close()



    @gen_test

    def test_init_close_race(self):

        # Regression test for #2367

        def f():

            for i in range(10):

                loop = IOLoop()

                loop.close()



        yield gen.multi([self.io_loop.run_in_executor(None, f) for i in range(2)])





# Deliberately not a subclass of AsyncTestCase so the IOLoop isn't

# automatically set as current.

class TestIOLoopCurrent(unittest.TestCase):

    def setUp(self):

        self.io_loop = None

        IOLoop.clear_current()



    def tearDown(self):

        if self.io_loop is not None:

            self.io_loop.close()



    def test_default_current(self):

        self.io_loop = IOLoop()

        # The first IOLoop with default arguments is made current.

        self.assertIs(self.io_loop, IOLoop.current())

        # A second IOLoop can be created but is not made current.

        io_loop2 = IOLoop()

        self.assertIs(self.io_loop, IOLoop.current())

        io_loop2.close()



    def test_non_current(self):

        self.io_loop = IOLoop(make_current=False)

        # The new IOLoop is not initially made current.

        self.assertIsNone(IOLoop.current(instance=False))

        # Starting the IOLoop makes it current, and stopping the loop

        # makes it non-current. This process is repeatable.

        for i in range(3):



            def f():

                self.current_io_loop = IOLoop.current()

                self.io_loop.stop()



            self.io_loop.add_callback(f)

            self.io_loop.start()

            self.assertIs(self.current_io_loop, self.io_loop)

            # Now that the loop is stopped, it is no longer current.

            self.assertIsNone(IOLoop.current(instance=False))



    def test_force_current(self):

        self.io_loop = IOLoop(make_current=True)

        self.assertIs(self.io_loop, IOLoop.current())

        with self.assertRaises(RuntimeError):

            # A second make_current=True construction cannot succeed.

            IOLoop(make_current=True)

        # current() was not affected by the failed construction.

        self.assertIs(self.io_loop, IOLoop.current())





class TestIOLoopCurrentAsync(AsyncTestCase):

    @gen_test

    def test_clear_without_current(self):

        # If there is no current IOLoop, clear_current is a no-op (but

        # should not fail). Use a thread so we see the threading.Local

        # in a pristine state.

        with ThreadPoolExecutor(1) as e:

            yield e.submit(IOLoop.clear_current)





class TestIOLoopFutures(AsyncTestCase):

    def test_add_future_threads(self):

        with futures.ThreadPoolExecutor(1) as pool:



            def dummy():

                pass



            self.io_loop.add_future(

                pool.submit(dummy), lambda future: self.stop(future)

            )

            future = self.wait()

            self.assertTrue(future.done())

            self.assertTrue(future.result() is None)



    @gen_test

    def test_run_in_executor_gen(self):

        event1 = threading.Event()

        event2 = threading.Event()



        def sync_func(self_event, other_event):

            self_event.set()

            other_event.wait()

            # Note that return value doesn't actually do anything,

            # it is just passed through to our final assertion to

            # make sure it is passed through properly.

            return self_event



        # Run two synchronous functions, which would deadlock if not

        # run in parallel.

        res = yield [

            IOLoop.current().run_in_executor(None, sync_func, event1, event2),

            IOLoop.current().run_in_executor(None, sync_func, event2, event1),

        ]



        self.assertEqual([event1, event2], res)



    @gen_test

    def test_run_in_executor_native(self):

        event1 = threading.Event()

        event2 = threading.Event()



        def sync_func(self_event, other_event):

            self_event.set()

            other_event.wait()

            return self_event



        # Go through an async wrapper to ensure that the result of

        # run_in_executor works with await and not just gen.coroutine

        # (simply passing the underlying concurrrent future would do that).

        async def async_wrapper(self_event, other_event):

            return await IOLoop.current().run_in_executor(

                None, sync_func, self_event, other_event

            )



        res = yield [async_wrapper(event1, event2), async_wrapper(event2, event1)]



        self.assertEqual([event1, event2], res)



    @gen_test

    def test_set_default_executor(self):

        count = [0]



        class MyExecutor(futures.ThreadPoolExecutor):

            def submit(self, func, *args):

                count[0] += 1

                return super(MyExecutor, self).submit(func, *args)



        event = threading.Event()



        def sync_func():

            event.set()



        executor = MyExecutor(1)

        loop = IOLoop.current()

        loop.set_default_executor(executor)

        yield loop.run_in_executor(None, sync_func)

        self.assertEqual(1, count[0])

        self.assertTrue(event.is_set())





class TestIOLoopRunSync(unittest.TestCase):

    def setUp(self):

        self.io_loop = IOLoop()



    def tearDown(self):

        self.io_loop.close()



    def test_sync_result(self):

        with self.assertRaises(gen.BadYieldError):

            self.io_loop.run_sync(lambda: 42)



    def test_sync_exception(self):

        with self.assertRaises(ZeroDivisionError):

            self.io_loop.run_sync(lambda: 1 / 0)



    def test_async_result(self):

        @gen.coroutine

        def f():

            yield gen.moment

            raise gen.Return(42)



        self.assertEqual(self.io_loop.run_sync(f), 42)



    def test_async_exception(self):

        @gen.coroutine

        def f():

            yield gen.moment

            1 / 0



        with self.assertRaises(ZeroDivisionError):

            self.io_loop.run_sync(f)



    def test_current(self):

        def f():

            self.assertIs(IOLoop.current(), self.io_loop)



        self.io_loop.run_sync(f)



    def test_timeout(self):

        @gen.coroutine

        def f():

            yield gen.sleep(1)



        self.assertRaises(TimeoutError, self.io_loop.run_sync, f, timeout=0.01)



    def test_native_coroutine(self):

        @gen.coroutine

        def f1():

            yield gen.moment



        async def f2():

            await f1()



        self.io_loop.run_sync(f2)





class TestPeriodicCallbackMath(unittest.TestCase):

    def simulate_calls(self, pc, durations):

        """Simulate a series of calls to the PeriodicCallback.



        Pass a list of call durations in seconds (negative values

        work to simulate clock adjustments during the call, or more or

        less equivalently, between calls). This method returns the

        times at which each call would be made.

        """

        calls = []

        now = 1000

        pc._next_timeout = now

        for d in durations:

            pc._update_next(now)

            calls.append(pc._next_timeout)

            now = pc._next_timeout + d

        return calls



    def dummy(self):

        pass



    def test_basic(self):

        pc = PeriodicCallback(self.dummy, 10000)

        self.assertEqual(

            self.simulate_calls(pc, [0] * 5), [1010, 1020, 1030, 1040, 1050]

        )



    def test_overrun(self):

        # If a call runs for too long, we skip entire cycles to get

        # back on schedule.

        call_durations = [9, 9, 10, 11, 20, 20, 35, 35, 0, 0, 0]

        expected = [

            1010,

            1020,

            1030,  # first 3 calls on schedule

            1050,

            1070,  # next 2 delayed one cycle

            1100,

            1130,  # next 2 delayed 2 cycles

            1170,

            1210,  # next 2 delayed 3 cycles

            1220,

            1230,  # then back on schedule.

        ]



        pc = PeriodicCallback(self.dummy, 10000)

        self.assertEqual(self.simulate_calls(pc, call_durations), expected)



    def test_clock_backwards(self):

        pc = PeriodicCallback(self.dummy, 10000)

        # Backwards jumps are ignored, potentially resulting in a

        # slightly slow schedule (although we assume that when

        # time.time() and time.monotonic() are different, time.time()

        # is getting adjusted by NTP and is therefore more accurate)

        self.assertEqual(

            self.simulate_calls(pc, [-2, -1, -3, -2, 0]), [1010, 1020, 1030, 1040, 1050]

        )



        # For big jumps, we should perhaps alter the schedule, but we

        # don't currently. This trace shows that we run callbacks

        # every 10s of time.time(), but the first and second calls are

        # 110s of real time apart because the backwards jump is

        # ignored.

        self.assertEqual(self.simulate_calls(pc, [-100, 0, 0]), [1010, 1020, 1030])



    def test_jitter(self):

        random_times = [0.5, 1, 0, 0.75]

        expected = [1010, 1022.5, 1030, 1041.25]

        call_durations = [0] * len(random_times)

        pc = PeriodicCallback(self.dummy, 10000, jitter=0.5)



        def mock_random():

            return random_times.pop(0)



        with mock.patch("random.random", mock_random):

            self.assertEqual(self.simulate_calls(pc, call_durations), expected)





class TestIOLoopConfiguration(unittest.TestCase):

    def run_python(self, *statements):

        stmt_list = [

            "from tornado.ioloop import IOLoop",

            "classname = lambda x: x.__class__.__name__",

        ] + list(statements)

        args = [sys.executable, "-c", "; ".join(stmt_list)]

        return native_str(subprocess.check_output(args)).strip()



    def test_default(self):

        # When asyncio is available, it is used by default.

        cls = self.run_python("print(classname(IOLoop.current()))")

        self.assertEqual(cls, "AsyncIOMainLoop")

        cls = self.run_python("print(classname(IOLoop()))")

        self.assertEqual(cls, "AsyncIOLoop")



    def test_asyncio(self):

        cls = self.run_python(

            'IOLoop.configure("tornado.platform.asyncio.AsyncIOLoop")',

            "print(classname(IOLoop.current()))",

        )

        self.assertEqual(cls, "AsyncIOMainLoop")



    def test_asyncio_main(self):

        cls = self.run_python(

            "from tornado.platform.asyncio import AsyncIOMainLoop",

            "AsyncIOMainLoop().install()",

            "print(classname(IOLoop.current()))",

        )

        self.assertEqual(cls, "AsyncIOMainLoop")





if __name__ == "__main__":

    unittest.main()

from tornado.concurrent import Future

from tornado import gen

from tornado import netutil

from tornado.iostream import (

    IOStream,

    SSLIOStream,

    PipeIOStream,

    StreamClosedError,

    _StreamBuffer,

)

from tornado.httputil import HTTPHeaders

from tornado.locks import Condition, Event

from tornado.log import gen_log

from tornado.netutil import ssl_wrap_socket

from tornado.tcpserver import TCPServer

from tornado.testing import (

    AsyncHTTPTestCase,

    AsyncHTTPSTestCase,

    AsyncTestCase,

    bind_unused_port,

    ExpectLog,

    gen_test,

)

from tornado.test.util import skipIfNonUnix, refusing_port, skipPypy3V58

from tornado.web import RequestHandler, Application

import errno

import hashlib

import os

import platform

import random

import socket

import ssl

import sys

from unittest import mock

import unittest





def _server_ssl_options():

    return dict(

        certfile=os.path.join(os.path.dirname(__file__), "test.crt"),

        keyfile=os.path.join(os.path.dirname(__file__), "test.key"),

    )





class HelloHandler(RequestHandler):

    def get(self):

        self.write("Hello")





class TestIOStreamWebMixin(object):

    def _make_client_iostream(self):

        raise NotImplementedError()



    def get_app(self):

        return Application([("/", HelloHandler)])



    def test_connection_closed(self):

        # When a server sends a response and then closes the connection,

        # the client must be allowed to read the data before the IOStream

        # closes itself.  Epoll reports closed connections with a separate

        # EPOLLRDHUP event delivered at the same time as the read event,

        # while kqueue reports them as a second read/write event with an EOF

        # flag.

        response = self.fetch("/", headers={"Connection": "close"})

        response.rethrow()



    @gen_test

    def test_read_until_close(self):

        stream = self._make_client_iostream()

        yield stream.connect(("127.0.0.1", self.get_http_port()))

        stream.write(b"GET / HTTP/1.0\r\n\r\n")



        data = yield stream.read_until_close()

        self.assertTrue(data.startswith(b"HTTP/1.1 200"))

        self.assertTrue(data.endswith(b"Hello"))



    @gen_test

    def test_read_zero_bytes(self):

        self.stream = self._make_client_iostream()

        yield self.stream.connect(("127.0.0.1", self.get_http_port()))

        self.stream.write(b"GET / HTTP/1.0\r\n\r\n")



        # normal read

        data = yield self.stream.read_bytes(9)

        self.assertEqual(data, b"HTTP/1.1 ")



        # zero bytes

        data = yield self.stream.read_bytes(0)

        self.assertEqual(data, b"")



        # another normal read

        data = yield self.stream.read_bytes(3)

        self.assertEqual(data, b"200")



        self.stream.close()



    @gen_test

    def test_write_while_connecting(self):

        stream = self._make_client_iostream()

        connect_fut = stream.connect(("127.0.0.1", self.get_http_port()))

        # unlike the previous tests, try to write before the connection

        # is complete.

        write_fut = stream.write(b"GET / HTTP/1.0\r\nConnection: close\r\n\r\n")

        self.assertFalse(connect_fut.done())



        # connect will always complete before write.

        it = gen.WaitIterator(connect_fut, write_fut)

        resolved_order = []

        while not it.done():

            yield it.next()

            resolved_order.append(it.current_future)

        self.assertEqual(resolved_order, [connect_fut, write_fut])



        data = yield stream.read_until_close()

        self.assertTrue(data.endswith(b"Hello"))



        stream.close()



    @gen_test

    def test_future_interface(self):

        """Basic test of IOStream's ability to return Futures."""

        stream = self._make_client_iostream()

        connect_result = yield stream.connect(("127.0.0.1", self.get_http_port()))

        self.assertIs(connect_result, stream)

        yield stream.write(b"GET / HTTP/1.0\r\n\r\n")

        first_line = yield stream.read_until(b"\r\n")

        self.assertEqual(first_line, b"HTTP/1.1 200 OK\r\n")

        # callback=None is equivalent to no callback.

        header_data = yield stream.read_until(b"\r\n\r\n")

        headers = HTTPHeaders.parse(header_data.decode("latin1"))

        content_length = int(headers["Content-Length"])

        body = yield stream.read_bytes(content_length)

        self.assertEqual(body, b"Hello")

        stream.close()



    @gen_test

    def test_future_close_while_reading(self):

        stream = self._make_client_iostream()

        yield stream.connect(("127.0.0.1", self.get_http_port()))

        yield stream.write(b"GET / HTTP/1.0\r\n\r\n")

        with self.assertRaises(StreamClosedError):

            yield stream.read_bytes(1024 * 1024)

        stream.close()



    @gen_test

    def test_future_read_until_close(self):

        # Ensure that the data comes through before the StreamClosedError.

        stream = self._make_client_iostream()

        yield stream.connect(("127.0.0.1", self.get_http_port()))

        yield stream.write(b"GET / HTTP/1.0\r\nConnection: close\r\n\r\n")

        yield stream.read_until(b"\r\n\r\n")

        body = yield stream.read_until_close()

        self.assertEqual(body, b"Hello")



        # Nothing else to read; the error comes immediately without waiting

        # for yield.

        with self.assertRaises(StreamClosedError):

            stream.read_bytes(1)





class TestReadWriteMixin(object):

    # Tests where one stream reads and the other writes.

    # These should work for BaseIOStream implementations.



    def make_iostream_pair(self, **kwargs):

        raise NotImplementedError



    @gen_test

    def test_write_zero_bytes(self):

        # Attempting to write zero bytes should run the callback without

        # going into an infinite loop.

        rs, ws = yield self.make_iostream_pair()

        yield ws.write(b"")

        ws.close()

        rs.close()



    @gen_test

    def test_future_delayed_close_callback(self):

        # Same as test_delayed_close_callback, but with the future interface.

        rs, ws = yield self.make_iostream_pair()



        try:

            ws.write(b"12")

            chunks = []

            chunks.append((yield rs.read_bytes(1)))

            ws.close()

            chunks.append((yield rs.read_bytes(1)))

            self.assertEqual(chunks, [b"1", b"2"])

        finally:

            ws.close()

            rs.close()



    @gen_test

    def test_close_buffered_data(self):

        # Similar to the previous test, but with data stored in the OS's

        # socket buffers instead of the IOStream's read buffer.  Out-of-band

        # close notifications must be delayed until all data has been

        # drained into the IOStream buffer. (epoll used to use out-of-band

        # close events with EPOLLRDHUP, but no longer)

        #

        # This depends on the read_chunk_size being smaller than the

        # OS socket buffer, so make it small.

        rs, ws = yield self.make_iostream_pair(read_chunk_size=256)

        try:

            ws.write(b"A" * 512)

            data = yield rs.read_bytes(256)

            self.assertEqual(b"A" * 256, data)

            ws.close()

            # Allow the close to propagate to the `rs` side of the

            # connection.  Using add_callback instead of add_timeout

            # doesn't seem to work, even with multiple iterations

            yield gen.sleep(0.01)

            data = yield rs.read_bytes(256)

            self.assertEqual(b"A" * 256, data)

        finally:

            ws.close()

            rs.close()



    @gen_test

    def test_read_until_close_after_close(self):

        # Similar to test_delayed_close_callback, but read_until_close takes

        # a separate code path so test it separately.

        rs, ws = yield self.make_iostream_pair()

        try:

            ws.write(b"1234")

            ws.close()

            # Read one byte to make sure the client has received the data.

            # It won't run the close callback as long as there is more buffered

            # data that could satisfy a later read.

            data = yield rs.read_bytes(1)

            self.assertEqual(data, b"1")

            data = yield rs.read_until_close()

            self.assertEqual(data, b"234")

        finally:

            ws.close()

            rs.close()



    @gen_test

    def test_large_read_until(self):

        # Performance test: read_until used to have a quadratic component

        # so a read_until of 4MB would take 8 seconds; now it takes 0.25

        # seconds.

        rs, ws = yield self.make_iostream_pair()

        try:

            # This test fails on pypy with ssl.  I think it's because

            # pypy's gc defeats moves objects, breaking the

            # "frozen write buffer" assumption.

            if (

                isinstance(rs, SSLIOStream)

                and platform.python_implementation() == "PyPy"

            ):

                raise unittest.SkipTest("pypy gc causes problems with openssl")

            NUM_KB = 4096

            for i in range(NUM_KB):

                ws.write(b"A" * 1024)

            ws.write(b"\r\n")

            data = yield rs.read_until(b"\r\n")

            self.assertEqual(len(data), NUM_KB * 1024 + 2)

        finally:

            ws.close()

            rs.close()



    @gen_test

    def test_close_callback_with_pending_read(self):

        # Regression test for a bug that was introduced in 2.3

        # where the IOStream._close_callback would never be called

        # if there were pending reads.

        OK = b"OK\r\n"

        rs, ws = yield self.make_iostream_pair()

        event = Event()

        rs.set_close_callback(event.set)

        try:

            ws.write(OK)

            res = yield rs.read_until(b"\r\n")

            self.assertEqual(res, OK)



            ws.close()

            rs.read_until(b"\r\n")

            # If _close_callback (self.stop) is not called,

            # an AssertionError: Async operation timed out after 5 seconds

            # will be raised.

            yield event.wait()

        finally:

            ws.close()

            rs.close()



    @gen_test

    def test_future_close_callback(self):

        # Regression test for interaction between the Future read interfaces

        # and IOStream._maybe_add_error_listener.

        rs, ws = yield self.make_iostream_pair()

        closed = [False]

        cond = Condition()



        def close_callback():

            closed[0] = True

            cond.notify()



        rs.set_close_callback(close_callback)

        try:

            ws.write(b"a")

            res = yield rs.read_bytes(1)

            self.assertEqual(res, b"a")

            self.assertFalse(closed[0])

            ws.close()

            yield cond.wait()

            self.assertTrue(closed[0])

        finally:

            rs.close()

            ws.close()



    @gen_test

    def test_write_memoryview(self):

        rs, ws = yield self.make_iostream_pair()

        try:

            fut = rs.read_bytes(4)

            ws.write(memoryview(b"hello"))

            data = yield fut

            self.assertEqual(data, b"hell")

        finally:

            ws.close()

            rs.close()



    @gen_test

    def test_read_bytes_partial(self):

        rs, ws = yield self.make_iostream_pair()

        try:

            # Ask for more than is available with partial=True

            fut = rs.read_bytes(50, partial=True)

            ws.write(b"hello")

            data = yield fut

            self.assertEqual(data, b"hello")



            # Ask for less than what is available; num_bytes is still

            # respected.

            fut = rs.read_bytes(3, partial=True)

            ws.write(b"world")

            data = yield fut

            self.assertEqual(data, b"wor")



            # Partial reads won't return an empty string, but read_bytes(0)

            # will.

            data = yield rs.read_bytes(0, partial=True)

            self.assertEqual(data, b"")

        finally:

            ws.close()

            rs.close()



    @gen_test

    def test_read_until_max_bytes(self):

        rs, ws = yield self.make_iostream_pair()

        closed = Event()

        rs.set_close_callback(closed.set)

        try:

            # Extra room under the limit

            fut = rs.read_until(b"def", max_bytes=50)

            ws.write(b"abcdef")

            data = yield fut

            self.assertEqual(data, b"abcdef")



            # Just enough space

            fut = rs.read_until(b"def", max_bytes=6)

            ws.write(b"abcdef")

            data = yield fut

            self.assertEqual(data, b"abcdef")



            # Not enough space, but we don't know it until all we can do is

            # log a warning and close the connection.

            with ExpectLog(gen_log, "Unsatisfiable read"):

                fut = rs.read_until(b"def", max_bytes=5)

                ws.write(b"123456")

                yield closed.wait()

        finally:

            ws.close()

            rs.close()



    @gen_test

    def test_read_until_max_bytes_inline(self):

        rs, ws = yield self.make_iostream_pair()

        closed = Event()

        rs.set_close_callback(closed.set)

        try:

            # Similar to the error case in the previous test, but the

            # ws writes first so rs reads are satisfied

            # inline.  For consistency with the out-of-line case, we

            # do not raise the error synchronously.

            ws.write(b"123456")

            with ExpectLog(gen_log, "Unsatisfiable read"):

                with self.assertRaises(StreamClosedError):

                    yield rs.read_until(b"def", max_bytes=5)

            yield closed.wait()

        finally:

            ws.close()

            rs.close()



    @gen_test

    def test_read_until_max_bytes_ignores_extra(self):

        rs, ws = yield self.make_iostream_pair()

        closed = Event()

        rs.set_close_callback(closed.set)

        try:

            # Even though data that matches arrives the same packet that

            # puts us over the limit, we fail the request because it was not

            # found within the limit.

            ws.write(b"abcdef")

            with ExpectLog(gen_log, "Unsatisfiable read"):

                rs.read_until(b"def", max_bytes=5)

                yield closed.wait()

        finally:

            ws.close()

            rs.close()



    @gen_test

    def test_read_until_regex_max_bytes(self):

        rs, ws = yield self.make_iostream_pair()

        closed = Event()

        rs.set_close_callback(closed.set)

        try:

            # Extra room under the limit

            fut = rs.read_until_regex(b"def", max_bytes=50)

            ws.write(b"abcdef")

            data = yield fut

            self.assertEqual(data, b"abcdef")



            # Just enough space

            fut = rs.read_until_regex(b"def", max_bytes=6)

            ws.write(b"abcdef")

            data = yield fut

            self.assertEqual(data, b"abcdef")



            # Not enough space, but we don't know it until all we can do is

            # log a warning and close the connection.

            with ExpectLog(gen_log, "Unsatisfiable read"):

                rs.read_until_regex(b"def", max_bytes=5)

                ws.write(b"123456")

                yield closed.wait()

        finally:

            ws.close()

            rs.close()



    @gen_test

    def test_read_until_regex_max_bytes_inline(self):

        rs, ws = yield self.make_iostream_pair()

        closed = Event()

        rs.set_close_callback(closed.set)

        try:

            # Similar to the error case in the previous test, but the

            # ws writes first so rs reads are satisfied

            # inline.  For consistency with the out-of-line case, we

            # do not raise the error synchronously.

            ws.write(b"123456")

            with ExpectLog(gen_log, "Unsatisfiable read"):

                rs.read_until_regex(b"def", max_bytes=5)

                yield closed.wait()

        finally:

            ws.close()

            rs.close()



    @gen_test

    def test_read_until_regex_max_bytes_ignores_extra(self):

        rs, ws = yield self.make_iostream_pair()

        closed = Event()

        rs.set_close_callback(closed.set)

        try:

            # Even though data that matches arrives the same packet that

            # puts us over the limit, we fail the request because it was not

            # found within the limit.

            ws.write(b"abcdef")

            with ExpectLog(gen_log, "Unsatisfiable read"):

                rs.read_until_regex(b"def", max_bytes=5)

                yield closed.wait()

        finally:

            ws.close()

            rs.close()



    @gen_test

    def test_small_reads_from_large_buffer(self):

        # 10KB buffer size, 100KB available to read.

        # Read 1KB at a time and make sure that the buffer is not eagerly

        # filled.

        rs, ws = yield self.make_iostream_pair(max_buffer_size=10 * 1024)

        try:

            ws.write(b"a" * 1024 * 100)

            for i in range(100):

                data = yield rs.read_bytes(1024)

                self.assertEqual(data, b"a" * 1024)

        finally:

            ws.close()

            rs.close()



    @gen_test

    def test_small_read_untils_from_large_buffer(self):

        # 10KB buffer size, 100KB available to read.

        # Read 1KB at a time and make sure that the buffer is not eagerly

        # filled.

        rs, ws = yield self.make_iostream_pair(max_buffer_size=10 * 1024)

        try:

            ws.write((b"a" * 1023 + b"\n") * 100)

            for i in range(100):

                data = yield rs.read_until(b"\n", max_bytes=4096)

                self.assertEqual(data, b"a" * 1023 + b"\n")

        finally:

            ws.close()

            rs.close()



    @gen_test

    def test_flow_control(self):

        MB = 1024 * 1024

        rs, ws = yield self.make_iostream_pair(max_buffer_size=5 * MB)

        try:

            # Client writes more than the rs will accept.

            ws.write(b"a" * 10 * MB)

            # The rs pauses while reading.

            yield rs.read_bytes(MB)

            yield gen.sleep(0.1)

            # The ws's writes have been blocked; the rs can

            # continue to read gradually.

            for i in range(9):

                yield rs.read_bytes(MB)

        finally:

            rs.close()

            ws.close()



    @gen_test

    def test_read_into(self):

        rs, ws = yield self.make_iostream_pair()



        def sleep_some():

            self.io_loop.run_sync(lambda: gen.sleep(0.05))



        try:

            buf = bytearray(10)

            fut = rs.read_into(buf)

            ws.write(b"hello")

            yield gen.sleep(0.05)

            self.assertTrue(rs.reading())

            ws.write(b"world!!")

            data = yield fut

            self.assertFalse(rs.reading())

            self.assertEqual(data, 10)

            self.assertEqual(bytes(buf), b"helloworld")



            # Existing buffer is fed into user buffer

            fut = rs.read_into(buf)

            yield gen.sleep(0.05)

            self.assertTrue(rs.reading())

            ws.write(b"1234567890")

            data = yield fut

            self.assertFalse(rs.reading())

            self.assertEqual(data, 10)

            self.assertEqual(bytes(buf), b"!!12345678")



            # Existing buffer can satisfy read immediately

            buf = bytearray(4)

            ws.write(b"abcdefghi")

            data = yield rs.read_into(buf)

            self.assertEqual(data, 4)

            self.assertEqual(bytes(buf), b"90ab")



            data = yield rs.read_bytes(7)

            self.assertEqual(data, b"cdefghi")

        finally:

            ws.close()

            rs.close()



    @gen_test

    def test_read_into_partial(self):

        rs, ws = yield self.make_iostream_pair()



        try:

            # Partial read

            buf = bytearray(10)

            fut = rs.read_into(buf, partial=True)

            ws.write(b"hello")

            data = yield fut

            self.assertFalse(rs.reading())

            self.assertEqual(data, 5)

            self.assertEqual(bytes(buf), b"hello\0\0\0\0\0")



            # Full read despite partial=True

            ws.write(b"world!1234567890")

            data = yield rs.read_into(buf, partial=True)

            self.assertEqual(data, 10)

            self.assertEqual(bytes(buf), b"world!1234")



            # Existing buffer can satisfy read immediately

            data = yield rs.read_into(buf, partial=True)

            self.assertEqual(data, 6)

            self.assertEqual(bytes(buf), b"5678901234")



        finally:

            ws.close()

            rs.close()



    @gen_test

    def test_read_into_zero_bytes(self):

        rs, ws = yield self.make_iostream_pair()

        try:

            buf = bytearray()

            fut = rs.read_into(buf)

            self.assertEqual(fut.result(), 0)

        finally:

            ws.close()

            rs.close()



    @gen_test

    def test_many_mixed_reads(self):

        # Stress buffer handling when going back and forth between

        # read_bytes() (using an internal buffer) and read_into()

        # (using a user-allocated buffer).

        r = random.Random(42)

        nbytes = 1000000

        rs, ws = yield self.make_iostream_pair()



        produce_hash = hashlib.sha1()

        consume_hash = hashlib.sha1()



        @gen.coroutine

        def produce():

            remaining = nbytes

            while remaining > 0:

                size = r.randint(1, min(1000, remaining))

                data = os.urandom(size)

                produce_hash.update(data)

                yield ws.write(data)

                remaining -= size

            assert remaining == 0



        @gen.coroutine

        def consume():

            remaining = nbytes

            while remaining > 0:

                if r.random() > 0.5:

                    # read_bytes()

                    size = r.randint(1, min(1000, remaining))

                    data = yield rs.read_bytes(size)

                    consume_hash.update(data)

                    remaining -= size

                else:

                    # read_into()

                    size = r.randint(1, min(1000, remaining))

                    buf = bytearray(size)

                    n = yield rs.read_into(buf)

                    assert n == size

                    consume_hash.update(buf)

                    remaining -= size

            assert remaining == 0



        try:

            yield [produce(), consume()]

            assert produce_hash.hexdigest() == consume_hash.hexdigest()

        finally:

            ws.close()

            rs.close()





class TestIOStreamMixin(TestReadWriteMixin):

    def _make_server_iostream(self, connection, **kwargs):

        raise NotImplementedError()



    def _make_client_iostream(self, connection, **kwargs):

        raise NotImplementedError()



    @gen.coroutine

    def make_iostream_pair(self, **kwargs):

        listener, port = bind_unused_port()

        server_stream_fut = Future()  # type: Future[IOStream]



        def accept_callback(connection, address):

            server_stream_fut.set_result(

                self._make_server_iostream(connection, **kwargs)

            )



        netutil.add_accept_handler(listener, accept_callback)

        client_stream = self._make_client_iostream(socket.socket(), **kwargs)

        connect_fut = client_stream.connect(("127.0.0.1", port))

        server_stream, client_stream = yield [server_stream_fut, connect_fut]

        self.io_loop.remove_handler(listener.fileno())

        listener.close()

        raise gen.Return((server_stream, client_stream))



    @gen_test

    def test_connection_refused(self):

        # When a connection is refused, the connect callback should not

        # be run.  (The kqueue IOLoop used to behave differently from the

        # epoll IOLoop in this respect)

        cleanup_func, port = refusing_port()

        self.addCleanup(cleanup_func)

        stream = IOStream(socket.socket())



        stream.set_close_callback(self.stop)

        # log messages vary by platform and ioloop implementation

        with ExpectLog(gen_log, ".*", required=False):

            with self.assertRaises(StreamClosedError):

                yield stream.connect(("127.0.0.1", port))



        self.assertTrue(isinstance(stream.error, socket.error), stream.error)

        if sys.platform != "cygwin":

            _ERRNO_CONNREFUSED = [errno.ECONNREFUSED]

            if hasattr(errno, "WSAECONNREFUSED"):

                _ERRNO_CONNREFUSED.append(errno.WSAECONNREFUSED)  # type: ignore

            # cygwin's errnos don't match those used on native windows python

            self.assertTrue(stream.error.args[0] in _ERRNO_CONNREFUSED)  # type: ignore



    @gen_test

    def test_gaierror(self):

        # Test that IOStream sets its exc_info on getaddrinfo error.

        # It's difficult to reliably trigger a getaddrinfo error;

        # some resolvers own't even return errors for malformed names,

        # so we mock it instead. If IOStream changes to call a Resolver

        # before sock.connect, the mock target will need to change too.

        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM, 0)

        stream = IOStream(s)

        stream.set_close_callback(self.stop)

        with mock.patch(

            "socket.socket.connect", side_effect=socket.gaierror(errno.EIO, "boom")

        ):

            with self.assertRaises(StreamClosedError):

                yield stream.connect(("localhost", 80))

            self.assertTrue(isinstance(stream.error, socket.gaierror))



    @gen_test

    def test_read_until_close_with_error(self):

        server, client = yield self.make_iostream_pair()

        try:

            with mock.patch(

                "tornado.iostream.BaseIOStream._try_inline_read",

                side_effect=IOError("boom"),

            ):

                with self.assertRaisesRegexp(IOError, "boom"):

                    client.read_until_close()

        finally:

            server.close()

            client.close()



    @skipIfNonUnix

    @skipPypy3V58

    @gen_test

    def test_inline_read_error(self):

        # An error on an inline read is raised without logging (on the

        # assumption that it will eventually be noticed or logged further

        # up the stack).

        #

        # This test is posix-only because windows os.close() doesn't work

        # on socket FDs, but we can't close the socket object normally

        # because we won't get the error we want if the socket knows

        # it's closed.

        server, client = yield self.make_iostream_pair()

        try:

            os.close(server.socket.fileno())

            with self.assertRaises(socket.error):

                server.read_bytes(1)

        finally:

            server.close()

            client.close()



    @skipPypy3V58

    @gen_test

    def test_async_read_error_logging(self):

        # Socket errors on asynchronous reads should be logged (but only

        # once).

        server, client = yield self.make_iostream_pair()

        closed = Event()

        server.set_close_callback(closed.set)

        try:

            # Start a read that will be fulfilled asynchronously.

            server.read_bytes(1)

            client.write(b"a")

            # Stub out read_from_fd to make it fail.



            def fake_read_from_fd():

                os.close(server.socket.fileno())

                server.__class__.read_from_fd(server)



            server.read_from_fd = fake_read_from_fd

            # This log message is from _handle_read (not read_from_fd).

            with ExpectLog(gen_log, "error on read"):

                yield closed.wait()

        finally:

            server.close()

            client.close()



    @gen_test

    def test_future_write(self):

        """

        Test that write() Futures are never orphaned.

        """

        # Run concurrent writers that will write enough bytes so as to

        # clog the socket buffer and accumulate bytes in our write buffer.

        m, n = 5000, 1000

        nproducers = 10

        total_bytes = m * n * nproducers

        server, client = yield self.make_iostream_pair(max_buffer_size=total_bytes)



        @gen.coroutine

        def produce():

            data = b"x" * m

            for i in range(n):

                yield server.write(data)



        @gen.coroutine

        def consume():

            nread = 0

            while nread < total_bytes:

                res = yield client.read_bytes(m)

                nread += len(res)



        try:

            yield [produce() for i in range(nproducers)] + [consume()]

        finally:

            server.close()

            client.close()





class TestIOStreamWebHTTP(TestIOStreamWebMixin, AsyncHTTPTestCase):

    def _make_client_iostream(self):

        return IOStream(socket.socket())





class TestIOStreamWebHTTPS(TestIOStreamWebMixin, AsyncHTTPSTestCase):

    def _make_client_iostream(self):

        return SSLIOStream(socket.socket(), ssl_options=dict(cert_reqs=ssl.CERT_NONE))





class TestIOStream(TestIOStreamMixin, AsyncTestCase):

    def _make_server_iostream(self, connection, **kwargs):

        return IOStream(connection, **kwargs)



    def _make_client_iostream(self, connection, **kwargs):

        return IOStream(connection, **kwargs)





class TestIOStreamSSL(TestIOStreamMixin, AsyncTestCase):

    def _make_server_iostream(self, connection, **kwargs):

        connection = ssl.wrap_socket(

            connection,

            server_side=True,

            do_handshake_on_connect=False,

            **_server_ssl_options()

        )

        return SSLIOStream(connection, **kwargs)



    def _make_client_iostream(self, connection, **kwargs):

        return SSLIOStream(

            connection, ssl_options=dict(cert_reqs=ssl.CERT_NONE), **kwargs

        )





# This will run some tests that are basically redundant but it's the

# simplest way to make sure that it works to pass an SSLContext

# instead of an ssl_options dict to the SSLIOStream constructor.

class TestIOStreamSSLContext(TestIOStreamMixin, AsyncTestCase):

    def _make_server_iostream(self, connection, **kwargs):

        context = ssl.SSLContext(ssl.PROTOCOL_SSLv23)

        context.load_cert_chain(

            os.path.join(os.path.dirname(__file__), "test.crt"),

            os.path.join(os.path.dirname(__file__), "test.key"),

        )

        connection = ssl_wrap_socket(

            connection, context, server_side=True, do_handshake_on_connect=False

        )

        return SSLIOStream(connection, **kwargs)



    def _make_client_iostream(self, connection, **kwargs):

        context = ssl.SSLContext(ssl.PROTOCOL_SSLv23)

        return SSLIOStream(connection, ssl_options=context, **kwargs)





class TestIOStreamStartTLS(AsyncTestCase):

    def setUp(self):

        try:

            super(TestIOStreamStartTLS, self).setUp()

            self.listener, self.port = bind_unused_port()

            self.server_stream = None

            self.server_accepted = Future()  # type: Future[None]

            netutil.add_accept_handler(self.listener, self.accept)

            self.client_stream = IOStream(socket.socket())

            self.io_loop.add_future(

                self.client_stream.connect(("127.0.0.1", self.port)), self.stop

            )

            self.wait()

            self.io_loop.add_future(self.server_accepted, self.stop)

            self.wait()

        except Exception as e:

            print(e)

            raise



    def tearDown(self):

        if self.server_stream is not None:

            self.server_stream.close()

        if self.client_stream is not None:

            self.client_stream.close()

        self.listener.close()

        super(TestIOStreamStartTLS, self).tearDown()



    def accept(self, connection, address):

        if self.server_stream is not None:

            self.fail("should only get one connection")

        self.server_stream = IOStream(connection)

        self.server_accepted.set_result(None)



    @gen.coroutine

    def client_send_line(self, line):

        self.client_stream.write(line)

        recv_line = yield self.server_stream.read_until(b"\r\n")

        self.assertEqual(line, recv_line)



    @gen.coroutine

    def server_send_line(self, line):

        self.server_stream.write(line)

        recv_line = yield self.client_stream.read_until(b"\r\n")

        self.assertEqual(line, recv_line)



    def client_start_tls(self, ssl_options=None, server_hostname=None):

        client_stream = self.client_stream

        self.client_stream = None

        return client_stream.start_tls(False, ssl_options, server_hostname)



    def server_start_tls(self, ssl_options=None):

        server_stream = self.server_stream

        self.server_stream = None

        return server_stream.start_tls(True, ssl_options)



    @gen_test

    def test_start_tls_smtp(self):

        # This flow is simplified from RFC 3207 section 5.

        # We don't really need all of this, but it helps to make sure

        # that after realistic back-and-forth traffic the buffers end up

        # in a sane state.

        yield self.server_send_line(b"220 mail.example.com ready\r\n")

        yield self.client_send_line(b"EHLO mail.example.com\r\n")

        yield self.server_send_line(b"250-mail.example.com welcome\r\n")

        yield self.server_send_line(b"250 STARTTLS\r\n")

        yield self.client_send_line(b"STARTTLS\r\n")

        yield self.server_send_line(b"220 Go ahead\r\n")

        client_future = self.client_start_tls(dict(cert_reqs=ssl.CERT_NONE))

        server_future = self.server_start_tls(_server_ssl_options())

        self.client_stream = yield client_future

        self.server_stream = yield server_future

        self.assertTrue(isinstance(self.client_stream, SSLIOStream))

        self.assertTrue(isinstance(self.server_stream, SSLIOStream))

        yield self.client_send_line(b"EHLO mail.example.com\r\n")

        yield self.server_send_line(b"250 mail.example.com welcome\r\n")



    @gen_test

    def test_handshake_fail(self):

        server_future = self.server_start_tls(_server_ssl_options())

        # Certificates are verified with the default configuration.

        with ExpectLog(gen_log, "SSL Error"):

            client_future = self.client_start_tls(server_hostname="localhost")

            with self.assertRaises(ssl.SSLError):

                yield client_future

            with self.assertRaises((ssl.SSLError, socket.error)):

                yield server_future



    @gen_test

    def test_check_hostname(self):

        # Test that server_hostname parameter to start_tls is being used.

        # The check_hostname functionality is only available in python 2.7 and

        # up and in python 3.4 and up.

        server_future = self.server_start_tls(_server_ssl_options())

        with ExpectLog(gen_log, "SSL Error"):

            client_future = self.client_start_tls(

                ssl.create_default_context(), server_hostname="127.0.0.1"

            )

            with self.assertRaises(ssl.SSLError):

                # The client fails to connect with an SSL error.

                yield client_future

            with self.assertRaises(Exception):

                # The server fails to connect, but the exact error is unspecified.

                yield server_future





class WaitForHandshakeTest(AsyncTestCase):

    @gen.coroutine

    def connect_to_server(self, server_cls):

        server = client = None

        try:

            sock, port = bind_unused_port()

            server = server_cls(ssl_options=_server_ssl_options())

            server.add_socket(sock)



            client = SSLIOStream(

                socket.socket(), ssl_options=dict(cert_reqs=ssl.CERT_NONE)

            )

            yield client.connect(("127.0.0.1", port))

            self.assertIsNotNone(client.socket.cipher())

        finally:

            if server is not None:

                server.stop()

            if client is not None:

                client.close()



    @gen_test

    def test_wait_for_handshake_future(self):

        test = self

        handshake_future = Future()  # type: Future[None]



        class TestServer(TCPServer):

            def handle_stream(self, stream, address):

                test.assertIsNone(stream.socket.cipher())

                test.io_loop.spawn_callback(self.handle_connection, stream)



            @gen.coroutine

            def handle_connection(self, stream):

                yield stream.wait_for_handshake()

                handshake_future.set_result(None)



        yield self.connect_to_server(TestServer)

        yield handshake_future



    @gen_test

    def test_wait_for_handshake_already_waiting_error(self):

        test = self

        handshake_future = Future()  # type: Future[None]



        class TestServer(TCPServer):

            @gen.coroutine

            def handle_stream(self, stream, address):

                fut = stream.wait_for_handshake()

                test.assertRaises(RuntimeError, stream.wait_for_handshake)

                yield fut



                handshake_future.set_result(None)



        yield self.connect_to_server(TestServer)

        yield handshake_future



    @gen_test

    def test_wait_for_handshake_already_connected(self):

        handshake_future = Future()  # type: Future[None]



        class TestServer(TCPServer):

            @gen.coroutine

            def handle_stream(self, stream, address):

                yield stream.wait_for_handshake()

                yield stream.wait_for_handshake()

                handshake_future.set_result(None)



        yield self.connect_to_server(TestServer)

        yield handshake_future





@skipIfNonUnix

class TestPipeIOStream(TestReadWriteMixin, AsyncTestCase):

    @gen.coroutine

    def make_iostream_pair(self, **kwargs):

        r, w = os.pipe()



        return PipeIOStream(r, **kwargs), PipeIOStream(w, **kwargs)



    @gen_test

    def test_pipe_iostream(self):

        rs, ws = yield self.make_iostream_pair()



        ws.write(b"hel")

        ws.write(b"lo world")



        data = yield rs.read_until(b" ")

        self.assertEqual(data, b"hello ")



        data = yield rs.read_bytes(3)

        self.assertEqual(data, b"wor")



        ws.close()



        data = yield rs.read_until_close()

        self.assertEqual(data, b"ld")



        rs.close()



    @gen_test

    def test_pipe_iostream_big_write(self):

        rs, ws = yield self.make_iostream_pair()



        NUM_BYTES = 1048576



        # Write 1MB of data, which should fill the buffer

        ws.write(b"1" * NUM_BYTES)



        data = yield rs.read_bytes(NUM_BYTES)

        self.assertEqual(data, b"1" * NUM_BYTES)



        ws.close()

        rs.close()





class TestStreamBuffer(unittest.TestCase):

    """

    Unit tests for the private _StreamBuffer class.

    """



    def setUp(self):

        self.random = random.Random(42)



    def to_bytes(self, b):

        if isinstance(b, (bytes, bytearray)):

            return bytes(b)

        elif isinstance(b, memoryview):

            return b.tobytes()  # For py2

        else:

            raise TypeError(b)



    def make_streambuffer(self, large_buf_threshold=10):

        buf = _StreamBuffer()

        assert buf._large_buf_threshold

        buf._large_buf_threshold = large_buf_threshold

        return buf



    def check_peek(self, buf, expected):

        size = 1

        while size < 2 * len(expected):

            got = self.to_bytes(buf.peek(size))

            self.assertTrue(got)  # Not empty

            self.assertLessEqual(len(got), size)

            self.assertTrue(expected.startswith(got), (expected, got))

            size = (size * 3 + 1) // 2



    def check_append_all_then_skip_all(self, buf, objs, input_type):

        self.assertEqual(len(buf), 0)



        expected = b""



        for o in objs:

            expected += o

            buf.append(input_type(o))

            self.assertEqual(len(buf), len(expected))

            self.check_peek(buf, expected)



        while expected:

            n = self.random.randrange(1, len(expected) + 1)

            expected = expected[n:]

            buf.advance(n)

            self.assertEqual(len(buf), len(expected))

            self.check_peek(buf, expected)



        self.assertEqual(len(buf), 0)



    def test_small(self):

        objs = [b"12", b"345", b"67", b"89a", b"bcde", b"fgh", b"ijklmn"]



        buf = self.make_streambuffer()

        self.check_append_all_then_skip_all(buf, objs, bytes)



        buf = self.make_streambuffer()

        self.check_append_all_then_skip_all(buf, objs, bytearray)



        buf = self.make_streambuffer()

        self.check_append_all_then_skip_all(buf, objs, memoryview)



        # Test internal algorithm

        buf = self.make_streambuffer(10)

        for i in range(9):

            buf.append(b"x")

        self.assertEqual(len(buf._buffers), 1)

        for i in range(9):

            buf.append(b"x")

        self.assertEqual(len(buf._buffers), 2)

        buf.advance(10)

        self.assertEqual(len(buf._buffers), 1)

        buf.advance(8)

        self.assertEqual(len(buf._buffers), 0)

        self.assertEqual(len(buf), 0)



    def test_large(self):

        objs = [

            b"12" * 5,

            b"345" * 2,

            b"67" * 20,

            b"89a" * 12,

            b"bcde" * 1,

            b"fgh" * 7,

            b"ijklmn" * 2,

        ]



        buf = self.make_streambuffer()

        self.check_append_all_then_skip_all(buf, objs, bytes)



        buf = self.make_streambuffer()

        self.check_append_all_then_skip_all(buf, objs, bytearray)



        buf = self.make_streambuffer()

        self.check_append_all_then_skip_all(buf, objs, memoryview)



        # Test internal algorithm

        buf = self.make_streambuffer(10)

        for i in range(3):

            buf.append(b"x" * 11)

        self.assertEqual(len(buf._buffers), 3)

        buf.append(b"y")

        self.assertEqual(len(buf._buffers), 4)

        buf.append(b"z")

        self.assertEqual(len(buf._buffers), 4)

        buf.advance(33)

        self.assertEqual(len(buf._buffers), 1)

        buf.advance(2)

        self.assertEqual(len(buf._buffers), 0)

        self.assertEqual(len(buf), 0)

import datetime

import os

import shutil

import tempfile

import unittest



import tornado.locale

from tornado.escape import utf8, to_unicode

from tornado.util import unicode_type





class TranslationLoaderTest(unittest.TestCase):

    # TODO: less hacky way to get isolated tests

    SAVE_VARS = ["_translations", "_supported_locales", "_use_gettext"]



    def clear_locale_cache(self):

        tornado.locale.Locale._cache = {}



    def setUp(self):

        self.saved = {}  # type: dict

        for var in TranslationLoaderTest.SAVE_VARS:

            self.saved[var] = getattr(tornado.locale, var)

        self.clear_locale_cache()



    def tearDown(self):

        for k, v in self.saved.items():

            setattr(tornado.locale, k, v)

        self.clear_locale_cache()



    def test_csv(self):

        tornado.locale.load_translations(

            os.path.join(os.path.dirname(__file__), "csv_translations")

        )

        locale = tornado.locale.get("fr_FR")

        self.assertTrue(isinstance(locale, tornado.locale.CSVLocale))

        self.assertEqual(locale.translate("school"), u"\u00e9cole")



    def test_csv_bom(self):

        with open(

            os.path.join(os.path.dirname(__file__), "csv_translations", "fr_FR.csv"),

            "rb",

        ) as f:

            char_data = to_unicode(f.read())

        # Re-encode our input data (which is utf-8 without BOM) in

        # encodings that use the BOM and ensure that we can still load

        # it. Note that utf-16-le and utf-16-be do not write a BOM,

        # so we only test whichver variant is native to our platform.

        for encoding in ["utf-8-sig", "utf-16"]:

            tmpdir = tempfile.mkdtemp()

            try:

                with open(os.path.join(tmpdir, "fr_FR.csv"), "wb") as f:

                    f.write(char_data.encode(encoding))

                tornado.locale.load_translations(tmpdir)

                locale = tornado.locale.get("fr_FR")

                self.assertIsInstance(locale, tornado.locale.CSVLocale)

                self.assertEqual(locale.translate("school"), u"\u00e9cole")

            finally:

                shutil.rmtree(tmpdir)



    def test_gettext(self):

        tornado.locale.load_gettext_translations(

            os.path.join(os.path.dirname(__file__), "gettext_translations"),

            "tornado_test",

        )

        locale = tornado.locale.get("fr_FR")

        self.assertTrue(isinstance(locale, tornado.locale.GettextLocale))

        self.assertEqual(locale.translate("school"), u"\u00e9cole")

        self.assertEqual(locale.pgettext("law", "right"), u"le droit")

        self.assertEqual(locale.pgettext("good", "right"), u"le bien")

        self.assertEqual(

            locale.pgettext("organization", "club", "clubs", 1), u"le club"

        )

        self.assertEqual(

            locale.pgettext("organization", "club", "clubs", 2), u"les clubs"

        )

        self.assertEqual(locale.pgettext("stick", "club", "clubs", 1), u"le b\xe2ton")

        self.assertEqual(locale.pgettext("stick", "club", "clubs", 2), u"les b\xe2tons")





class LocaleDataTest(unittest.TestCase):

    def test_non_ascii_name(self):

        name = tornado.locale.LOCALE_NAMES["es_LA"]["name"]

        self.assertTrue(isinstance(name, unicode_type))

        self.assertEqual(name, u"Espa\u00f1ol")

        self.assertEqual(utf8(name), b"Espa\xc3\xb1ol")





class EnglishTest(unittest.TestCase):

    def test_format_date(self):

        locale = tornado.locale.get("en_US")

        date = datetime.datetime(2013, 4, 28, 18, 35)

        self.assertEqual(

            locale.format_date(date, full_format=True), "April 28, 2013 at 6:35 pm"

        )



        now = datetime.datetime.utcnow()



        self.assertEqual(

            locale.format_date(now - datetime.timedelta(seconds=2), full_format=False),

            "2 seconds ago",

        )

        self.assertEqual(

            locale.format_date(now - datetime.timedelta(minutes=2), full_format=False),

            "2 minutes ago",

        )

        self.assertEqual(

            locale.format_date(now - datetime.timedelta(hours=2), full_format=False),

            "2 hours ago",

        )



        self.assertEqual(

            locale.format_date(

                now - datetime.timedelta(days=1), full_format=False, shorter=True

            ),

            "yesterday",

        )



        date = now - datetime.timedelta(days=2)

        self.assertEqual(

            locale.format_date(date, full_format=False, shorter=True),

            locale._weekdays[date.weekday()],

        )



        date = now - datetime.timedelta(days=300)

        self.assertEqual(

            locale.format_date(date, full_format=False, shorter=True),

            "%s %d" % (locale._months[date.month - 1], date.day),

        )



        date = now - datetime.timedelta(days=500)

        self.assertEqual(

            locale.format_date(date, full_format=False, shorter=True),

            "%s %d, %d" % (locale._months[date.month - 1], date.day, date.year),

        )



    def test_friendly_number(self):

        locale = tornado.locale.get("en_US")

        self.assertEqual(locale.friendly_number(1000000), "1,000,000")



    def test_list(self):

        locale = tornado.locale.get("en_US")

        self.assertEqual(locale.list([]), "")

        self.assertEqual(locale.list(["A"]), "A")

        self.assertEqual(locale.list(["A", "B"]), "A and B")

        self.assertEqual(locale.list(["A", "B", "C"]), "A, B and C")



    def test_format_day(self):

        locale = tornado.locale.get("en_US")

        date = datetime.datetime(2013, 4, 28, 18, 35)

        self.assertEqual(locale.format_day(date=date, dow=True), "Sunday, April 28")

        self.assertEqual(locale.format_day(date=date, dow=False), "April 28")

# Licensed under the Apache License, Version 2.0 (the "License"); you may

# not use this file except in compliance with the License. You may obtain

# a copy of the License at

#

#     http://www.apache.org/licenses/LICENSE-2.0

#

# Unless required by applicable law or agreed to in writing, software

# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT

# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the

# License for the specific language governing permissions and limitations

# under the License.



import asyncio

from datetime import timedelta

import typing  # noqa: F401

import unittest



from tornado import gen, locks

from tornado.gen import TimeoutError

from tornado.testing import gen_test, AsyncTestCase





class ConditionTest(AsyncTestCase):

    def setUp(self):

        super(ConditionTest, self).setUp()

        self.history = []  # type: typing.List[typing.Union[int, str]]



    def record_done(self, future, key):

        """Record the resolution of a Future returned by Condition.wait."""



        def callback(_):

            if not future.result():

                # wait() resolved to False, meaning it timed out.

                self.history.append("timeout")

            else:

                self.history.append(key)



        future.add_done_callback(callback)



    def loop_briefly(self):

        """Run all queued callbacks on the IOLoop.



        In these tests, this method is used after calling notify() to

        preserve the pre-5.0 behavior in which callbacks ran

        synchronously.

        """

        self.io_loop.add_callback(self.stop)

        self.wait()



    def test_repr(self):

        c = locks.Condition()

        self.assertIn("Condition", repr(c))

        self.assertNotIn("waiters", repr(c))

        c.wait()

        self.assertIn("waiters", repr(c))



    @gen_test

    def test_notify(self):

        c = locks.Condition()

        self.io_loop.call_later(0.01, c.notify)

        yield c.wait()



    def test_notify_1(self):

        c = locks.Condition()

        self.record_done(c.wait(), "wait1")

        self.record_done(c.wait(), "wait2")

        c.notify(1)

        self.loop_briefly()

        self.history.append("notify1")

        c.notify(1)

        self.loop_briefly()

        self.history.append("notify2")

        self.assertEqual(["wait1", "notify1", "wait2", "notify2"], self.history)



    def test_notify_n(self):

        c = locks.Condition()

        for i in range(6):

            self.record_done(c.wait(), i)



        c.notify(3)

        self.loop_briefly()



        # Callbacks execute in the order they were registered.

        self.assertEqual(list(range(3)), self.history)

        c.notify(1)

        self.loop_briefly()

        self.assertEqual(list(range(4)), self.history)

        c.notify(2)

        self.loop_briefly()

        self.assertEqual(list(range(6)), self.history)



    def test_notify_all(self):

        c = locks.Condition()

        for i in range(4):

            self.record_done(c.wait(), i)



        c.notify_all()

        self.loop_briefly()

        self.history.append("notify_all")



        # Callbacks execute in the order they were registered.

        self.assertEqual(list(range(4)) + ["notify_all"], self.history)  # type: ignore



    @gen_test

    def test_wait_timeout(self):

        c = locks.Condition()

        wait = c.wait(timedelta(seconds=0.01))

        self.io_loop.call_later(0.02, c.notify)  # Too late.

        yield gen.sleep(0.03)

        self.assertFalse((yield wait))



    @gen_test

    def test_wait_timeout_preempted(self):

        c = locks.Condition()



        # This fires before the wait times out.

        self.io_loop.call_later(0.01, c.notify)

        wait = c.wait(timedelta(seconds=0.02))

        yield gen.sleep(0.03)

        yield wait  # No TimeoutError.



    @gen_test

    def test_notify_n_with_timeout(self):

        # Register callbacks 0, 1, 2, and 3. Callback 1 has a timeout.

        # Wait for that timeout to expire, then do notify(2) and make

        # sure everyone runs. Verifies that a timed-out callback does

        # not count against the 'n' argument to notify().

        c = locks.Condition()

        self.record_done(c.wait(), 0)

        self.record_done(c.wait(timedelta(seconds=0.01)), 1)

        self.record_done(c.wait(), 2)

        self.record_done(c.wait(), 3)



        # Wait for callback 1 to time out.

        yield gen.sleep(0.02)

        self.assertEqual(["timeout"], self.history)



        c.notify(2)

        yield gen.sleep(0.01)

        self.assertEqual(["timeout", 0, 2], self.history)

        self.assertEqual(["timeout", 0, 2], self.history)

        c.notify()

        yield

        self.assertEqual(["timeout", 0, 2, 3], self.history)



    @gen_test

    def test_notify_all_with_timeout(self):

        c = locks.Condition()

        self.record_done(c.wait(), 0)

        self.record_done(c.wait(timedelta(seconds=0.01)), 1)

        self.record_done(c.wait(), 2)



        # Wait for callback 1 to time out.

        yield gen.sleep(0.02)

        self.assertEqual(["timeout"], self.history)



        c.notify_all()

        yield

        self.assertEqual(["timeout", 0, 2], self.history)



    @gen_test

    def test_nested_notify(self):

        # Ensure no notifications lost, even if notify() is reentered by a

        # waiter calling notify().

        c = locks.Condition()



        # Three waiters.

        futures = [asyncio.ensure_future(c.wait()) for _ in range(3)]



        # First and second futures resolved. Second future reenters notify(),

        # resolving third future.

        futures[1].add_done_callback(lambda _: c.notify())

        c.notify(2)

        yield

        self.assertTrue(all(f.done() for f in futures))



    @gen_test

    def test_garbage_collection(self):

        # Test that timed-out waiters are occasionally cleaned from the queue.

        c = locks.Condition()

        for _ in range(101):

            c.wait(timedelta(seconds=0.01))



        future = asyncio.ensure_future(c.wait())

        self.assertEqual(102, len(c._waiters))



        # Let first 101 waiters time out, triggering a collection.

        yield gen.sleep(0.02)

        self.assertEqual(1, len(c._waiters))



        # Final waiter is still active.

        self.assertFalse(future.done())

        c.notify()

        self.assertTrue(future.done())





class EventTest(AsyncTestCase):

    def test_repr(self):

        event = locks.Event()

        self.assertTrue("clear" in str(event))

        self.assertFalse("set" in str(event))

        event.set()

        self.assertFalse("clear" in str(event))

        self.assertTrue("set" in str(event))



    def test_event(self):

        e = locks.Event()

        future_0 = asyncio.ensure_future(e.wait())

        e.set()

        future_1 = asyncio.ensure_future(e.wait())

        e.clear()

        future_2 = asyncio.ensure_future(e.wait())



        self.assertTrue(future_0.done())

        self.assertTrue(future_1.done())

        self.assertFalse(future_2.done())



    @gen_test

    def test_event_timeout(self):

        e = locks.Event()

        with self.assertRaises(TimeoutError):

            yield e.wait(timedelta(seconds=0.01))



        # After a timed-out waiter, normal operation works.

        self.io_loop.add_timeout(timedelta(seconds=0.01), e.set)

        yield e.wait(timedelta(seconds=1))



    def test_event_set_multiple(self):

        e = locks.Event()

        e.set()

        e.set()

        self.assertTrue(e.is_set())



    def test_event_wait_clear(self):

        e = locks.Event()

        f0 = asyncio.ensure_future(e.wait())

        e.clear()

        f1 = asyncio.ensure_future(e.wait())

        e.set()

        self.assertTrue(f0.done())

        self.assertTrue(f1.done())





class SemaphoreTest(AsyncTestCase):

    def test_negative_value(self):

        self.assertRaises(ValueError, locks.Semaphore, value=-1)



    def test_repr(self):

        sem = locks.Semaphore()

        self.assertIn("Semaphore", repr(sem))

        self.assertIn("unlocked,value:1", repr(sem))

        sem.acquire()

        self.assertIn("locked", repr(sem))

        self.assertNotIn("waiters", repr(sem))

        sem.acquire()

        self.assertIn("waiters", repr(sem))



    def test_acquire(self):

        sem = locks.Semaphore()

        f0 = asyncio.ensure_future(sem.acquire())

        self.assertTrue(f0.done())



        # Wait for release().

        f1 = asyncio.ensure_future(sem.acquire())

        self.assertFalse(f1.done())

        f2 = asyncio.ensure_future(sem.acquire())

        sem.release()

        self.assertTrue(f1.done())

        self.assertFalse(f2.done())

        sem.release()

        self.assertTrue(f2.done())



        sem.release()

        # Now acquire() is instant.

        self.assertTrue(asyncio.ensure_future(sem.acquire()).done())

        self.assertEqual(0, len(sem._waiters))



    @gen_test

    def test_acquire_timeout(self):

        sem = locks.Semaphore(2)

        yield sem.acquire()

        yield sem.acquire()

        acquire = sem.acquire(timedelta(seconds=0.01))

        self.io_loop.call_later(0.02, sem.release)  # Too late.

        yield gen.sleep(0.3)

        with self.assertRaises(gen.TimeoutError):

            yield acquire



        sem.acquire()

        f = asyncio.ensure_future(sem.acquire())

        self.assertFalse(f.done())

        sem.release()

        self.assertTrue(f.done())



    @gen_test

    def test_acquire_timeout_preempted(self):

        sem = locks.Semaphore(1)

        yield sem.acquire()



        # This fires before the wait times out.

        self.io_loop.call_later(0.01, sem.release)

        acquire = sem.acquire(timedelta(seconds=0.02))

        yield gen.sleep(0.03)

        yield acquire  # No TimeoutError.



    def test_release_unacquired(self):

        # Unbounded releases are allowed, and increment the semaphore's value.

        sem = locks.Semaphore()

        sem.release()

        sem.release()



        # Now the counter is 3. We can acquire three times before blocking.

        self.assertTrue(asyncio.ensure_future(sem.acquire()).done())

        self.assertTrue(asyncio.ensure_future(sem.acquire()).done())

        self.assertTrue(asyncio.ensure_future(sem.acquire()).done())

        self.assertFalse(asyncio.ensure_future(sem.acquire()).done())



    @gen_test

    def test_garbage_collection(self):

        # Test that timed-out waiters are occasionally cleaned from the queue.

        sem = locks.Semaphore(value=0)

        futures = [

            asyncio.ensure_future(sem.acquire(timedelta(seconds=0.01)))

            for _ in range(101)

        ]



        future = asyncio.ensure_future(sem.acquire())

        self.assertEqual(102, len(sem._waiters))



        # Let first 101 waiters time out, triggering a collection.

        yield gen.sleep(0.02)

        self.assertEqual(1, len(sem._waiters))



        # Final waiter is still active.

        self.assertFalse(future.done())

        sem.release()

        self.assertTrue(future.done())



        # Prevent "Future exception was never retrieved" messages.

        for future in futures:

            self.assertRaises(TimeoutError, future.result)





class SemaphoreContextManagerTest(AsyncTestCase):

    @gen_test

    def test_context_manager(self):

        sem = locks.Semaphore()

        with (yield sem.acquire()) as yielded:

            self.assertTrue(yielded is None)



        # Semaphore was released and can be acquired again.

        self.assertTrue(asyncio.ensure_future(sem.acquire()).done())



    @gen_test

    def test_context_manager_async_await(self):

        # Repeat the above test using 'async with'.

        sem = locks.Semaphore()



        async def f():

            async with sem as yielded:

                self.assertTrue(yielded is None)



        yield f()



        # Semaphore was released and can be acquired again.

        self.assertTrue(asyncio.ensure_future(sem.acquire()).done())



    @gen_test

    def test_context_manager_exception(self):

        sem = locks.Semaphore()

        with self.assertRaises(ZeroDivisionError):

            with (yield sem.acquire()):

                1 / 0



        # Semaphore was released and can be acquired again.

        self.assertTrue(asyncio.ensure_future(sem.acquire()).done())



    @gen_test

    def test_context_manager_timeout(self):

        sem = locks.Semaphore()

        with (yield sem.acquire(timedelta(seconds=0.01))):

            pass



        # Semaphore was released and can be acquired again.

        self.assertTrue(asyncio.ensure_future(sem.acquire()).done())



    @gen_test

    def test_context_manager_timeout_error(self):

        sem = locks.Semaphore(value=0)

        with self.assertRaises(gen.TimeoutError):

            with (yield sem.acquire(timedelta(seconds=0.01))):

                pass



        # Counter is still 0.

        self.assertFalse(asyncio.ensure_future(sem.acquire()).done())



    @gen_test

    def test_context_manager_contended(self):

        sem = locks.Semaphore()

        history = []



        @gen.coroutine

        def f(index):

            with (yield sem.acquire()):

                history.append("acquired %d" % index)

                yield gen.sleep(0.01)

                history.append("release %d" % index)



        yield [f(i) for i in range(2)]



        expected_history = []

        for i in range(2):

            expected_history.extend(["acquired %d" % i, "release %d" % i])



        self.assertEqual(expected_history, history)



    @gen_test

    def test_yield_sem(self):

        # Ensure we catch a "with (yield sem)", which should be

        # "with (yield sem.acquire())".

        with self.assertRaises(gen.BadYieldError):

            with (yield locks.Semaphore()):

                pass



    def test_context_manager_misuse(self):

        # Ensure we catch a "with sem", which should be

        # "with (yield sem.acquire())".

        with self.assertRaises(RuntimeError):

            with locks.Semaphore():

                pass





class BoundedSemaphoreTest(AsyncTestCase):

    def test_release_unacquired(self):

        sem = locks.BoundedSemaphore()

        self.assertRaises(ValueError, sem.release)

        # Value is 0.

        sem.acquire()

        # Block on acquire().

        future = asyncio.ensure_future(sem.acquire())

        self.assertFalse(future.done())

        sem.release()

        self.assertTrue(future.done())

        # Value is 1.

        sem.release()

        self.assertRaises(ValueError, sem.release)





class LockTests(AsyncTestCase):

    def test_repr(self):

        lock = locks.Lock()

        # No errors.

        repr(lock)

        lock.acquire()

        repr(lock)



    def test_acquire_release(self):

        lock = locks.Lock()

        self.assertTrue(asyncio.ensure_future(lock.acquire()).done())

        future = asyncio.ensure_future(lock.acquire())

        self.assertFalse(future.done())

        lock.release()

        self.assertTrue(future.done())



    @gen_test

    def test_acquire_fifo(self):

        lock = locks.Lock()

        self.assertTrue(asyncio.ensure_future(lock.acquire()).done())

        N = 5

        history = []



        @gen.coroutine

        def f(idx):

            with (yield lock.acquire()):

                history.append(idx)



        futures = [f(i) for i in range(N)]

        self.assertFalse(any(future.done() for future in futures))

        lock.release()

        yield futures

        self.assertEqual(list(range(N)), history)



    @gen_test

    def test_acquire_fifo_async_with(self):

        # Repeat the above test using `async with lock:`

        # instead of `with (yield lock.acquire()):`.

        lock = locks.Lock()

        self.assertTrue(asyncio.ensure_future(lock.acquire()).done())

        N = 5

        history = []



        async def f(idx):

            async with lock:

                history.append(idx)



        futures = [f(i) for i in range(N)]

        lock.release()

        yield futures

        self.assertEqual(list(range(N)), history)



    @gen_test

    def test_acquire_timeout(self):

        lock = locks.Lock()

        lock.acquire()

        with self.assertRaises(gen.TimeoutError):

            yield lock.acquire(timeout=timedelta(seconds=0.01))



        # Still locked.

        self.assertFalse(asyncio.ensure_future(lock.acquire()).done())



    def test_multi_release(self):

        lock = locks.Lock()

        self.assertRaises(RuntimeError, lock.release)

        lock.acquire()

        lock.release()

        self.assertRaises(RuntimeError, lock.release)



    @gen_test

    def test_yield_lock(self):

        # Ensure we catch a "with (yield lock)", which should be

        # "with (yield lock.acquire())".

        with self.assertRaises(gen.BadYieldError):

            with (yield locks.Lock()):

                pass



    def test_context_manager_misuse(self):

        # Ensure we catch a "with lock", which should be

        # "with (yield lock.acquire())".

        with self.assertRaises(RuntimeError):

            with locks.Lock():

                pass





if __name__ == "__main__":

    unittest.main()

#

# Copyright 2012 Facebook

#

# Licensed under the Apache License, Version 2.0 (the "License"); you may

# not use this file except in compliance with the License. You may obtain

# a copy of the License at

#

#     http://www.apache.org/licenses/LICENSE-2.0

#

# Unless required by applicable law or agreed to in writing, software

# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT

# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the

# License for the specific language governing permissions and limitations

# under the License.

import contextlib

import glob

import logging

import os

import re

import subprocess

import sys

import tempfile

import unittest

import warnings



from tornado.escape import utf8

from tornado.log import LogFormatter, define_logging_options, enable_pretty_logging

from tornado.options import OptionParser

from tornado.util import basestring_type





@contextlib.contextmanager

def ignore_bytes_warning():

    with warnings.catch_warnings():

        warnings.simplefilter("ignore", category=BytesWarning)

        yield





class LogFormatterTest(unittest.TestCase):

    # Matches the output of a single logging call (which may be multiple lines

    # if a traceback was included, so we use the DOTALL option)

    LINE_RE = re.compile(

        b"(?s)\x01\\[E [0-9]{6} [0-9]{2}:[0-9]{2}:[0-9]{2} log_test:[0-9]+\\]\x02 (.*)"

    )



    def setUp(self):

        self.formatter = LogFormatter(color=False)

        # Fake color support.  We can't guarantee anything about the $TERM

        # variable when the tests are run, so just patch in some values

        # for testing.  (testing with color off fails to expose some potential

        # encoding issues from the control characters)

        self.formatter._colors = {logging.ERROR: u"\u0001"}

        self.formatter._normal = u"\u0002"

        # construct a Logger directly to bypass getLogger's caching

        self.logger = logging.Logger("LogFormatterTest")

        self.logger.propagate = False

        self.tempdir = tempfile.mkdtemp()

        self.filename = os.path.join(self.tempdir, "log.out")

        self.handler = self.make_handler(self.filename)

        self.handler.setFormatter(self.formatter)

        self.logger.addHandler(self.handler)



    def tearDown(self):

        self.handler.close()

        os.unlink(self.filename)

        os.rmdir(self.tempdir)



    def make_handler(self, filename):

        # Base case: default setup without explicit encoding.

        # In python 2, supports arbitrary byte strings and unicode objects

        # that contain only ascii.  In python 3, supports ascii-only unicode

        # strings (but byte strings will be repr'd automatically).

        return logging.FileHandler(filename)



    def get_output(self):

        with open(self.filename, "rb") as f:

            line = f.read().strip()

            m = LogFormatterTest.LINE_RE.match(line)

            if m:

                return m.group(1)

            else:

                raise Exception("output didn't match regex: %r" % line)



    def test_basic_logging(self):

        self.logger.error("foo")

        self.assertEqual(self.get_output(), b"foo")



    def test_bytes_logging(self):

        with ignore_bytes_warning():

            # This will be "\xe9" on python 2 or "b'\xe9'" on python 3

            self.logger.error(b"\xe9")

            self.assertEqual(self.get_output(), utf8(repr(b"\xe9")))



    def test_utf8_logging(self):

        with ignore_bytes_warning():

            self.logger.error(u"\u00e9".encode("utf8"))

        if issubclass(bytes, basestring_type):

            # on python 2, utf8 byte strings (and by extension ascii byte

            # strings) are passed through as-is.

            self.assertEqual(self.get_output(), utf8(u"\u00e9"))

        else:

            # on python 3, byte strings always get repr'd even if

            # they're ascii-only, so this degenerates into another

            # copy of test_bytes_logging.

            self.assertEqual(self.get_output(), utf8(repr(utf8(u"\u00e9"))))



    def test_bytes_exception_logging(self):

        try:

            raise Exception(b"\xe9")

        except Exception:

            self.logger.exception("caught exception")

        # This will be "Exception: \xe9" on python 2 or

        # "Exception: b'\xe9'" on python 3.

        output = self.get_output()

        self.assertRegexpMatches(output, br"Exception.*\\xe9")

        # The traceback contains newlines, which should not have been escaped.

        self.assertNotIn(br"\n", output)





class UnicodeLogFormatterTest(LogFormatterTest):

    def make_handler(self, filename):

        # Adding an explicit encoding configuration allows non-ascii unicode

        # strings in both python 2 and 3, without changing the behavior

        # for byte strings.

        return logging.FileHandler(filename, encoding="utf8")



    def test_unicode_logging(self):

        self.logger.error(u"\u00e9")

        self.assertEqual(self.get_output(), utf8(u"\u00e9"))





class EnablePrettyLoggingTest(unittest.TestCase):

    def setUp(self):

        super(EnablePrettyLoggingTest, self).setUp()

        self.options = OptionParser()

        define_logging_options(self.options)

        self.logger = logging.Logger("tornado.test.log_test.EnablePrettyLoggingTest")

        self.logger.propagate = False



    def test_log_file(self):

        tmpdir = tempfile.mkdtemp()

        try:

            self.options.log_file_prefix = tmpdir + "/test_log"

            enable_pretty_logging(options=self.options, logger=self.logger)

            self.assertEqual(1, len(self.logger.handlers))

            self.logger.error("hello")

            self.logger.handlers[0].flush()

            filenames = glob.glob(tmpdir + "/test_log*")

            self.assertEqual(1, len(filenames))

            with open(filenames[0]) as f:

                self.assertRegexpMatches(f.read(), r"^\[E [^]]*\] hello$")

        finally:

            for handler in self.logger.handlers:

                handler.flush()

                handler.close()

            for filename in glob.glob(tmpdir + "/test_log*"):

                os.unlink(filename)

            os.rmdir(tmpdir)



    def test_log_file_with_timed_rotating(self):

        tmpdir = tempfile.mkdtemp()

        try:

            self.options.log_file_prefix = tmpdir + "/test_log"

            self.options.log_rotate_mode = "time"

            enable_pretty_logging(options=self.options, logger=self.logger)

            self.logger.error("hello")

            self.logger.handlers[0].flush()

            filenames = glob.glob(tmpdir + "/test_log*")

            self.assertEqual(1, len(filenames))

            with open(filenames[0]) as f:

                self.assertRegexpMatches(f.read(), r"^\[E [^]]*\] hello$")

        finally:

            for handler in self.logger.handlers:

                handler.flush()

                handler.close()

            for filename in glob.glob(tmpdir + "/test_log*"):

                os.unlink(filename)

            os.rmdir(tmpdir)



    def test_wrong_rotate_mode_value(self):

        try:

            self.options.log_file_prefix = "some_path"

            self.options.log_rotate_mode = "wrong_mode"

            self.assertRaises(

                ValueError,

                enable_pretty_logging,

                options=self.options,

                logger=self.logger,

            )

        finally:

            for handler in self.logger.handlers:

                handler.flush()

                handler.close()





class LoggingOptionTest(unittest.TestCase):

    """Test the ability to enable and disable Tornado's logging hooks."""



    def logs_present(self, statement, args=None):

        # Each test may manipulate and/or parse the options and then logs

        # a line at the 'info' level.  This level is ignored in the

        # logging module by default, but Tornado turns it on by default

        # so it is the easiest way to tell whether tornado's logging hooks

        # ran.

        IMPORT = "from tornado.options import options, parse_command_line"

        LOG_INFO = 'import logging; logging.info("hello")'

        program = ";".join([IMPORT, statement, LOG_INFO])

        proc = subprocess.Popen(

            [sys.executable, "-c", program] + (args or []),

            stdout=subprocess.PIPE,

            stderr=subprocess.STDOUT,

        )

        stdout, stderr = proc.communicate()

        self.assertEqual(proc.returncode, 0, "process failed: %r" % stdout)

        return b"hello" in stdout



    def test_default(self):

        self.assertFalse(self.logs_present("pass"))



    def test_tornado_default(self):

        self.assertTrue(self.logs_present("parse_command_line()"))



    def test_disable_command_line(self):

        self.assertFalse(self.logs_present("parse_command_line()", ["--logging=none"]))



    def test_disable_command_line_case_insensitive(self):

        self.assertFalse(self.logs_present("parse_command_line()", ["--logging=None"]))



    def test_disable_code_string(self):

        self.assertFalse(

            self.logs_present('options.logging = "none"; parse_command_line()')

        )



    def test_disable_code_none(self):

        self.assertFalse(

            self.logs_present("options.logging = None; parse_command_line()")

        )



    def test_disable_override(self):

        # command line trumps code defaults

        self.assertTrue(

            self.logs_present(

                "options.logging = None; parse_command_line()", ["--logging=info"]

            )

        )

import errno

import os

import signal

import socket

from subprocess import Popen

import sys

import time

import unittest



from tornado.netutil import (

    BlockingResolver,

    OverrideResolver,

    ThreadedResolver,

    is_valid_ip,

    bind_sockets,

)

from tornado.testing import AsyncTestCase, gen_test, bind_unused_port

from tornado.test.util import skipIfNoNetwork



import typing



if typing.TYPE_CHECKING:

    from typing import List  # noqa: F401



try:

    import pycares  # type: ignore

except ImportError:

    pycares = None

else:

    from tornado.platform.caresresolver import CaresResolver



try:

    import twisted  # type: ignore

    import twisted.names  # type: ignore

except ImportError:

    twisted = None

else:

    from tornado.platform.twisted import TwistedResolver





class _ResolverTestMixin(object):

    @gen_test

    def test_localhost(self):

        addrinfo = yield self.resolver.resolve("localhost", 80, socket.AF_UNSPEC)

        self.assertIn((socket.AF_INET, ("127.0.0.1", 80)), addrinfo)





# It is impossible to quickly and consistently generate an error in name

# resolution, so test this case separately, using mocks as needed.

class _ResolverErrorTestMixin(object):

    @gen_test

    def test_bad_host(self):

        with self.assertRaises(IOError):

            yield self.resolver.resolve("an invalid domain", 80, socket.AF_UNSPEC)





def _failing_getaddrinfo(*args):

    """Dummy implementation of getaddrinfo for use in mocks"""

    raise socket.gaierror(errno.EIO, "mock: lookup failed")





@skipIfNoNetwork

class BlockingResolverTest(AsyncTestCase, _ResolverTestMixin):

    def setUp(self):

        super(BlockingResolverTest, self).setUp()

        self.resolver = BlockingResolver()





# getaddrinfo-based tests need mocking to reliably generate errors;

# some configurations are slow to produce errors and take longer than

# our default timeout.

class BlockingResolverErrorTest(AsyncTestCase, _ResolverErrorTestMixin):

    def setUp(self):

        super(BlockingResolverErrorTest, self).setUp()

        self.resolver = BlockingResolver()

        self.real_getaddrinfo = socket.getaddrinfo

        socket.getaddrinfo = _failing_getaddrinfo



    def tearDown(self):

        socket.getaddrinfo = self.real_getaddrinfo

        super(BlockingResolverErrorTest, self).tearDown()





class OverrideResolverTest(AsyncTestCase, _ResolverTestMixin):

    def setUp(self):

        super(OverrideResolverTest, self).setUp()

        mapping = {

            ("google.com", 80): ("1.2.3.4", 80),

            ("google.com", 80, socket.AF_INET): ("1.2.3.4", 80),

            ("google.com", 80, socket.AF_INET6): (

                "2a02:6b8:7c:40c:c51e:495f:e23a:3",

                80,

            ),

        }

        self.resolver = OverrideResolver(BlockingResolver(), mapping)



    @gen_test

    def test_resolve_multiaddr(self):

        result = yield self.resolver.resolve("google.com", 80, socket.AF_INET)

        self.assertIn((socket.AF_INET, ("1.2.3.4", 80)), result)



        result = yield self.resolver.resolve("google.com", 80, socket.AF_INET6)

        self.assertIn(

            (socket.AF_INET6, ("2a02:6b8:7c:40c:c51e:495f:e23a:3", 80, 0, 0)), result

        )





@skipIfNoNetwork

class ThreadedResolverTest(AsyncTestCase, _ResolverTestMixin):

    def setUp(self):

        super(ThreadedResolverTest, self).setUp()

        self.resolver = ThreadedResolver()



    def tearDown(self):

        self.resolver.close()

        super(ThreadedResolverTest, self).tearDown()





class ThreadedResolverErrorTest(AsyncTestCase, _ResolverErrorTestMixin):

    def setUp(self):

        super(ThreadedResolverErrorTest, self).setUp()

        self.resolver = BlockingResolver()

        self.real_getaddrinfo = socket.getaddrinfo

        socket.getaddrinfo = _failing_getaddrinfo



    def tearDown(self):

        socket.getaddrinfo = self.real_getaddrinfo

        super(ThreadedResolverErrorTest, self).tearDown()





@skipIfNoNetwork

@unittest.skipIf(sys.platform == "win32", "preexec_fn not available on win32")

class ThreadedResolverImportTest(unittest.TestCase):

    def test_import(self):

        TIMEOUT = 5



        # Test for a deadlock when importing a module that runs the

        # ThreadedResolver at import-time. See resolve_test.py for

        # full explanation.

        command = [sys.executable, "-c", "import tornado.test.resolve_test_helper"]



        start = time.time()

        popen = Popen(command, preexec_fn=lambda: signal.alarm(TIMEOUT))

        while time.time() - start < TIMEOUT:

            return_code = popen.poll()

            if return_code is not None:

                self.assertEqual(0, return_code)

                return  # Success.

            time.sleep(0.05)



        self.fail("import timed out")





# We do not test errors with CaresResolver:

# Some DNS-hijacking ISPs (e.g. Time Warner) return non-empty results

# with an NXDOMAIN status code.  Most resolvers treat this as an error;

# C-ares returns the results, making the "bad_host" tests unreliable.

# C-ares will try to resolve even malformed names, such as the

# name with spaces used in this test.

@skipIfNoNetwork

@unittest.skipIf(pycares is None, "pycares module not present")

class CaresResolverTest(AsyncTestCase, _ResolverTestMixin):

    def setUp(self):

        super(CaresResolverTest, self).setUp()

        self.resolver = CaresResolver()





# TwistedResolver produces consistent errors in our test cases so we

# could test the regular and error cases in the same class. However,

# in the error cases it appears that cleanup of socket objects is

# handled asynchronously and occasionally results in "unclosed socket"

# warnings if not given time to shut down (and there is no way to

# explicitly shut it down). This makes the test flaky, so we do not

# test error cases here.

@skipIfNoNetwork

@unittest.skipIf(twisted is None, "twisted module not present")

@unittest.skipIf(

    getattr(twisted, "__version__", "0.0") < "12.1", "old version of twisted"

)

class TwistedResolverTest(AsyncTestCase, _ResolverTestMixin):

    def setUp(self):

        super(TwistedResolverTest, self).setUp()

        self.resolver = TwistedResolver()





class IsValidIPTest(unittest.TestCase):

    def test_is_valid_ip(self):

        self.assertTrue(is_valid_ip("127.0.0.1"))

        self.assertTrue(is_valid_ip("4.4.4.4"))

        self.assertTrue(is_valid_ip("::1"))

        self.assertTrue(is_valid_ip("2620:0:1cfe:face:b00c::3"))

        self.assertTrue(not is_valid_ip("www.google.com"))

        self.assertTrue(not is_valid_ip("localhost"))

        self.assertTrue(not is_valid_ip("4.4.4.4<"))

        self.assertTrue(not is_valid_ip(" 127.0.0.1"))

        self.assertTrue(not is_valid_ip(""))

        self.assertTrue(not is_valid_ip(" "))

        self.assertTrue(not is_valid_ip("\n"))

        self.assertTrue(not is_valid_ip("\x00"))





class TestPortAllocation(unittest.TestCase):

    def test_same_port_allocation(self):

        if "TRAVIS" in os.environ:

            self.skipTest("dual-stack servers often have port conflicts on travis")

        sockets = bind_sockets(0, "localhost")

        try:

            port = sockets[0].getsockname()[1]

            self.assertTrue(all(s.getsockname()[1] == port for s in sockets[1:]))

        finally:

            for sock in sockets:

                sock.close()



    @unittest.skipIf(

        not hasattr(socket, "SO_REUSEPORT"), "SO_REUSEPORT is not supported"

    )

    def test_reuse_port(self):

        sockets = []  # type: List[socket.socket]

        socket, port = bind_unused_port(reuse_port=True)

        try:

            sockets = bind_sockets(port, "127.0.0.1", reuse_port=True)

            self.assertTrue(all(s.getsockname()[1] == port for s in sockets))

        finally:

            socket.close()

            for sock in sockets:

                sock.close()

port=443

port=443

username='李康'



foo_bar='a'



my_path = __file__

# -*- coding: utf-8 -*-

import datetime

from io import StringIO

import os

import sys

from unittest import mock

import unittest



from tornado.options import OptionParser, Error

from tornado.util import basestring_type

from tornado.test.util import subTest



import typing



if typing.TYPE_CHECKING:

    from typing import List  # noqa: F401





class Email(object):

    def __init__(self, value):

        if isinstance(value, str) and "@" in value:

            self._value = value

        else:

            raise ValueError()



    @property

    def value(self):

        return self._value





class OptionsTest(unittest.TestCase):

    def test_parse_command_line(self):

        options = OptionParser()

        options.define("port", default=80)

        options.parse_command_line(["main.py", "--port=443"])

        self.assertEqual(options.port, 443)



    def test_parse_config_file(self):

        options = OptionParser()

        options.define("port", default=80)

        options.define("username", default="foo")

        options.define("my_path")

        config_path = os.path.join(

            os.path.dirname(os.path.abspath(__file__)), "options_test.cfg"

        )

        options.parse_config_file(config_path)

        self.assertEqual(options.port, 443)

        self.assertEqual(options.username, "李康")

        self.assertEqual(options.my_path, config_path)



    def test_parse_callbacks(self):

        options = OptionParser()

        self.called = False



        def callback():

            self.called = True



        options.add_parse_callback(callback)



        # non-final parse doesn't run callbacks

        options.parse_command_line(["main.py"], final=False)

        self.assertFalse(self.called)



        # final parse does

        options.parse_command_line(["main.py"])

        self.assertTrue(self.called)



        # callbacks can be run more than once on the same options

        # object if there are multiple final parses

        self.called = False

        options.parse_command_line(["main.py"])

        self.assertTrue(self.called)



    def test_help(self):

        options = OptionParser()

        try:

            orig_stderr = sys.stderr

            sys.stderr = StringIO()

            with self.assertRaises(SystemExit):

                options.parse_command_line(["main.py", "--help"])

            usage = sys.stderr.getvalue()

        finally:

            sys.stderr = orig_stderr

        self.assertIn("Usage:", usage)



    def test_subcommand(self):

        base_options = OptionParser()

        base_options.define("verbose", default=False)

        sub_options = OptionParser()

        sub_options.define("foo", type=str)

        rest = base_options.parse_command_line(

            ["main.py", "--verbose", "subcommand", "--foo=bar"]

        )

        self.assertEqual(rest, ["subcommand", "--foo=bar"])

        self.assertTrue(base_options.verbose)

        rest2 = sub_options.parse_command_line(rest)

        self.assertEqual(rest2, [])

        self.assertEqual(sub_options.foo, "bar")



        # the two option sets are distinct

        try:

            orig_stderr = sys.stderr

            sys.stderr = StringIO()

            with self.assertRaises(Error):

                sub_options.parse_command_line(["subcommand", "--verbose"])

        finally:

            sys.stderr = orig_stderr



    def test_setattr(self):

        options = OptionParser()

        options.define("foo", default=1, type=int)

        options.foo = 2

        self.assertEqual(options.foo, 2)



    def test_setattr_type_check(self):

        # setattr requires that options be the right type and doesn't

        # parse from string formats.

        options = OptionParser()

        options.define("foo", default=1, type=int)

        with self.assertRaises(Error):

            options.foo = "2"



    def test_setattr_with_callback(self):

        values = []  # type: List[int]

        options = OptionParser()

        options.define("foo", default=1, type=int, callback=values.append)

        options.foo = 2

        self.assertEqual(values, [2])



    def _sample_options(self):

        options = OptionParser()

        options.define("a", default=1)

        options.define("b", default=2)

        return options



    def test_iter(self):

        options = self._sample_options()

        # OptionParsers always define 'help'.

        self.assertEqual(set(["a", "b", "help"]), set(iter(options)))



    def test_getitem(self):

        options = self._sample_options()

        self.assertEqual(1, options["a"])



    def test_setitem(self):

        options = OptionParser()

        options.define("foo", default=1, type=int)

        options["foo"] = 2

        self.assertEqual(options["foo"], 2)



    def test_items(self):

        options = self._sample_options()

        # OptionParsers always define 'help'.

        expected = [("a", 1), ("b", 2), ("help", options.help)]

        actual = sorted(options.items())

        self.assertEqual(expected, actual)



    def test_as_dict(self):

        options = self._sample_options()

        expected = {"a": 1, "b": 2, "help": options.help}

        self.assertEqual(expected, options.as_dict())



    def test_group_dict(self):

        options = OptionParser()

        options.define("a", default=1)

        options.define("b", group="b_group", default=2)



        frame = sys._getframe(0)

        this_file = frame.f_code.co_filename

        self.assertEqual(set(["b_group", "", this_file]), options.groups())



        b_group_dict = options.group_dict("b_group")

        self.assertEqual({"b": 2}, b_group_dict)



        self.assertEqual({}, options.group_dict("nonexistent"))



    def test_mock_patch(self):

        # ensure that our setattr hooks don't interfere with mock.patch

        options = OptionParser()

        options.define("foo", default=1)

        options.parse_command_line(["main.py", "--foo=2"])

        self.assertEqual(options.foo, 2)



        with mock.patch.object(options.mockable(), "foo", 3):

            self.assertEqual(options.foo, 3)

        self.assertEqual(options.foo, 2)



        # Try nested patches mixed with explicit sets

        with mock.patch.object(options.mockable(), "foo", 4):

            self.assertEqual(options.foo, 4)

            options.foo = 5

            self.assertEqual(options.foo, 5)

            with mock.patch.object(options.mockable(), "foo", 6):

                self.assertEqual(options.foo, 6)

            self.assertEqual(options.foo, 5)

        self.assertEqual(options.foo, 2)



    def _define_options(self):

        options = OptionParser()

        options.define("str", type=str)

        options.define("basestring", type=basestring_type)

        options.define("int", type=int)

        options.define("float", type=float)

        options.define("datetime", type=datetime.datetime)

        options.define("timedelta", type=datetime.timedelta)

        options.define("email", type=Email)

        options.define("list-of-int", type=int, multiple=True)

        return options



    def _check_options_values(self, options):

        self.assertEqual(options.str, "asdf")

        self.assertEqual(options.basestring, "qwer")

        self.assertEqual(options.int, 42)

        self.assertEqual(options.float, 1.5)

        self.assertEqual(options.datetime, datetime.datetime(2013, 4, 28, 5, 16))

        self.assertEqual(options.timedelta, datetime.timedelta(seconds=45))

        self.assertEqual(options.email.value, "tornado@web.com")

        self.assertTrue(isinstance(options.email, Email))

        self.assertEqual(options.list_of_int, [1, 2, 3])



    def test_types(self):

        options = self._define_options()

        options.parse_command_line(

            [

                "main.py",

                "--str=asdf",

                "--basestring=qwer",

                "--int=42",

                "--float=1.5",

                "--datetime=2013-04-28 05:16",

                "--timedelta=45s",

                "--email=tornado@web.com",

                "--list-of-int=1,2,3",

            ]

        )

        self._check_options_values(options)



    def test_types_with_conf_file(self):

        for config_file_name in (

            "options_test_types.cfg",

            "options_test_types_str.cfg",

        ):

            options = self._define_options()

            options.parse_config_file(

                os.path.join(os.path.dirname(__file__), config_file_name)

            )

            self._check_options_values(options)



    def test_multiple_string(self):

        options = OptionParser()

        options.define("foo", type=str, multiple=True)

        options.parse_command_line(["main.py", "--foo=a,b,c"])

        self.assertEqual(options.foo, ["a", "b", "c"])



    def test_multiple_int(self):

        options = OptionParser()

        options.define("foo", type=int, multiple=True)

        options.parse_command_line(["main.py", "--foo=1,3,5:7"])

        self.assertEqual(options.foo, [1, 3, 5, 6, 7])



    def test_error_redefine(self):

        options = OptionParser()

        options.define("foo")

        with self.assertRaises(Error) as cm:

            options.define("foo")

        self.assertRegexpMatches(str(cm.exception), "Option.*foo.*already defined")



    def test_error_redefine_underscore(self):

        # Ensure that the dash/underscore normalization doesn't

        # interfere with the redefinition error.

        tests = [

            ("foo-bar", "foo-bar"),

            ("foo_bar", "foo_bar"),

            ("foo-bar", "foo_bar"),

            ("foo_bar", "foo-bar"),

        ]

        for a, b in tests:

            with subTest(self, a=a, b=b):

                options = OptionParser()

                options.define(a)

                with self.assertRaises(Error) as cm:

                    options.define(b)

                self.assertRegexpMatches(

                    str(cm.exception), "Option.*foo.bar.*already defined"

                )



    def test_dash_underscore_cli(self):

        # Dashes and underscores should be interchangeable.

        for defined_name in ["foo-bar", "foo_bar"]:

            for flag in ["--foo-bar=a", "--foo_bar=a"]:

                options = OptionParser()

                options.define(defined_name)

                options.parse_command_line(["main.py", flag])

                # Attr-style access always uses underscores.

                self.assertEqual(options.foo_bar, "a")

                # Dict-style access allows both.

                self.assertEqual(options["foo-bar"], "a")

                self.assertEqual(options["foo_bar"], "a")



    def test_dash_underscore_file(self):

        # No matter how an option was defined, it can be set with underscores

        # in a config file.

        for defined_name in ["foo-bar", "foo_bar"]:

            options = OptionParser()

            options.define(defined_name)

            options.parse_config_file(

                os.path.join(os.path.dirname(__file__), "options_test.cfg")

            )

            self.assertEqual(options.foo_bar, "a")



    def test_dash_underscore_introspection(self):

        # Original names are preserved in introspection APIs.

        options = OptionParser()

        options.define("with-dash", group="g")

        options.define("with_underscore", group="g")

        all_options = ["help", "with-dash", "with_underscore"]

        self.assertEqual(sorted(options), all_options)

        self.assertEqual(sorted(k for (k, v) in options.items()), all_options)

        self.assertEqual(sorted(options.as_dict().keys()), all_options)



        self.assertEqual(

            sorted(options.group_dict("g")), ["with-dash", "with_underscore"]

        )



        # --help shows CLI-style names with dashes.

        buf = StringIO()

        options.print_help(buf)

        self.assertIn("--with-dash", buf.getvalue())

        self.assertIn("--with-underscore", buf.getvalue())

from datetime import datetime, timedelta

from tornado.test.options_test import Email



str = 'asdf'

basestring = 'qwer'

int = 42

float = 1.5

datetime = datetime(2013, 4, 28, 5, 16)

timedelta = timedelta(0, 45)

email = Email('tornado@web.com')

list_of_int = [1, 2, 3]

str = 'asdf'

basestring = 'qwer'

int = 42

float = 1.5

datetime = '2013-04-28 05:16'

timedelta = '45s'

email = 'tornado@web.com'

list_of_int = '1,2,3'

import asyncio

import logging

import os

import signal

import subprocess

import sys

import unittest



from tornado.httpclient import HTTPClient, HTTPError

from tornado.httpserver import HTTPServer

from tornado.ioloop import IOLoop

from tornado.log import gen_log

from tornado.process import fork_processes, task_id, Subprocess

from tornado.simple_httpclient import SimpleAsyncHTTPClient

from tornado.testing import bind_unused_port, ExpectLog, AsyncTestCase, gen_test

from tornado.test.util import skipIfNonUnix

from tornado.web import RequestHandler, Application





# Not using AsyncHTTPTestCase because we need control over the IOLoop.

@skipIfNonUnix

class ProcessTest(unittest.TestCase):

    def get_app(self):

        class ProcessHandler(RequestHandler):

            def get(self):

                if self.get_argument("exit", None):

                    # must use os._exit instead of sys.exit so unittest's

                    # exception handler doesn't catch it

                    os._exit(int(self.get_argument("exit")))

                if self.get_argument("signal", None):

                    os.kill(os.getpid(), int(self.get_argument("signal")))

                self.write(str(os.getpid()))



        return Application([("/", ProcessHandler)])



    def tearDown(self):

        if task_id() is not None:

            # We're in a child process, and probably got to this point

            # via an uncaught exception.  If we return now, both

            # processes will continue with the rest of the test suite.

            # Exit now so the parent process will restart the child

            # (since we don't have a clean way to signal failure to

            # the parent that won't restart)

            logging.error("aborting child process from tearDown")

            logging.shutdown()

            os._exit(1)

        # In the surviving process, clear the alarm we set earlier

        signal.alarm(0)

        super(ProcessTest, self).tearDown()



    def test_multi_process(self):

        # This test doesn't work on twisted because we use the global

        # reactor and don't restore it to a sane state after the fork

        # (asyncio has the same issue, but we have a special case in

        # place for it).

        with ExpectLog(

            gen_log, "(Starting .* processes|child .* exited|uncaught exception)"

        ):

            sock, port = bind_unused_port()



            def get_url(path):

                return "http://127.0.0.1:%d%s" % (port, path)



            # ensure that none of these processes live too long

            signal.alarm(5)  # master process

            try:

                id = fork_processes(3, max_restarts=3)

                self.assertTrue(id is not None)

                signal.alarm(5)  # child processes

            except SystemExit as e:

                # if we exit cleanly from fork_processes, all the child processes

                # finished with status 0

                self.assertEqual(e.code, 0)

                self.assertTrue(task_id() is None)

                sock.close()

                return

            try:

                if asyncio is not None:

                    # Reset the global asyncio event loop, which was put into

                    # a broken state by the fork.

                    asyncio.set_event_loop(asyncio.new_event_loop())

                if id in (0, 1):

                    self.assertEqual(id, task_id())

                    server = HTTPServer(self.get_app())

                    server.add_sockets([sock])

                    IOLoop.current().start()

                elif id == 2:

                    self.assertEqual(id, task_id())

                    sock.close()

                    # Always use SimpleAsyncHTTPClient here; the curl

                    # version appears to get confused sometimes if the

                    # connection gets closed before it's had a chance to

                    # switch from writing mode to reading mode.

                    client = HTTPClient(SimpleAsyncHTTPClient)



                    def fetch(url, fail_ok=False):

                        try:

                            return client.fetch(get_url(url))

                        except HTTPError as e:

                            if not (fail_ok and e.code == 599):

                                raise



                    # Make two processes exit abnormally

                    fetch("/?exit=2", fail_ok=True)

                    fetch("/?exit=3", fail_ok=True)



                    # They've been restarted, so a new fetch will work

                    int(fetch("/").body)



                    # Now the same with signals

                    # Disabled because on the mac a process dying with a signal

                    # can trigger an "Application exited abnormally; send error

                    # report to Apple?" prompt.

                    # fetch("/?signal=%d" % signal.SIGTERM, fail_ok=True)

                    # fetch("/?signal=%d" % signal.SIGABRT, fail_ok=True)

                    # int(fetch("/").body)



                    # Now kill them normally so they won't be restarted

                    fetch("/?exit=0", fail_ok=True)

                    # One process left; watch it's pid change

                    pid = int(fetch("/").body)

                    fetch("/?exit=4", fail_ok=True)

                    pid2 = int(fetch("/").body)

                    self.assertNotEqual(pid, pid2)



                    # Kill the last one so we shut down cleanly

                    fetch("/?exit=0", fail_ok=True)



                    os._exit(0)

            except Exception:

                logging.error("exception in child process %d", id, exc_info=True)

                raise





@skipIfNonUnix

class SubprocessTest(AsyncTestCase):

    def term_and_wait(self, subproc):

        subproc.proc.terminate()

        subproc.proc.wait()



    @gen_test

    def test_subprocess(self):

        if IOLoop.configured_class().__name__.endswith("LayeredTwistedIOLoop"):

            # This test fails non-deterministically with LayeredTwistedIOLoop.

            # (the read_until('\n') returns '\n' instead of 'hello\n')

            # This probably indicates a problem with either TornadoReactor

            # or TwistedIOLoop, but I haven't been able to track it down

            # and for now this is just causing spurious travis-ci failures.

            raise unittest.SkipTest(

                "Subprocess tests not compatible with " "LayeredTwistedIOLoop"

            )

        subproc = Subprocess(

            [sys.executable, "-u", "-i"],

            stdin=Subprocess.STREAM,

            stdout=Subprocess.STREAM,

            stderr=subprocess.STDOUT,

        )

        self.addCleanup(lambda: self.term_and_wait(subproc))

        self.addCleanup(subproc.stdout.close)

        self.addCleanup(subproc.stdin.close)

        yield subproc.stdout.read_until(b">>> ")

        subproc.stdin.write(b"print('hello')\n")

        data = yield subproc.stdout.read_until(b"\n")

        self.assertEqual(data, b"hello\n")



        yield subproc.stdout.read_until(b">>> ")

        subproc.stdin.write(b"raise SystemExit\n")

        data = yield subproc.stdout.read_until_close()

        self.assertEqual(data, b"")



    @gen_test

    def test_close_stdin(self):

        # Close the parent's stdin handle and see that the child recognizes it.

        subproc = Subprocess(

            [sys.executable, "-u", "-i"],

            stdin=Subprocess.STREAM,

            stdout=Subprocess.STREAM,

            stderr=subprocess.STDOUT,

        )

        self.addCleanup(lambda: self.term_and_wait(subproc))

        yield subproc.stdout.read_until(b">>> ")

        subproc.stdin.close()

        data = yield subproc.stdout.read_until_close()

        self.assertEqual(data, b"\n")



    @gen_test

    def test_stderr(self):

        # This test is mysteriously flaky on twisted: it succeeds, but logs

        # an error of EBADF on closing a file descriptor.

        subproc = Subprocess(

            [sys.executable, "-u", "-c", r"import sys; sys.stderr.write('hello\n')"],

            stderr=Subprocess.STREAM,

        )

        self.addCleanup(lambda: self.term_and_wait(subproc))

        data = yield subproc.stderr.read_until(b"\n")

        self.assertEqual(data, b"hello\n")

        # More mysterious EBADF: This fails if done with self.addCleanup instead of here.

        subproc.stderr.close()



    def test_sigchild(self):

        Subprocess.initialize()

        self.addCleanup(Subprocess.uninitialize)

        subproc = Subprocess([sys.executable, "-c", "pass"])

        subproc.set_exit_callback(self.stop)

        ret = self.wait()

        self.assertEqual(ret, 0)

        self.assertEqual(subproc.returncode, ret)



    @gen_test

    def test_sigchild_future(self):

        Subprocess.initialize()

        self.addCleanup(Subprocess.uninitialize)

        subproc = Subprocess([sys.executable, "-c", "pass"])

        ret = yield subproc.wait_for_exit()

        self.assertEqual(ret, 0)

        self.assertEqual(subproc.returncode, ret)



    def test_sigchild_signal(self):

        Subprocess.initialize()

        self.addCleanup(Subprocess.uninitialize)

        subproc = Subprocess(

            [sys.executable, "-c", "import time; time.sleep(30)"],

            stdout=Subprocess.STREAM,

        )

        self.addCleanup(subproc.stdout.close)

        subproc.set_exit_callback(self.stop)

        os.kill(subproc.pid, signal.SIGTERM)

        try:

            ret = self.wait(timeout=1.0)

        except AssertionError:

            # We failed to get the termination signal. This test is

            # occasionally flaky on pypy, so try to get a little more

            # information: did the process close its stdout

            # (indicating that the problem is in the parent process's

            # signal handling) or did the child process somehow fail

            # to terminate?

            fut = subproc.stdout.read_until_close()

            fut.add_done_callback(lambda f: self.stop())  # type: ignore

            try:

                self.wait(timeout=1.0)

            except AssertionError:

                raise AssertionError("subprocess failed to terminate")

            else:

                raise AssertionError(

                    "subprocess closed stdout but failed to " "get termination signal"

                )

        self.assertEqual(subproc.returncode, ret)

        self.assertEqual(ret, -signal.SIGTERM)



    @gen_test

    def test_wait_for_exit_raise(self):

        Subprocess.initialize()

        self.addCleanup(Subprocess.uninitialize)

        subproc = Subprocess([sys.executable, "-c", "import sys; sys.exit(1)"])

        with self.assertRaises(subprocess.CalledProcessError) as cm:

            yield subproc.wait_for_exit()

        self.assertEqual(cm.exception.returncode, 1)



    @gen_test

    def test_wait_for_exit_raise_disabled(self):

        Subprocess.initialize()

        self.addCleanup(Subprocess.uninitialize)

        subproc = Subprocess([sys.executable, "-c", "import sys; sys.exit(1)"])

        ret = yield subproc.wait_for_exit(raise_error=False)

        self.assertEqual(ret, 1)

# Licensed under the Apache License, Version 2.0 (the "License"); you may

# not use this file except in compliance with the License. You may obtain

# a copy of the License at

#

#     http://www.apache.org/licenses/LICENSE-2.0

#

# Unless required by applicable law or agreed to in writing, software

# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT

# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the

# License for the specific language governing permissions and limitations

# under the License.



import asyncio

from datetime import timedelta

from random import random

import unittest



from tornado import gen, queues

from tornado.gen import TimeoutError

from tornado.testing import gen_test, AsyncTestCase





class QueueBasicTest(AsyncTestCase):

    def test_repr_and_str(self):

        q = queues.Queue(maxsize=1)  # type: queues.Queue[None]

        self.assertIn(hex(id(q)), repr(q))

        self.assertNotIn(hex(id(q)), str(q))

        q.get()



        for q_str in repr(q), str(q):

            self.assertTrue(q_str.startswith("<Queue"))

            self.assertIn("maxsize=1", q_str)

            self.assertIn("getters[1]", q_str)

            self.assertNotIn("putters", q_str)

            self.assertNotIn("tasks", q_str)



        q.put(None)

        q.put(None)

        # Now the queue is full, this putter blocks.

        q.put(None)



        for q_str in repr(q), str(q):

            self.assertNotIn("getters", q_str)

            self.assertIn("putters[1]", q_str)

            self.assertIn("tasks=2", q_str)



    def test_order(self):

        q = queues.Queue()  # type: queues.Queue[int]

        for i in [1, 3, 2]:

            q.put_nowait(i)



        items = [q.get_nowait() for _ in range(3)]

        self.assertEqual([1, 3, 2], items)



    @gen_test

    def test_maxsize(self):

        self.assertRaises(TypeError, queues.Queue, maxsize=None)

        self.assertRaises(ValueError, queues.Queue, maxsize=-1)



        q = queues.Queue(maxsize=2)  # type: queues.Queue[int]

        self.assertTrue(q.empty())

        self.assertFalse(q.full())

        self.assertEqual(2, q.maxsize)

        self.assertTrue(q.put(0).done())

        self.assertTrue(q.put(1).done())

        self.assertFalse(q.empty())

        self.assertTrue(q.full())

        put2 = q.put(2)

        self.assertFalse(put2.done())

        self.assertEqual(0, (yield q.get()))  # Make room.

        self.assertTrue(put2.done())

        self.assertFalse(q.empty())

        self.assertTrue(q.full())





class QueueGetTest(AsyncTestCase):

    @gen_test

    def test_blocking_get(self):

        q = queues.Queue()  # type: queues.Queue[int]

        q.put_nowait(0)

        self.assertEqual(0, (yield q.get()))



    def test_nonblocking_get(self):

        q = queues.Queue()  # type: queues.Queue[int]

        q.put_nowait(0)

        self.assertEqual(0, q.get_nowait())



    def test_nonblocking_get_exception(self):

        q = queues.Queue()  # type: queues.Queue[int]

        self.assertRaises(queues.QueueEmpty, q.get_nowait)



    @gen_test

    def test_get_with_putters(self):

        q = queues.Queue(1)  # type: queues.Queue[int]

        q.put_nowait(0)

        put = q.put(1)

        self.assertEqual(0, (yield q.get()))

        self.assertIsNone((yield put))



    @gen_test

    def test_blocking_get_wait(self):

        q = queues.Queue()  # type: queues.Queue[int]

        q.put(0)

        self.io_loop.call_later(0.01, q.put, 1)

        self.io_loop.call_later(0.02, q.put, 2)

        self.assertEqual(0, (yield q.get(timeout=timedelta(seconds=1))))

        self.assertEqual(1, (yield q.get(timeout=timedelta(seconds=1))))



    @gen_test

    def test_get_timeout(self):

        q = queues.Queue()  # type: queues.Queue[int]

        get_timeout = q.get(timeout=timedelta(seconds=0.01))

        get = q.get()

        with self.assertRaises(TimeoutError):

            yield get_timeout



        q.put_nowait(0)

        self.assertEqual(0, (yield get))



    @gen_test

    def test_get_timeout_preempted(self):

        q = queues.Queue()  # type: queues.Queue[int]

        get = q.get(timeout=timedelta(seconds=0.01))

        q.put(0)

        yield gen.sleep(0.02)

        self.assertEqual(0, (yield get))



    @gen_test

    def test_get_clears_timed_out_putters(self):

        q = queues.Queue(1)  # type: queues.Queue[int]

        # First putter succeeds, remainder block.

        putters = [q.put(i, timedelta(seconds=0.01)) for i in range(10)]

        put = q.put(10)

        self.assertEqual(10, len(q._putters))

        yield gen.sleep(0.02)

        self.assertEqual(10, len(q._putters))

        self.assertFalse(put.done())  # Final waiter is still active.

        q.put(11)

        self.assertEqual(0, (yield q.get()))  # get() clears the waiters.

        self.assertEqual(1, len(q._putters))

        for putter in putters[1:]:

            self.assertRaises(TimeoutError, putter.result)



    @gen_test

    def test_get_clears_timed_out_getters(self):

        q = queues.Queue()  # type: queues.Queue[int]

        getters = [

            asyncio.ensure_future(q.get(timedelta(seconds=0.01))) for _ in range(10)

        ]

        get = asyncio.ensure_future(q.get())

        self.assertEqual(11, len(q._getters))

        yield gen.sleep(0.02)

        self.assertEqual(11, len(q._getters))

        self.assertFalse(get.done())  # Final waiter is still active.

        q.get()  # get() clears the waiters.

        self.assertEqual(2, len(q._getters))

        for getter in getters:

            self.assertRaises(TimeoutError, getter.result)



    @gen_test

    def test_async_for(self):

        q = queues.Queue()  # type: queues.Queue[int]

        for i in range(5):

            q.put(i)



        async def f():

            results = []

            async for i in q:

                results.append(i)

                if i == 4:

                    return results



        results = yield f()

        self.assertEqual(results, list(range(5)))





class QueuePutTest(AsyncTestCase):

    @gen_test

    def test_blocking_put(self):

        q = queues.Queue()  # type: queues.Queue[int]

        q.put(0)

        self.assertEqual(0, q.get_nowait())



    def test_nonblocking_put_exception(self):

        q = queues.Queue(1)  # type: queues.Queue[int]

        q.put(0)

        self.assertRaises(queues.QueueFull, q.put_nowait, 1)



    @gen_test

    def test_put_with_getters(self):

        q = queues.Queue()  # type: queues.Queue[int]

        get0 = q.get()

        get1 = q.get()

        yield q.put(0)

        self.assertEqual(0, (yield get0))

        yield q.put(1)

        self.assertEqual(1, (yield get1))



    @gen_test

    def test_nonblocking_put_with_getters(self):

        q = queues.Queue()  # type: queues.Queue[int]

        get0 = q.get()

        get1 = q.get()

        q.put_nowait(0)

        # put_nowait does *not* immediately unblock getters.

        yield gen.moment

        self.assertEqual(0, (yield get0))

        q.put_nowait(1)

        yield gen.moment

        self.assertEqual(1, (yield get1))



    @gen_test

    def test_blocking_put_wait(self):

        q = queues.Queue(1)  # type: queues.Queue[int]

        q.put_nowait(0)

        self.io_loop.call_later(0.01, q.get)

        self.io_loop.call_later(0.02, q.get)

        futures = [q.put(0), q.put(1)]

        self.assertFalse(any(f.done() for f in futures))

        yield futures



    @gen_test

    def test_put_timeout(self):

        q = queues.Queue(1)  # type: queues.Queue[int]

        q.put_nowait(0)  # Now it's full.

        put_timeout = q.put(1, timeout=timedelta(seconds=0.01))

        put = q.put(2)

        with self.assertRaises(TimeoutError):

            yield put_timeout



        self.assertEqual(0, q.get_nowait())

        # 1 was never put in the queue.

        self.assertEqual(2, (yield q.get()))



        # Final get() unblocked this putter.

        yield put



    @gen_test

    def test_put_timeout_preempted(self):

        q = queues.Queue(1)  # type: queues.Queue[int]

        q.put_nowait(0)

        put = q.put(1, timeout=timedelta(seconds=0.01))

        q.get()

        yield gen.sleep(0.02)

        yield put  # No TimeoutError.



    @gen_test

    def test_put_clears_timed_out_putters(self):

        q = queues.Queue(1)  # type: queues.Queue[int]

        # First putter succeeds, remainder block.

        putters = [q.put(i, timedelta(seconds=0.01)) for i in range(10)]

        put = q.put(10)

        self.assertEqual(10, len(q._putters))

        yield gen.sleep(0.02)

        self.assertEqual(10, len(q._putters))

        self.assertFalse(put.done())  # Final waiter is still active.

        q.put(11)  # put() clears the waiters.

        self.assertEqual(2, len(q._putters))

        for putter in putters[1:]:

            self.assertRaises(TimeoutError, putter.result)



    @gen_test

    def test_put_clears_timed_out_getters(self):

        q = queues.Queue()  # type: queues.Queue[int]

        getters = [

            asyncio.ensure_future(q.get(timedelta(seconds=0.01))) for _ in range(10)

        ]

        get = asyncio.ensure_future(q.get())

        q.get()

        self.assertEqual(12, len(q._getters))

        yield gen.sleep(0.02)

        self.assertEqual(12, len(q._getters))

        self.assertFalse(get.done())  # Final waiters still active.

        q.put(0)  # put() clears the waiters.

        self.assertEqual(1, len(q._getters))

        self.assertEqual(0, (yield get))

        for getter in getters:

            self.assertRaises(TimeoutError, getter.result)



    @gen_test

    def test_float_maxsize(self):

        # If a float is passed for maxsize, a reasonable limit should

        # be enforced, instead of being treated as unlimited.

        # It happens to be rounded up.

        # http://bugs.python.org/issue21723

        q = queues.Queue(maxsize=1.3)  # type: ignore

        self.assertTrue(q.empty())

        self.assertFalse(q.full())

        q.put_nowait(0)

        q.put_nowait(1)

        self.assertFalse(q.empty())

        self.assertTrue(q.full())

        self.assertRaises(queues.QueueFull, q.put_nowait, 2)

        self.assertEqual(0, q.get_nowait())

        self.assertFalse(q.empty())

        self.assertFalse(q.full())



        yield q.put(2)

        put = q.put(3)

        self.assertFalse(put.done())

        self.assertEqual(1, (yield q.get()))

        yield put

        self.assertTrue(q.full())





class QueueJoinTest(AsyncTestCase):

    queue_class = queues.Queue



    def test_task_done_underflow(self):

        q = self.queue_class()

        self.assertRaises(ValueError, q.task_done)



    @gen_test

    def test_task_done(self):

        q = self.queue_class()

        for i in range(100):

            q.put_nowait(i)



        self.accumulator = 0



        @gen.coroutine

        def worker():

            while True:

                item = yield q.get()

                self.accumulator += item

                q.task_done()

                yield gen.sleep(random() * 0.01)



        # Two coroutines share work.

        worker()

        worker()

        yield q.join()

        self.assertEqual(sum(range(100)), self.accumulator)



    @gen_test

    def test_task_done_delay(self):

        # Verify it is task_done(), not get(), that unblocks join().

        q = self.queue_class()

        q.put_nowait(0)

        join = q.join()

        self.assertFalse(join.done())

        yield q.get()

        self.assertFalse(join.done())

        yield gen.moment

        self.assertFalse(join.done())

        q.task_done()

        self.assertTrue(join.done())



    @gen_test

    def test_join_empty_queue(self):

        q = self.queue_class()

        yield q.join()

        yield q.join()



    @gen_test

    def test_join_timeout(self):

        q = self.queue_class()

        q.put(0)

        with self.assertRaises(TimeoutError):

            yield q.join(timeout=timedelta(seconds=0.01))





class PriorityQueueJoinTest(QueueJoinTest):

    queue_class = queues.PriorityQueue



    @gen_test

    def test_order(self):

        q = self.queue_class(maxsize=2)

        q.put_nowait((1, "a"))

        q.put_nowait((0, "b"))

        self.assertTrue(q.full())

        q.put((3, "c"))

        q.put((2, "d"))

        self.assertEqual((0, "b"), q.get_nowait())

        self.assertEqual((1, "a"), (yield q.get()))

        self.assertEqual((2, "d"), q.get_nowait())

        self.assertEqual((3, "c"), (yield q.get()))

        self.assertTrue(q.empty())





class LifoQueueJoinTest(QueueJoinTest):

    queue_class = queues.LifoQueue



    @gen_test

    def test_order(self):

        q = self.queue_class(maxsize=2)

        q.put_nowait(1)

        q.put_nowait(0)

        self.assertTrue(q.full())

        q.put(3)

        q.put(2)

        self.assertEqual(3, q.get_nowait())

        self.assertEqual(2, (yield q.get()))

        self.assertEqual(0, q.get_nowait())

        self.assertEqual(1, (yield q.get()))

        self.assertTrue(q.empty())





class ProducerConsumerTest(AsyncTestCase):

    @gen_test

    def test_producer_consumer(self):

        q = queues.Queue(maxsize=3)  # type: queues.Queue[int]

        history = []



        # We don't yield between get() and task_done(), so get() must wait for

        # the next tick. Otherwise we'd immediately call task_done and unblock

        # join() before q.put() resumes, and we'd only process the first four

        # items.

        @gen.coroutine

        def consumer():

            while True:

                history.append((yield q.get()))

                q.task_done()



        @gen.coroutine

        def producer():

            for item in range(10):

                yield q.put(item)



        consumer()

        yield producer()

        yield q.join()

        self.assertEqual(list(range(10)), history)





if __name__ == "__main__":

    unittest.main()

from tornado.ioloop import IOLoop

from tornado.netutil import ThreadedResolver



# When this module is imported, it runs getaddrinfo on a thread. Since

# the hostname is unicode, getaddrinfo attempts to import encodings.idna

# but blocks on the import lock. Verify that ThreadedResolver avoids

# this deadlock.



resolver = ThreadedResolver()

IOLoop.current().run_sync(lambda: resolver.resolve(u"localhost", 80))

# Licensed under the Apache License, Version 2.0 (the "License"); you may

# not use this file except in compliance with the License. You may obtain

# a copy of the License at

#

#     http://www.apache.org/licenses/LICENSE-2.0

#

# Unless required by applicable law or agreed to in writing, software

# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT

# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the

# License for the specific language governing permissions and limitations

# under the License.



from tornado.httputil import (

    HTTPHeaders,

    HTTPMessageDelegate,

    HTTPServerConnectionDelegate,

    ResponseStartLine,

)

from tornado.routing import (

    HostMatches,

    PathMatches,

    ReversibleRouter,

    Router,

    Rule,

    RuleRouter,

)

from tornado.testing import AsyncHTTPTestCase

from tornado.web import Application, HTTPError, RequestHandler

from tornado.wsgi import WSGIContainer



import typing  # noqa: F401





class BasicRouter(Router):

    def find_handler(self, request, **kwargs):

        class MessageDelegate(HTTPMessageDelegate):

            def __init__(self, connection):

                self.connection = connection



            def finish(self):

                self.connection.write_headers(

                    ResponseStartLine("HTTP/1.1", 200, "OK"),

                    HTTPHeaders({"Content-Length": "2"}),

                    b"OK",

                )

                self.connection.finish()



        return MessageDelegate(request.connection)





class BasicRouterTestCase(AsyncHTTPTestCase):

    def get_app(self):

        return BasicRouter()



    def test_basic_router(self):

        response = self.fetch("/any_request")

        self.assertEqual(response.body, b"OK")





resources = {}  # type: typing.Dict[str, bytes]





class GetResource(RequestHandler):

    def get(self, path):

        if path not in resources:

            raise HTTPError(404)



        self.finish(resources[path])





class PostResource(RequestHandler):

    def post(self, path):

        resources[path] = self.request.body





class HTTPMethodRouter(Router):

    def __init__(self, app):

        self.app = app



    def find_handler(self, request, **kwargs):

        handler = GetResource if request.method == "GET" else PostResource

        return self.app.get_handler_delegate(request, handler, path_args=[request.path])





class HTTPMethodRouterTestCase(AsyncHTTPTestCase):

    def get_app(self):

        return HTTPMethodRouter(Application())



    def test_http_method_router(self):

        response = self.fetch("/post_resource", method="POST", body="data")

        self.assertEqual(response.code, 200)



        response = self.fetch("/get_resource")

        self.assertEqual(response.code, 404)



        response = self.fetch("/post_resource")

        self.assertEqual(response.code, 200)

        self.assertEqual(response.body, b"data")





def _get_named_handler(handler_name):

    class Handler(RequestHandler):

        def get(self, *args, **kwargs):

            if self.application.settings.get("app_name") is not None:

                self.write(self.application.settings["app_name"] + ": ")



            self.finish(handler_name + ": " + self.reverse_url(handler_name))



    return Handler





FirstHandler = _get_named_handler("first_handler")

SecondHandler = _get_named_handler("second_handler")





class CustomRouter(ReversibleRouter):

    def __init__(self):

        super(CustomRouter, self).__init__()

        self.routes = {}  # type: typing.Dict[str, typing.Any]



    def add_routes(self, routes):

        self.routes.update(routes)



    def find_handler(self, request, **kwargs):

        if request.path in self.routes:

            app, handler = self.routes[request.path]

            return app.get_handler_delegate(request, handler)



    def reverse_url(self, name, *args):

        handler_path = "/" + name

        return handler_path if handler_path in self.routes else None





class CustomRouterTestCase(AsyncHTTPTestCase):

    def get_app(self):

        router = CustomRouter()



        class CustomApplication(Application):

            def reverse_url(self, name, *args):

                return router.reverse_url(name, *args)



        app1 = CustomApplication(app_name="app1")

        app2 = CustomApplication(app_name="app2")



        router.add_routes(

            {

                "/first_handler": (app1, FirstHandler),

                "/second_handler": (app2, SecondHandler),

                "/first_handler_second_app": (app2, FirstHandler),

            }

        )



        return router



    def test_custom_router(self):

        response = self.fetch("/first_handler")

        self.assertEqual(response.body, b"app1: first_handler: /first_handler")

        response = self.fetch("/second_handler")

        self.assertEqual(response.body, b"app2: second_handler: /second_handler")

        response = self.fetch("/first_handler_second_app")

        self.assertEqual(response.body, b"app2: first_handler: /first_handler")





class ConnectionDelegate(HTTPServerConnectionDelegate):

    def start_request(self, server_conn, request_conn):

        class MessageDelegate(HTTPMessageDelegate):

            def __init__(self, connection):

                self.connection = connection



            def finish(self):

                response_body = b"OK"

                self.connection.write_headers(

                    ResponseStartLine("HTTP/1.1", 200, "OK"),

                    HTTPHeaders({"Content-Length": str(len(response_body))}),

                )

                self.connection.write(response_body)

                self.connection.finish()



        return MessageDelegate(request_conn)





class RuleRouterTest(AsyncHTTPTestCase):

    def get_app(self):

        app = Application()



        def request_callable(request):

            request.connection.write_headers(

                ResponseStartLine("HTTP/1.1", 200, "OK"),

                HTTPHeaders({"Content-Length": "2"}),

            )

            request.connection.write(b"OK")

            request.connection.finish()



        router = CustomRouter()

        router.add_routes(

            {"/nested_handler": (app, _get_named_handler("nested_handler"))}

        )



        app.add_handlers(

            ".*",

            [

                (

                    HostMatches("www.example.com"),

                    [

                        (

                            PathMatches("/first_handler"),

                            "tornado.test.routing_test.SecondHandler",

                            {},

                            "second_handler",

                        )

                    ],

                ),

                Rule(PathMatches("/.*handler"), router),

                Rule(PathMatches("/first_handler"), FirstHandler, name="first_handler"),

                Rule(PathMatches("/request_callable"), request_callable),

                ("/connection_delegate", ConnectionDelegate()),

            ],

        )



        return app



    def test_rule_based_router(self):

        response = self.fetch("/first_handler")

        self.assertEqual(response.body, b"first_handler: /first_handler")



        response = self.fetch("/first_handler", headers={"Host": "www.example.com"})

        self.assertEqual(response.body, b"second_handler: /first_handler")



        response = self.fetch("/nested_handler")

        self.assertEqual(response.body, b"nested_handler: /nested_handler")



        response = self.fetch("/nested_not_found_handler")

        self.assertEqual(response.code, 404)



        response = self.fetch("/connection_delegate")

        self.assertEqual(response.body, b"OK")



        response = self.fetch("/request_callable")

        self.assertEqual(response.body, b"OK")



        response = self.fetch("/404")

        self.assertEqual(response.code, 404)





class WSGIContainerTestCase(AsyncHTTPTestCase):

    def get_app(self):

        wsgi_app = WSGIContainer(self.wsgi_app)



        class Handler(RequestHandler):

            def get(self, *args, **kwargs):

                self.finish(self.reverse_url("tornado"))



        return RuleRouter(

            [

                (

                    PathMatches("/tornado.*"),

                    Application([(r"/tornado/test", Handler, {}, "tornado")]),

                ),

                (PathMatches("/wsgi"), wsgi_app),

            ]

        )



    def wsgi_app(self, environ, start_response):

        start_response("200 OK", [])

        return [b"WSGI"]



    def test_wsgi_container(self):

        response = self.fetch("/tornado/test")

        self.assertEqual(response.body, b"/tornado/test")



        response = self.fetch("/wsgi")

        self.assertEqual(response.body, b"WSGI")



    def test_delegate_not_found(self):

        response = self.fetch("/404")

        self.assertEqual(response.code, 404)

from functools import reduce

import gc

import io

import locale  # system locale module, not tornado.locale

import logging

import operator

import textwrap

import sys

import unittest

import warnings



from tornado.httpclient import AsyncHTTPClient

from tornado.httpserver import HTTPServer

from tornado.netutil import Resolver

from tornado.options import define, add_parse_callback





TEST_MODULES = [

    "tornado.httputil.doctests",

    "tornado.iostream.doctests",

    "tornado.util.doctests",

    "tornado.test.asyncio_test",

    "tornado.test.auth_test",

    "tornado.test.autoreload_test",

    "tornado.test.concurrent_test",

    "tornado.test.curl_httpclient_test",

    "tornado.test.escape_test",

    "tornado.test.gen_test",

    "tornado.test.http1connection_test",

    "tornado.test.httpclient_test",

    "tornado.test.httpserver_test",

    "tornado.test.httputil_test",

    "tornado.test.import_test",

    "tornado.test.ioloop_test",

    "tornado.test.iostream_test",

    "tornado.test.locale_test",

    "tornado.test.locks_test",

    "tornado.test.netutil_test",

    "tornado.test.log_test",

    "tornado.test.options_test",

    "tornado.test.process_test",

    "tornado.test.queues_test",

    "tornado.test.routing_test",

    "tornado.test.simple_httpclient_test",

    "tornado.test.tcpclient_test",

    "tornado.test.tcpserver_test",

    "tornado.test.template_test",

    "tornado.test.testing_test",

    "tornado.test.twisted_test",

    "tornado.test.util_test",

    "tornado.test.web_test",

    "tornado.test.websocket_test",

    "tornado.test.windows_test",

    "tornado.test.wsgi_test",

]





def all():

    return unittest.defaultTestLoader.loadTestsFromNames(TEST_MODULES)





def test_runner_factory(stderr):

    class TornadoTextTestRunner(unittest.TextTestRunner):

        def __init__(self, *args, **kwargs):

            kwargs["stream"] = stderr

            super(TornadoTextTestRunner, self).__init__(*args, **kwargs)



        def run(self, test):

            result = super(TornadoTextTestRunner, self).run(test)

            if result.skipped:

                skip_reasons = set(reason for (test, reason) in result.skipped)

                self.stream.write(

                    textwrap.fill(

                        "Some tests were skipped because: %s"

                        % ", ".join(sorted(skip_reasons))

                    )

                )

                self.stream.write("\n")

            return result



    return TornadoTextTestRunner





class LogCounter(logging.Filter):

    """Counts the number of WARNING or higher log records."""



    def __init__(self, *args, **kwargs):

        super(LogCounter, self).__init__(*args, **kwargs)

        self.info_count = self.warning_count = self.error_count = 0



    def filter(self, record):

        if record.levelno >= logging.ERROR:

            self.error_count += 1

        elif record.levelno >= logging.WARNING:

            self.warning_count += 1

        elif record.levelno >= logging.INFO:

            self.info_count += 1

        return True





class CountingStderr(io.IOBase):

    def __init__(self, real):

        self.real = real

        self.byte_count = 0



    def write(self, data):

        self.byte_count += len(data)

        return self.real.write(data)



    def flush(self):

        return self.real.flush()





def main():

    # Be strict about most warnings (This is set in our test running

    # scripts to catch import-time warnings, but set it again here to

    # be sure). This also turns on warnings that are ignored by

    # default, including DeprecationWarnings and python 3.2's

    # ResourceWarnings.

    warnings.filterwarnings("error")

    # setuptools sometimes gives ImportWarnings about things that are on

    # sys.path even if they're not being used.

    warnings.filterwarnings("ignore", category=ImportWarning)

    # Tornado generally shouldn't use anything deprecated, but some of

    # our dependencies do (last match wins).

    warnings.filterwarnings("ignore", category=DeprecationWarning)

    warnings.filterwarnings("error", category=DeprecationWarning, module=r"tornado\..*")

    warnings.filterwarnings("ignore", category=PendingDeprecationWarning)

    warnings.filterwarnings(

        "error", category=PendingDeprecationWarning, module=r"tornado\..*"

    )

    # The unittest module is aggressive about deprecating redundant methods,

    # leaving some without non-deprecated spellings that work on both

    # 2.7 and 3.2

    warnings.filterwarnings(

        "ignore", category=DeprecationWarning, message="Please use assert.* instead"

    )

    warnings.filterwarnings(

        "ignore",

        category=PendingDeprecationWarning,

        message="Please use assert.* instead",

    )

    # Twisted 15.0.0 triggers some warnings on py3 with -bb.

    warnings.filterwarnings("ignore", category=BytesWarning, module=r"twisted\..*")

    if (3,) < sys.version_info < (3, 6):

        # Prior to 3.6, async ResourceWarnings were rather noisy

        # and even

        # `python3.4 -W error -c 'import asyncio; asyncio.get_event_loop()'`

        # would generate a warning.

        warnings.filterwarnings(

            "ignore", category=ResourceWarning, module=r"asyncio\..*"

        )

    # This deprecation warning is introduced in Python 3.8 and is

    # triggered by pycurl. Unforunately, because it is raised in the C

    # layer it can't be filtered by module and we must match the

    # message text instead (Tornado's C module uses PY_SSIZE_T_CLEAN

    # so it's not at risk of running into this issue).

    warnings.filterwarnings(

        "ignore",

        category=DeprecationWarning,

        message="PY_SSIZE_T_CLEAN will be required",

    )



    logging.getLogger("tornado.access").setLevel(logging.CRITICAL)



    define(

        "httpclient",

        type=str,

        default=None,

        callback=lambda s: AsyncHTTPClient.configure(

            s, defaults=dict(allow_ipv6=False)

        ),

    )

    define("httpserver", type=str, default=None, callback=HTTPServer.configure)

    define("resolver", type=str, default=None, callback=Resolver.configure)

    define(

        "debug_gc",

        type=str,

        multiple=True,

        help="A comma-separated list of gc module debug constants, "

        "e.g. DEBUG_STATS or DEBUG_COLLECTABLE,DEBUG_OBJECTS",

        callback=lambda values: gc.set_debug(

            reduce(operator.or_, (getattr(gc, v) for v in values))

        ),

    )



    def set_locale(x):

        locale.setlocale(locale.LC_ALL, x)



    define("locale", type=str, default=None, callback=set_locale)



    log_counter = LogCounter()

    add_parse_callback(lambda: logging.getLogger().handlers[0].addFilter(log_counter))



    # Certain errors (especially "unclosed resource" errors raised in

    # destructors) go directly to stderr instead of logging. Count

    # anything written by anything but the test runner as an error.

    orig_stderr = sys.stderr

    counting_stderr = CountingStderr(orig_stderr)

    sys.stderr = counting_stderr  # type: ignore



    import tornado.testing



    kwargs = {}



    # HACK:  unittest.main will make its own changes to the warning

    # configuration, which may conflict with the settings above

    # or command-line flags like -bb.  Passing warnings=False

    # suppresses this behavior, although this looks like an implementation

    # detail.  http://bugs.python.org/issue15626

    kwargs["warnings"] = False



    kwargs["testRunner"] = test_runner_factory(orig_stderr)

    try:

        tornado.testing.main(**kwargs)

    finally:

        # The tests should run clean; consider it a failure if they

        # logged anything at info level or above.

        if (

            log_counter.info_count > 0

            or log_counter.warning_count > 0

            or log_counter.error_count > 0

            or counting_stderr.byte_count > 0

        ):

            logging.error(

                "logged %d infos, %d warnings, %d errors, and %d bytes to stderr",

                log_counter.info_count,

                log_counter.warning_count,

                log_counter.error_count,

                counting_stderr.byte_count,

            )

            sys.exit(1)





if __name__ == "__main__":

    main()

import collections

from contextlib import closing

import errno

import gzip

import logging

import os

import re

import socket

import ssl

import sys

import typing  # noqa: F401



from tornado.escape import to_unicode, utf8

from tornado import gen

from tornado.httpclient import AsyncHTTPClient

from tornado.httputil import HTTPHeaders, ResponseStartLine

from tornado.ioloop import IOLoop

from tornado.iostream import UnsatisfiableReadError

from tornado.locks import Event

from tornado.log import gen_log

from tornado.netutil import Resolver, bind_sockets

from tornado.simple_httpclient import (

    SimpleAsyncHTTPClient,

    HTTPStreamClosedError,

    HTTPTimeoutError,

)

from tornado.test.httpclient_test import (

    ChunkHandler,

    CountdownHandler,

    HelloWorldHandler,

    RedirectHandler,

)

from tornado.test import httpclient_test

from tornado.testing import (

    AsyncHTTPTestCase,

    AsyncHTTPSTestCase,

    AsyncTestCase,

    ExpectLog,

    gen_test,

)

from tornado.test.util import skipOnTravis, skipIfNoIPv6, refusing_port

from tornado.web import RequestHandler, Application, url, stream_request_body





class SimpleHTTPClientCommonTestCase(httpclient_test.HTTPClientCommonTestCase):

    def get_http_client(self):

        client = SimpleAsyncHTTPClient(force_instance=True)

        self.assertTrue(isinstance(client, SimpleAsyncHTTPClient))

        return client





class TriggerHandler(RequestHandler):

    def initialize(self, queue, wake_callback):

        self.queue = queue

        self.wake_callback = wake_callback



    @gen.coroutine

    def get(self):

        logging.debug("queuing trigger")

        event = Event()

        self.queue.append(event.set)

        if self.get_argument("wake", "true") == "true":

            self.wake_callback()

        yield event.wait()





class ContentLengthHandler(RequestHandler):

    def get(self):

        self.stream = self.detach()

        IOLoop.current().spawn_callback(self.write_response)



    @gen.coroutine

    def write_response(self):

        yield self.stream.write(

            utf8(

                "HTTP/1.0 200 OK\r\nContent-Length: %s\r\n\r\nok"

                % self.get_argument("value")

            )

        )

        self.stream.close()





class HeadHandler(RequestHandler):

    def head(self):

        self.set_header("Content-Length", "7")





class OptionsHandler(RequestHandler):

    def options(self):

        self.set_header("Access-Control-Allow-Origin", "*")

        self.write("ok")





class NoContentHandler(RequestHandler):

    def get(self):

        self.set_status(204)

        self.finish()





class SeeOtherPostHandler(RequestHandler):

    def post(self):

        redirect_code = int(self.request.body)

        assert redirect_code in (302, 303), "unexpected body %r" % self.request.body

        self.set_header("Location", "/see_other_get")

        self.set_status(redirect_code)





class SeeOtherGetHandler(RequestHandler):

    def get(self):

        if self.request.body:

            raise Exception("unexpected body %r" % self.request.body)

        self.write("ok")





class HostEchoHandler(RequestHandler):

    def get(self):

        self.write(self.request.headers["Host"])





class NoContentLengthHandler(RequestHandler):

    def get(self):

        if self.request.version.startswith("HTTP/1"):

            # Emulate the old HTTP/1.0 behavior of returning a body with no

            # content-length.  Tornado handles content-length at the framework

            # level so we have to go around it.

            stream = self.detach()

            stream.write(b"HTTP/1.0 200 OK\r\n\r\n" b"hello")

            stream.close()

        else:

            self.finish("HTTP/1 required")





class EchoPostHandler(RequestHandler):

    def post(self):

        self.write(self.request.body)





@stream_request_body

class RespondInPrepareHandler(RequestHandler):

    def prepare(self):

        self.set_status(403)

        self.finish("forbidden")





class SimpleHTTPClientTestMixin(object):

    def get_app(self):

        # callable objects to finish pending /trigger requests

        self.triggers = collections.deque()  # type: typing.Deque[str]

        return Application(

            [

                url(

                    "/trigger",

                    TriggerHandler,

                    dict(queue=self.triggers, wake_callback=self.stop),

                ),

                url("/chunk", ChunkHandler),

                url("/countdown/([0-9]+)", CountdownHandler, name="countdown"),

                url("/hello", HelloWorldHandler),

                url("/content_length", ContentLengthHandler),

                url("/head", HeadHandler),

                url("/options", OptionsHandler),

                url("/no_content", NoContentHandler),

                url("/see_other_post", SeeOtherPostHandler),

                url("/see_other_get", SeeOtherGetHandler),

                url("/host_echo", HostEchoHandler),

                url("/no_content_length", NoContentLengthHandler),

                url("/echo_post", EchoPostHandler),

                url("/respond_in_prepare", RespondInPrepareHandler),

                url("/redirect", RedirectHandler),

            ],

            gzip=True,

        )



    def test_singleton(self):

        # Class "constructor" reuses objects on the same IOLoop

        self.assertTrue(SimpleAsyncHTTPClient() is SimpleAsyncHTTPClient())

        # unless force_instance is used

        self.assertTrue(

            SimpleAsyncHTTPClient() is not SimpleAsyncHTTPClient(force_instance=True)

        )

        # different IOLoops use different objects

        with closing(IOLoop()) as io_loop2:



            async def make_client():

                await gen.sleep(0)

                return SimpleAsyncHTTPClient()



            client1 = self.io_loop.run_sync(make_client)

            client2 = io_loop2.run_sync(make_client)

            self.assertTrue(client1 is not client2)



    def test_connection_limit(self):

        with closing(self.create_client(max_clients=2)) as client:

            self.assertEqual(client.max_clients, 2)

            seen = []

            # Send 4 requests.  Two can be sent immediately, while the others

            # will be queued

            for i in range(4):



                def cb(fut, i=i):

                    seen.append(i)

                    self.stop()



                client.fetch(self.get_url("/trigger")).add_done_callback(cb)

            self.wait(condition=lambda: len(self.triggers) == 2)

            self.assertEqual(len(client.queue), 2)



            # Finish the first two requests and let the next two through

            self.triggers.popleft()()

            self.triggers.popleft()()

            self.wait(condition=lambda: (len(self.triggers) == 2 and len(seen) == 2))

            self.assertEqual(set(seen), set([0, 1]))

            self.assertEqual(len(client.queue), 0)



            # Finish all the pending requests

            self.triggers.popleft()()

            self.triggers.popleft()()

            self.wait(condition=lambda: len(seen) == 4)

            self.assertEqual(set(seen), set([0, 1, 2, 3]))

            self.assertEqual(len(self.triggers), 0)



    @gen_test

    def test_redirect_connection_limit(self):

        # following redirects should not consume additional connections

        with closing(self.create_client(max_clients=1)) as client:

            response = yield client.fetch(self.get_url("/countdown/3"), max_redirects=3)

            response.rethrow()



    def test_gzip(self):

        # All the tests in this file should be using gzip, but this test

        # ensures that it is in fact getting compressed.

        # Setting Accept-Encoding manually bypasses the client's

        # decompression so we can see the raw data.

        response = self.fetch(

            "/chunk", use_gzip=False, headers={"Accept-Encoding": "gzip"}

        )

        self.assertEqual(response.headers["Content-Encoding"], "gzip")

        self.assertNotEqual(response.body, b"asdfqwer")

        # Our test data gets bigger when gzipped.  Oops.  :)

        # Chunked encoding bypasses the MIN_LENGTH check.

        self.assertEqual(len(response.body), 34)

        f = gzip.GzipFile(mode="r", fileobj=response.buffer)

        self.assertEqual(f.read(), b"asdfqwer")



    def test_max_redirects(self):

        response = self.fetch("/countdown/5", max_redirects=3)

        self.assertEqual(302, response.code)

        # We requested 5, followed three redirects for 4, 3, 2, then the last

        # unfollowed redirect is to 1.

        self.assertTrue(response.request.url.endswith("/countdown/5"))

        self.assertTrue(response.effective_url.endswith("/countdown/2"))

        self.assertTrue(response.headers["Location"].endswith("/countdown/1"))



    def test_header_reuse(self):

        # Apps may reuse a headers object if they are only passing in constant

        # headers like user-agent.  The header object should not be modified.

        headers = HTTPHeaders({"User-Agent": "Foo"})

        self.fetch("/hello", headers=headers)

        self.assertEqual(list(headers.get_all()), [("User-Agent", "Foo")])



    def test_see_other_redirect(self):

        for code in (302, 303):

            response = self.fetch("/see_other_post", method="POST", body="%d" % code)

            self.assertEqual(200, response.code)

            self.assertTrue(response.request.url.endswith("/see_other_post"))

            self.assertTrue(response.effective_url.endswith("/see_other_get"))

            # request is the original request, is a POST still

            self.assertEqual("POST", response.request.method)



    @skipOnTravis

    @gen_test

    def test_connect_timeout(self):

        timeout = 0.1



        cleanup_event = Event()

        test = self



        class TimeoutResolver(Resolver):

            async def resolve(self, *args, **kwargs):

                await cleanup_event.wait()

                # Return something valid so the test doesn't raise during shutdown.

                return [(socket.AF_INET, ("127.0.0.1", test.get_http_port()))]



        with closing(self.create_client(resolver=TimeoutResolver())) as client:

            with self.assertRaises(HTTPTimeoutError):

                yield client.fetch(

                    self.get_url("/hello"),

                    connect_timeout=timeout,

                    request_timeout=3600,

                    raise_error=True,

                )



        # Let the hanging coroutine clean up after itself. We need to

        # wait more than a single IOLoop iteration for the SSL case,

        # which logs errors on unexpected EOF.

        cleanup_event.set()

        yield gen.sleep(0.2)



    @skipOnTravis

    def test_request_timeout(self):

        timeout = 0.1

        if os.name == "nt":

            timeout = 0.5



        with self.assertRaises(HTTPTimeoutError):

            self.fetch("/trigger?wake=false", request_timeout=timeout, raise_error=True)

        # trigger the hanging request to let it clean up after itself

        self.triggers.popleft()()

        self.io_loop.run_sync(lambda: gen.sleep(0))



    @skipIfNoIPv6

    def test_ipv6(self):

        [sock] = bind_sockets(0, "::1", family=socket.AF_INET6)

        port = sock.getsockname()[1]

        self.http_server.add_socket(sock)

        url = "%s://[::1]:%d/hello" % (self.get_protocol(), port)



        # ipv6 is currently enabled by default but can be disabled

        with self.assertRaises(Exception):

            self.fetch(url, allow_ipv6=False, raise_error=True)



        response = self.fetch(url)

        self.assertEqual(response.body, b"Hello world!")



    def test_multiple_content_length_accepted(self):

        response = self.fetch("/content_length?value=2,2")

        self.assertEqual(response.body, b"ok")

        response = self.fetch("/content_length?value=2,%202,2")

        self.assertEqual(response.body, b"ok")



        with ExpectLog(gen_log, ".*Multiple unequal Content-Lengths"):

            with self.assertRaises(HTTPStreamClosedError):

                self.fetch("/content_length?value=2,4", raise_error=True)

            with self.assertRaises(HTTPStreamClosedError):

                self.fetch("/content_length?value=2,%202,3", raise_error=True)



    def test_head_request(self):

        response = self.fetch("/head", method="HEAD")

        self.assertEqual(response.code, 200)

        self.assertEqual(response.headers["content-length"], "7")

        self.assertFalse(response.body)



    def test_options_request(self):

        response = self.fetch("/options", method="OPTIONS")

        self.assertEqual(response.code, 200)

        self.assertEqual(response.headers["content-length"], "2")

        self.assertEqual(response.headers["access-control-allow-origin"], "*")

        self.assertEqual(response.body, b"ok")



    def test_no_content(self):

        response = self.fetch("/no_content")

        self.assertEqual(response.code, 204)

        # 204 status shouldn't have a content-length

        #

        # Tests with a content-length header are included below

        # in HTTP204NoContentTestCase.

        self.assertNotIn("Content-Length", response.headers)



    def test_host_header(self):

        host_re = re.compile(b"^127.0.0.1:[0-9]+$")

        response = self.fetch("/host_echo")

        self.assertTrue(host_re.match(response.body))



        url = self.get_url("/host_echo").replace("http://", "http://me:secret@")

        response = self.fetch(url)

        self.assertTrue(host_re.match(response.body), response.body)



    def test_connection_refused(self):

        cleanup_func, port = refusing_port()

        self.addCleanup(cleanup_func)

        with ExpectLog(gen_log, ".*", required=False):

            with self.assertRaises(socket.error) as cm:

                self.fetch("http://127.0.0.1:%d/" % port, raise_error=True)



        if sys.platform != "cygwin":

            # cygwin returns EPERM instead of ECONNREFUSED here

            contains_errno = str(errno.ECONNREFUSED) in str(cm.exception)

            if not contains_errno and hasattr(errno, "WSAECONNREFUSED"):

                contains_errno = str(errno.WSAECONNREFUSED) in str(  # type: ignore

                    cm.exception

                )

            self.assertTrue(contains_errno, cm.exception)

            # This is usually "Connection refused".

            # On windows, strerror is broken and returns "Unknown error".

            expected_message = os.strerror(errno.ECONNREFUSED)

            self.assertTrue(expected_message in str(cm.exception), cm.exception)



    def test_queue_timeout(self):

        with closing(self.create_client(max_clients=1)) as client:

            # Wait for the trigger request to block, not complete.

            fut1 = client.fetch(self.get_url("/trigger"), request_timeout=10)

            self.wait()

            with self.assertRaises(HTTPTimeoutError) as cm:

                self.io_loop.run_sync(

                    lambda: client.fetch(

                        self.get_url("/hello"), connect_timeout=0.1, raise_error=True

                    )

                )



            self.assertEqual(str(cm.exception), "Timeout in request queue")

            self.triggers.popleft()()

            self.io_loop.run_sync(lambda: fut1)



    def test_no_content_length(self):

        response = self.fetch("/no_content_length")

        if response.body == b"HTTP/1 required":

            self.skipTest("requires HTTP/1.x")

        else:

            self.assertEquals(b"hello", response.body)



    def sync_body_producer(self, write):

        write(b"1234")

        write(b"5678")



    @gen.coroutine

    def async_body_producer(self, write):

        yield write(b"1234")

        yield gen.moment

        yield write(b"5678")



    def test_sync_body_producer_chunked(self):

        response = self.fetch(

            "/echo_post", method="POST", body_producer=self.sync_body_producer

        )

        response.rethrow()

        self.assertEqual(response.body, b"12345678")



    def test_sync_body_producer_content_length(self):

        response = self.fetch(

            "/echo_post",

            method="POST",

            body_producer=self.sync_body_producer,

            headers={"Content-Length": "8"},

        )

        response.rethrow()

        self.assertEqual(response.body, b"12345678")



    def test_async_body_producer_chunked(self):

        response = self.fetch(

            "/echo_post", method="POST", body_producer=self.async_body_producer

        )

        response.rethrow()

        self.assertEqual(response.body, b"12345678")



    def test_async_body_producer_content_length(self):

        response = self.fetch(

            "/echo_post",

            method="POST",

            body_producer=self.async_body_producer,

            headers={"Content-Length": "8"},

        )

        response.rethrow()

        self.assertEqual(response.body, b"12345678")



    def test_native_body_producer_chunked(self):

        async def body_producer(write):

            await write(b"1234")

            import asyncio



            await asyncio.sleep(0)

            await write(b"5678")



        response = self.fetch("/echo_post", method="POST", body_producer=body_producer)

        response.rethrow()

        self.assertEqual(response.body, b"12345678")



    def test_native_body_producer_content_length(self):

        async def body_producer(write):

            await write(b"1234")

            import asyncio



            await asyncio.sleep(0)

            await write(b"5678")



        response = self.fetch(

            "/echo_post",

            method="POST",

            body_producer=body_producer,

            headers={"Content-Length": "8"},

        )

        response.rethrow()

        self.assertEqual(response.body, b"12345678")



    def test_100_continue(self):

        response = self.fetch(

            "/echo_post", method="POST", body=b"1234", expect_100_continue=True

        )

        self.assertEqual(response.body, b"1234")



    def test_100_continue_early_response(self):

        def body_producer(write):

            raise Exception("should not be called")



        response = self.fetch(

            "/respond_in_prepare",

            method="POST",

            body_producer=body_producer,

            expect_100_continue=True,

        )

        self.assertEqual(response.code, 403)



    def test_streaming_follow_redirects(self):

        # When following redirects, header and streaming callbacks

        # should only be called for the final result.

        # TODO(bdarnell): this test belongs in httpclient_test instead of

        # simple_httpclient_test, but it fails with the version of libcurl

        # available on travis-ci. Move it when that has been upgraded

        # or we have a better framework to skip tests based on curl version.

        headers = []  # type: typing.List[str]

        chunk_bytes = []  # type: typing.List[bytes]

        self.fetch(

            "/redirect?url=/hello",

            header_callback=headers.append,

            streaming_callback=chunk_bytes.append,

        )

        chunks = list(map(to_unicode, chunk_bytes))

        self.assertEqual(chunks, ["Hello world!"])

        # Make sure we only got one set of headers.

        num_start_lines = len([h for h in headers if h.startswith("HTTP/")])

        self.assertEqual(num_start_lines, 1)





class SimpleHTTPClientTestCase(SimpleHTTPClientTestMixin, AsyncHTTPTestCase):

    def setUp(self):

        super(SimpleHTTPClientTestCase, self).setUp()

        self.http_client = self.create_client()



    def create_client(self, **kwargs):

        return SimpleAsyncHTTPClient(force_instance=True, **kwargs)





